
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Learn how to build a PyTorch-like, autograd-powered Deep Learning Framework with automatic differentiation.">
      
      
      
        <link rel="canonical" href="https://datasanta.net/2025/04/03/microtorch---deep-learning-from-scratch/">
      
      
        <link rel="prev" href="../../../02/05/classification---cross-entropy--softmax/">
      
      
        <link rel="next" href="../../13/empirical-risk-and-cross-entropy-in-microtorch/">
      
      
        <link rel="alternate" type="application/rss+xml" title="RSS feed" href="../../../../feed_rss_created.xml">
        <link rel="alternate" type="application/rss+xml" title="RSS feed of updated content" href="../../../../feed_rss_updated.xml">
      
      <link rel="icon" href="../../../../assets/logo.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.11">
    
    
      
        <title>MicroTorch - Deep Learning from Scratch! - DataSanta</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-Y9BWQQSE0S"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-Y9BWQQSE0S",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-Y9BWQQSE0S",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="MicroTorch - Deep Learning from Scratch! - DataSanta" >
      
        <meta  property="og:description"  content="Learn how to build a PyTorch-like, autograd-powered Deep Learning Framework with automatic differentiation." >
      
        <meta  property="og:image"  content="https://datasanta.net/assets/images/social/posts/autograd_essential.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://datasanta.net/2025/04/03/microtorch---deep-learning-from-scratch/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="MicroTorch - Deep Learning from Scratch! - DataSanta" >
      
        <meta  name="twitter:description"  content="Learn how to build a PyTorch-like, autograd-powered Deep Learning Framework with automatic differentiation." >
      
        <meta  name="twitter:image"  content="https://datasanta.net/assets/images/social/posts/autograd_essential.png" >
      
    
    
   <link href="../../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#check-the-jupyter-notebook" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="DataSanta" class="md-header__button md-logo" aria-label="DataSanta" data-md-component="logo">
      
  <img src="../../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            DataSanta
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              MicroTorch - Deep Learning from Scratch!
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="DataSanta" class="md-nav__button md-logo" aria-label="DataSanta" data-md-component="logo">
      
  <img src="../../../../assets/logo.png" alt="logo">

    </a>
    DataSanta
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DataSanta
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Archive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2025/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2025
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2024/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    2024
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Categories
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Categories
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/classification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Classification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/computational-methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Computational Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/data-science/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Science
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/data-transformations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Transformations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/data-visualization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Visualization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/deep-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deep Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/dimensionality-reduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dimensionality Reduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/feature-engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Feature Engineering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/linear-algebra/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linear Algebra
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/loss-functions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Loss Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/machine-learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Machine Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/mathematics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mathematics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/natural-language-processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Natural Language Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/neural-networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Networks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/numerical-methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Numerical Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/optimizations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimizations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/programming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/speech-and-audio-processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speech and Audio Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/tts-text-to-speech/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TTS (Text to Speech)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#check-the-jupyter-notebook" class="md-nav__link">
    <span class="md-ellipsis">
      Check the Jupyter Notebook
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#imports-and-type-definitions" class="md-nav__link">
    <span class="md-ellipsis">
      Imports and Type Definitions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computational-graph" class="md-nav__link">
    <span class="md-ellipsis">
      Computational graph
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tensor-class" class="md-nav__link">
    <span class="md-ellipsis">
      Tensor Class
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-operation-transpose" class="md-nav__link">
    <span class="md-ellipsis">
      First operation - Transpose
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-backward-method" class="md-nav__link">
    <span class="md-ellipsis">
      The backward Method
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#parameter-class-foundation-for-neural-network-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter Class: Foundation for Neural Network Parameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#module-class-base-for-all-neural-network-modules" class="md-nav__link">
    <span class="md-ellipsis">
      Module Class: Base for All Neural Network Modules
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequential-class-chaining-modules-in-order" class="md-nav__link">
    <span class="md-ellipsis">
      Sequential Class: Chaining Modules in Order
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-layer-matrix-matrix-dot-product" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Layer: Matrix-Matrix Dot Product
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear Layer: Matrix-Matrix Dot Product">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dot-product" class="md-nav__link">
    <span class="md-ellipsis">
      Dot product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ensuring-data-consistency-with-data_gate" class="md-nav__link">
    <span class="md-ellipsis">
      Ensuring Data Consistency with data_gate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matmul-operator" class="md-nav__link">
    <span class="md-ellipsis">
      Matmul operator @
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-layer-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Layer Implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#broadcasting-in-backward-mode" class="md-nav__link">
    <span class="md-ellipsis">
      Broadcasting in backward mode
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Broadcasting in backward mode">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#add-sub-and-their-friends" class="md-nav__link">
    <span class="md-ellipsis">
      add, sub and their friends
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mul" class="md-nav__link">
    <span class="md-ellipsis">
      mul
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#logs-exponents-and-activation-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Logs, Exponents, and Activation Functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Logs, Exponents, and Activation Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#log" class="md-nav__link">
    <span class="md-ellipsis">
      log
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tanh" class="md-nav__link">
    <span class="md-ellipsis">
      tanh
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pow" class="md-nav__link">
    <span class="md-ellipsis">
      pow
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#division" class="md-nav__link">
    <span class="md-ellipsis">
      division
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exp" class="md-nav__link">
    <span class="md-ellipsis">
      exp
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simple-activation-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Simple activation functions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#more-ops" class="md-nav__link">
    <span class="md-ellipsis">
      More Ops
    </span>
  </a>
  
    <nav class="md-nav" aria-label="More Ops">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#squeeze-and-unsqueeze-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Squeeze and Unsqueeze Operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#view" class="md-nav__link">
    <span class="md-ellipsis">
      view
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip" class="md-nav__link">
    <span class="md-ellipsis">
      clip
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#where" class="md-nav__link">
    <span class="md-ellipsis">
      where
    </span>
  </a>
  
    <nav class="md-nav" aria-label="where">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#expanding-where-more-useful-tensor-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Expanding where - More Useful Tensor Operations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#indexing" class="md-nav__link">
    <span class="md-ellipsis">
      Indexing
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#more-tensor-ops" class="md-nav__link">
    <span class="md-ellipsis">
      More Tensor ops
    </span>
  </a>
  
    <nav class="md-nav" aria-label="More Tensor ops">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#abs" class="md-nav__link">
    <span class="md-ellipsis">
      abs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#max" class="md-nav__link">
    <span class="md-ellipsis">
      max
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#min" class="md-nav__link">
    <span class="md-ellipsis">
      min
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sum" class="md-nav__link">
    <span class="md-ellipsis">
      sum
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mean" class="md-nav__link">
    <span class="md-ellipsis">
      mean
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#more-activation-functions" class="md-nav__link">
    <span class="md-ellipsis">
      More Activation Functions!
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../.." class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="https://avatars.githubusercontent.com/u/7540752" alt="Nick Ovchinnikov">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          <a href="https://github.com/nickovchinnikov">Nick Ovchinnikov</a>
                        
                      </strong>
                      <br>
                      Follow the white rabbit
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2025-04-03 00:00:00+00:00" class="md-ellipsis">April 3, 2025</time>
                      </div>
                    </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../../../category/deep-learning/">Deep Learning</a>, 
                              <a href="../../../../category/machine-learning/">Machine Learning</a>, 
                              <a href="../../../../category/neural-networks/">Neural Networks</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              58 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
            
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        
  


  <nav class="md-tags" >
    
      
      
      
        <span class="md-tag">Autograd</span>
      
    
      
      
      
        <span class="md-tag">Automatic Differentiation</span>
      
    
      
      
      
        <span class="md-tag">Backpropagation</span>
      
    
      
      
      
        <span class="md-tag">Computational Graphs</span>
      
    
      
      
      
        <span class="md-tag">Framework Development</span>
      
    
      
      
      
        <span class="md-tag">Tensor Operations</span>
      
    
  </nav>



  <h1>MicroTorch - Deep Learning from Scratch!</h1>

<p>Implementing deep learning algorithms involves managing data flow in two directions: <code>forward</code> and <code>backward</code>. While the <code>forward</code> pass is typically straightforward, handling the <code>backward</code> pass can be more challenging. As discussed in previous posts, implementing backpropagation requires a strong grasp of calculus, and even minor mistakes can lead to significant issues.</p>
<p>Fortunately, modern frameworks like PyTorch simplify this process with <strong>autograd</strong>, an automatic differentiation system that dynamically computes gradients during training. This eliminates the need for manually deriving and coding gradient calculations, making development more efficient and less error-prone.</p>
<p>Now, let's build the backbone of such an algorithm - <code>Tensor</code> class!</p>
<figure>
<p><a class="glightbox" href="../../../../assets/autograd/cover.jpg" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Autograd cover" src="../../../../assets/autograd/cover.jpg" /></a></p>
<figcaption>
<p>Build an autograd!</p>
</figcaption>
</figure>
<!-- more -->

<h3 id="check-the-jupyter-notebook"><a href="https://github.com/nickovchinnikov/datasanta/blob/master/code/13.Tensor.ipynb">Check the Jupyter Notebook</a><a class="headerlink" href="#check-the-jupyter-notebook" title="Permanent link">&para;</a></h3>
<p>In the previous chapters, I built everything from the ground up. Now, we will create a <code>Tensor</code> object that abstracts the implementation of the backward steps for our building blocks.  </p>
<p>Instead of manually coding the backward pass, we'll design a class that constructs a computation graph, tracking every operation within it. Once the graph is established, we will run the backward pass to compute gradients for all operations automatically.  </p>
<p>Let's start by defining the <code>Tensor</code> class and initializing its basic methods.</p>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/7GHa_Cla5wU" title="Building PyTorch from Scratch: Create a Tensor Class with AutoDiff" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<h3 id="imports-and-type-definitions">Imports and Type Definitions<a class="headerlink" href="#imports-and-type-definitions" title="Permanent link">&para;</a></h3>
<p>Imports necessary libraries and defines type aliases for better readability.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="c1"># Scalar is a type alias for either an int or float.</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">Scalar</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="c1"># Data is a type alias for any valid input that can be converted into a Tensor </span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="c1"># (e.g., scalars, lists, NumPy arrays, or other Tensor objects).</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="n">Data</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Scalar</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">]</span>
</span></code></pre></div>
<p><strong>Example:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="n">scalar_value</span><span class="p">:</span> <span class="n">Scalar</span> <span class="o">=</span> <span class="mf">5.0</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">data_list</span><span class="p">:</span> <span class="n">Data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">data_np</span><span class="p">:</span> <span class="n">Data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></code></pre></div>
<h3 id="computational-graph">Computational graph<a class="headerlink" href="#computational-graph" title="Permanent link">&para;</a></h3>
<p>A <strong>computation graph</strong> is a directed graph where nodes represent operations (like addition or multiplication), and edges represent the flow of data (tensors). In the context of your <code>Tensor</code> class, each tensor is a node, and operations between tensors create edges between them.</p>
<p>When you perform an operation, such as <code>Tensor A + Tensor B</code>, a new tensor is created, which records its dependencies on <code>A</code> and <code>B</code>. These dependencies are tracked in the <code>Tensor</code> object via the <code>dependencies</code> list. During the backward pass, the graph is traversed in reverse order to compute gradients for each tensor, starting from the final result back to the input tensors.</p>
<p>By storing these dependencies in <code>Leaf</code> objects, the graph allows automatic differentiation, meaning gradients are computed for all involved tensors without manually specifying the backpropagation steps.</p>
<p><code>Leaf</code> is a simple class used for storing the relationship between a <code>Tensor</code> (as value) and a function (<code>grad_fn</code>), which is responsible for computing the gradient for that <code>Tensor</code>. The <code>frozen=True</code> parameter makes the instance of the class immutable, meaning once created, its attributes cannot be changed.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="k">class</span><span class="w"> </span><span class="nc">Leaf</span><span class="p">:</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>    <span class="n">value</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>    <span class="n">grad_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
</span></code></pre></div>
<h3 id="tensor-class">Tensor Class<a class="headerlink" href="#tensor-class" title="Permanent link">&para;</a></h3>
<p>The Tensor class is the core of this implementation. It encapsulates data and provides functionality for automatic differentiation. The code bellow defines a simple <code>Tensor</code> class backbone. </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">Tensor</span><span class="p">:</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="sd">    A class representing a multi-dimensional array (Tensor) with automatic differentiation support.</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>        <span class="n">data</span><span class="p">:</span> <span class="n">Data</span><span class="p">,</span>
</span><span id="__span-3-9"><a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="__span-3-10"><a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>        <span class="n">dependencies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-3-11"><a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
</span><span id="__span-3-12"><a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-3-13"><a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-14"><a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a><span class="sd">        Initializes a Tensor object.</span>
</span><span id="__span-3-15"><a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a>
</span><span id="__span-3-16"><a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a><span class="sd">        Args:</span>
</span><span id="__span-3-17"><a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a><span class="sd">            data (Data): The input data, which can be a scalar, list, NumPy array, or another Tensor.</span>
</span><span id="__span-3-18"><a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a><span class="sd">            requires_grad (bool, optional): If True, enables gradient tracking. Defaults to False.</span>
</span><span id="__span-3-19"><a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a><span class="sd">            dependencies (Optional[List[Leaf]], optional): Dependencies in the computation graph. Defaults to None.</span>
</span><span id="__span-3-20"><a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a><span class="sd">            dtype (dtype, optional): The data type of the Tensor. Defaults to np.float32.</span>
</span><span id="__span-3-21"><a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-3-22"><a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a>
</span><span id="__span-3-23"><a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a>        <span class="c1"># data: The input data, which can be a scalar, list, NumPy array, or another Tensor.</span>
</span><span id="__span-3-24"><a id="__codelineno-3-24" name="__codelineno-3-24" href="#__codelineno-3-24"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">build_ndarray</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</span><span id="__span-3-25"><a id="__codelineno-3-25" name="__codelineno-3-25" href="#__codelineno-3-25"></a>        <span class="c1"># dtype: The data type of the Tensor (default is np.float32)</span>
</span><span id="__span-3-26"><a id="__codelineno-3-26" name="__codelineno-3-26" href="#__codelineno-3-26"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="__span-3-27"><a id="__codelineno-3-27" name="__codelineno-3-27" href="#__codelineno-3-27"></a>        <span class="c1"># requires_grad: If True, the Tensor will track operations for gradient computation.</span>
</span><span id="__span-3-28"><a id="__codelineno-3-28" name="__codelineno-3-28" href="#__codelineno-3-28"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
</span><span id="__span-3-29"><a id="__codelineno-3-29" name="__codelineno-3-29" href="#__codelineno-3-29"></a>        <span class="c1"># dependencies: A list of Leaf objects representing dependencies in the computation graph.</span>
</span><span id="__span-3-30"><a id="__codelineno-3-30" name="__codelineno-3-30" href="#__codelineno-3-30"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="n">dependencies</span> <span class="ow">or</span> <span class="p">[]</span>
</span><span id="__span-3-31"><a id="__codelineno-3-31" name="__codelineno-3-31" href="#__codelineno-3-31"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="__span-3-32"><a id="__codelineno-3-32" name="__codelineno-3-32" href="#__codelineno-3-32"></a>
</span><span id="__span-3-33"><a id="__codelineno-3-33" name="__codelineno-3-33" href="#__codelineno-3-33"></a>        <span class="c1"># zero_grad(): Initializes the gradient to zero if requires_grad is True.</span>
</span><span id="__span-3-34"><a id="__codelineno-3-34" name="__codelineno-3-34" href="#__codelineno-3-34"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-3-35"><a id="__codelineno-3-35" name="__codelineno-3-35" href="#__codelineno-3-35"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="__span-3-36"><a id="__codelineno-3-36" name="__codelineno-3-36" href="#__codelineno-3-36"></a>
</span><span id="__span-3-37"><a id="__codelineno-3-37" name="__codelineno-3-37" href="#__codelineno-3-37"></a>        <span class="c1">############################</span>
</span><span id="__span-3-38"><a id="__codelineno-3-38" name="__codelineno-3-38" href="#__codelineno-3-38"></a>        <span class="c1"># Properties of the Tensor # </span>
</span><span id="__span-3-39"><a id="__codelineno-3-39" name="__codelineno-3-39" href="#__codelineno-3-39"></a>        <span class="c1">############################</span>
</span><span id="__span-3-40"><a id="__codelineno-3-40" name="__codelineno-3-40" href="#__codelineno-3-40"></a>
</span><span id="__span-3-41"><a id="__codelineno-3-41" name="__codelineno-3-41" href="#__codelineno-3-41"></a>        <span class="c1"># ndim: Returns the number of dimensions of the Tensor</span>
</span><span id="__span-3-42"><a id="__codelineno-3-42" name="__codelineno-3-42" href="#__codelineno-3-42"></a>        <span class="nd">@property</span>
</span><span id="__span-3-43"><a id="__codelineno-3-43" name="__codelineno-3-43" href="#__codelineno-3-43"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="__span-3-44"><a id="__codelineno-3-44" name="__codelineno-3-44" href="#__codelineno-3-44"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-45"><a id="__codelineno-3-45" name="__codelineno-3-45" href="#__codelineno-3-45"></a><span class="sd">            Returns the number of dimensions of the Tensor.</span>
</span><span id="__span-3-46"><a id="__codelineno-3-46" name="__codelineno-3-46" href="#__codelineno-3-46"></a>
</span><span id="__span-3-47"><a id="__codelineno-3-47" name="__codelineno-3-47" href="#__codelineno-3-47"></a><span class="sd">            Returns:</span>
</span><span id="__span-3-48"><a id="__codelineno-3-48" name="__codelineno-3-48" href="#__codelineno-3-48"></a><span class="sd">                int: Number of dimensions.</span>
</span><span id="__span-3-49"><a id="__codelineno-3-49" name="__codelineno-3-49" href="#__codelineno-3-49"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-3-50"><a id="__codelineno-3-50" name="__codelineno-3-50" href="#__codelineno-3-50"></a>
</span><span id="__span-3-51"><a id="__codelineno-3-51" name="__codelineno-3-51" href="#__codelineno-3-51"></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">ndim</span>
</span><span id="__span-3-52"><a id="__codelineno-3-52" name="__codelineno-3-52" href="#__codelineno-3-52"></a>
</span><span id="__span-3-53"><a id="__codelineno-3-53" name="__codelineno-3-53" href="#__codelineno-3-53"></a>        <span class="c1"># shape: Returns the shape of the Tensor</span>
</span><span id="__span-3-54"><a id="__codelineno-3-54" name="__codelineno-3-54" href="#__codelineno-3-54"></a>        <span class="nd">@property</span>
</span><span id="__span-3-55"><a id="__codelineno-3-55" name="__codelineno-3-55" href="#__codelineno-3-55"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span>
</span><span id="__span-3-56"><a id="__codelineno-3-56" name="__codelineno-3-56" href="#__codelineno-3-56"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-57"><a id="__codelineno-3-57" name="__codelineno-3-57" href="#__codelineno-3-57"></a><span class="sd">            Returns the shape of the Tensor.</span>
</span><span id="__span-3-58"><a id="__codelineno-3-58" name="__codelineno-3-58" href="#__codelineno-3-58"></a>
</span><span id="__span-3-59"><a id="__codelineno-3-59" name="__codelineno-3-59" href="#__codelineno-3-59"></a><span class="sd">            Returns:</span>
</span><span id="__span-3-60"><a id="__codelineno-3-60" name="__codelineno-3-60" href="#__codelineno-3-60"></a><span class="sd">                Tuple[int, ...]: Shape of the Tensor.</span>
</span><span id="__span-3-61"><a id="__codelineno-3-61" name="__codelineno-3-61" href="#__codelineno-3-61"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-3-62"><a id="__codelineno-3-62" name="__codelineno-3-62" href="#__codelineno-3-62"></a>
</span><span id="__span-3-63"><a id="__codelineno-3-63" name="__codelineno-3-63" href="#__codelineno-3-63"></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span>
</span><span id="__span-3-64"><a id="__codelineno-3-64" name="__codelineno-3-64" href="#__codelineno-3-64"></a>
</span><span id="__span-3-65"><a id="__codelineno-3-65" name="__codelineno-3-65" href="#__codelineno-3-65"></a>        <span class="c1"># size: Returns the total number of elements in the Tensor</span>
</span><span id="__span-3-66"><a id="__codelineno-3-66" name="__codelineno-3-66" href="#__codelineno-3-66"></a>        <span class="nd">@property</span>
</span><span id="__span-3-67"><a id="__codelineno-3-67" name="__codelineno-3-67" href="#__codelineno-3-67"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="__span-3-68"><a id="__codelineno-3-68" name="__codelineno-3-68" href="#__codelineno-3-68"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-69"><a id="__codelineno-3-69" name="__codelineno-3-69" href="#__codelineno-3-69"></a><span class="sd">            Returns the total number of elements in the Tensor.</span>
</span><span id="__span-3-70"><a id="__codelineno-3-70" name="__codelineno-3-70" href="#__codelineno-3-70"></a>
</span><span id="__span-3-71"><a id="__codelineno-3-71" name="__codelineno-3-71" href="#__codelineno-3-71"></a><span class="sd">            Returns:</span>
</span><span id="__span-3-72"><a id="__codelineno-3-72" name="__codelineno-3-72" href="#__codelineno-3-72"></a><span class="sd">                int: Total number of elements.</span>
</span><span id="__span-3-73"><a id="__codelineno-3-73" name="__codelineno-3-73" href="#__codelineno-3-73"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-3-74"><a id="__codelineno-3-74" name="__codelineno-3-74" href="#__codelineno-3-74"></a>
</span><span id="__span-3-75"><a id="__codelineno-3-75" name="__codelineno-3-75" href="#__codelineno-3-75"></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">size</span>
</span><span id="__span-3-76"><a id="__codelineno-3-76" name="__codelineno-3-76" href="#__codelineno-3-76"></a>
</span><span id="__span-3-77"><a id="__codelineno-3-77" name="__codelineno-3-77" href="#__codelineno-3-77"></a>        <span class="c1"># data: Gets or sets the underlying NumPy array</span>
</span><span id="__span-3-78"><a id="__codelineno-3-78" name="__codelineno-3-78" href="#__codelineno-3-78"></a>        <span class="nd">@property</span>
</span><span id="__span-3-79"><a id="__codelineno-3-79" name="__codelineno-3-79" href="#__codelineno-3-79"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-3-80"><a id="__codelineno-3-80" name="__codelineno-3-80" href="#__codelineno-3-80"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-81"><a id="__codelineno-3-81" name="__codelineno-3-81" href="#__codelineno-3-81"></a><span class="sd">            Gets the underlying NumPy array.</span>
</span><span id="__span-3-82"><a id="__codelineno-3-82" name="__codelineno-3-82" href="#__codelineno-3-82"></a>
</span><span id="__span-3-83"><a id="__codelineno-3-83" name="__codelineno-3-83" href="#__codelineno-3-83"></a><span class="sd">            Returns:</span>
</span><span id="__span-3-84"><a id="__codelineno-3-84" name="__codelineno-3-84" href="#__codelineno-3-84"></a><span class="sd">                np.ndarray: The data stored in the Tensor.</span>
</span><span id="__span-3-85"><a id="__codelineno-3-85" name="__codelineno-3-85" href="#__codelineno-3-85"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-3-86"><a id="__codelineno-3-86" name="__codelineno-3-86" href="#__codelineno-3-86"></a>
</span><span id="__span-3-87"><a id="__codelineno-3-87" name="__codelineno-3-87" href="#__codelineno-3-87"></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span>
</span><span id="__span-3-88"><a id="__codelineno-3-88" name="__codelineno-3-88" href="#__codelineno-3-88"></a>
</span><span id="__span-3-89"><a id="__codelineno-3-89" name="__codelineno-3-89" href="#__codelineno-3-89"></a>        <span class="nd">@data</span><span class="o">.</span><span class="n">setter</span>
</span><span id="__span-3-90"><a id="__codelineno-3-90" name="__codelineno-3-90" href="#__codelineno-3-90"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_data</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-3-91"><a id="__codelineno-3-91" name="__codelineno-3-91" href="#__codelineno-3-91"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-92"><a id="__codelineno-3-92" name="__codelineno-3-92" href="#__codelineno-3-92"></a><span class="sd">            Sets new data for the Tensor and resets gradients if required.</span>
</span><span id="__span-3-93"><a id="__codelineno-3-93" name="__codelineno-3-93" href="#__codelineno-3-93"></a>
</span><span id="__span-3-94"><a id="__codelineno-3-94" name="__codelineno-3-94" href="#__codelineno-3-94"></a><span class="sd">            Args:</span>
</span><span id="__span-3-95"><a id="__codelineno-3-95" name="__codelineno-3-95" href="#__codelineno-3-95"></a><span class="sd">                new_data (Data): The new data to be assigned to the Tensor.</span>
</span><span id="__span-3-96"><a id="__codelineno-3-96" name="__codelineno-3-96" href="#__codelineno-3-96"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-3-97"><a id="__codelineno-3-97" name="__codelineno-3-97" href="#__codelineno-3-97"></a>
</span><span id="__span-3-98"><a id="__codelineno-3-98" name="__codelineno-3-98" href="#__codelineno-3-98"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">build_ndarray</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="__span-3-99"><a id="__codelineno-3-99" name="__codelineno-3-99" href="#__codelineno-3-99"></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-3-100"><a id="__codelineno-3-100" name="__codelineno-3-100" href="#__codelineno-3-100"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="__span-3-101"><a id="__codelineno-3-101" name="__codelineno-3-101" href="#__codelineno-3-101"></a>
</span><span id="__span-3-102"><a id="__codelineno-3-102" name="__codelineno-3-102" href="#__codelineno-3-102"></a>        <span class="c1"># String Representation: Provides a string representation of the Tensor</span>
</span><span id="__span-3-103"><a id="__codelineno-3-103" name="__codelineno-3-103" href="#__codelineno-3-103"></a>        <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="__span-3-104"><a id="__codelineno-3-104" name="__codelineno-3-104" href="#__codelineno-3-104"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-105"><a id="__codelineno-3-105" name="__codelineno-3-105" href="#__codelineno-3-105"></a><span class="sd">            Returns a string representation of the Tensor.</span>
</span><span id="__span-3-106"><a id="__codelineno-3-106" name="__codelineno-3-106" href="#__codelineno-3-106"></a>
</span><span id="__span-3-107"><a id="__codelineno-3-107" name="__codelineno-3-107" href="#__codelineno-3-107"></a><span class="sd">            Returns:</span>
</span><span id="__span-3-108"><a id="__codelineno-3-108" name="__codelineno-3-108" href="#__codelineno-3-108"></a><span class="sd">                str: A string describing the Tensor.</span>
</span><span id="__span-3-109"><a id="__codelineno-3-109" name="__codelineno-3-109" href="#__codelineno-3-109"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-3-110"><a id="__codelineno-3-110" name="__codelineno-3-110" href="#__codelineno-3-110"></a>
</span><span id="__span-3-111"><a id="__codelineno-3-111" name="__codelineno-3-111" href="#__codelineno-3-111"></a>            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Tensor(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">, requires_grad=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="si">}</span><span class="s2">, shape=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">)&quot;</span>
</span><span id="__span-3-112"><a id="__codelineno-3-112" name="__codelineno-3-112" href="#__codelineno-3-112"></a>
</span><span id="__span-3-113"><a id="__codelineno-3-113" name="__codelineno-3-113" href="#__codelineno-3-113"></a>        <span class="c1"># Gradient Management - resets the gradient to zero</span>
</span><span id="__span-3-114"><a id="__codelineno-3-114" name="__codelineno-3-114" href="#__codelineno-3-114"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-3-115"><a id="__codelineno-3-115" name="__codelineno-3-115" href="#__codelineno-3-115"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-116"><a id="__codelineno-3-116" name="__codelineno-3-116" href="#__codelineno-3-116"></a><span class="sd">            Resets the gradient of the Tensor to zero.</span>
</span><span id="__span-3-117"><a id="__codelineno-3-117" name="__codelineno-3-117" href="#__codelineno-3-117"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-3-118"><a id="__codelineno-3-118" name="__codelineno-3-118" href="#__codelineno-3-118"></a>
</span><span id="__span-3-119"><a id="__codelineno-3-119" name="__codelineno-3-119" href="#__codelineno-3-119"></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-3-120"><a id="__codelineno-3-120" name="__codelineno-3-120" href="#__codelineno-3-120"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="__span-3-121"><a id="__codelineno-3-121" name="__codelineno-3-121" href="#__codelineno-3-121"></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="__span-3-122"><a id="__codelineno-3-122" name="__codelineno-3-122" href="#__codelineno-3-122"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span><span id="__span-3-123"><a id="__codelineno-3-123" name="__codelineno-3-123" href="#__codelineno-3-123"></a>
</span><span id="__span-3-124"><a id="__codelineno-3-124" name="__codelineno-3-124" href="#__codelineno-3-124"></a>        <span class="c1">##################</span>
</span><span id="__span-3-125"><a id="__codelineno-3-125" name="__codelineno-3-125" href="#__codelineno-3-125"></a>        <span class="c1"># Static Methods #</span>
</span><span id="__span-3-126"><a id="__codelineno-3-126" name="__codelineno-3-126" href="#__codelineno-3-126"></a>        <span class="c1">##################</span>
</span><span id="__span-3-127"><a id="__codelineno-3-127" name="__codelineno-3-127" href="#__codelineno-3-127"></a>
</span><span id="__span-3-128"><a id="__codelineno-3-128" name="__codelineno-3-128" href="#__codelineno-3-128"></a>        <span class="c1"># build_ndarray: Converts input data into a NumPy array.</span>
</span><span id="__span-3-129"><a id="__codelineno-3-129" name="__codelineno-3-129" href="#__codelineno-3-129"></a>        <span class="nd">@staticmethod</span>
</span><span id="__span-3-130"><a id="__codelineno-3-130" name="__codelineno-3-130" href="#__codelineno-3-130"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">build_ndarray</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">Data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-3-131"><a id="__codelineno-3-131" name="__codelineno-3-131" href="#__codelineno-3-131"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-3-132"><a id="__codelineno-3-132" name="__codelineno-3-132" href="#__codelineno-3-132"></a><span class="sd">            Converts input data into a NumPy array.</span>
</span><span id="__span-3-133"><a id="__codelineno-3-133" name="__codelineno-3-133" href="#__codelineno-3-133"></a>
</span><span id="__span-3-134"><a id="__codelineno-3-134" name="__codelineno-3-134" href="#__codelineno-3-134"></a><span class="sd">            Args:</span>
</span><span id="__span-3-135"><a id="__codelineno-3-135" name="__codelineno-3-135" href="#__codelineno-3-135"></a><span class="sd">                data (Data): The input data which could be a Tensor, NumPy array, or a list.</span>
</span><span id="__span-3-136"><a id="__codelineno-3-136" name="__codelineno-3-136" href="#__codelineno-3-136"></a><span class="sd">                dtype (dtype, optional): The target data type. Defaults to np.float32.</span>
</span><span id="__span-3-137"><a id="__codelineno-3-137" name="__codelineno-3-137" href="#__codelineno-3-137"></a>
</span><span id="__span-3-138"><a id="__codelineno-3-138" name="__codelineno-3-138" href="#__codelineno-3-138"></a><span class="sd">            Returns:</span>
</span><span id="__span-3-139"><a id="__codelineno-3-139" name="__codelineno-3-139" href="#__codelineno-3-139"></a><span class="sd">                np.ndarray: The converted NumPy array.</span>
</span><span id="__span-3-140"><a id="__codelineno-3-140" name="__codelineno-3-140" href="#__codelineno-3-140"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-3-141"><a id="__codelineno-3-141" name="__codelineno-3-141" href="#__codelineno-3-141"></a>
</span><span id="__span-3-142"><a id="__codelineno-3-142" name="__codelineno-3-142" href="#__codelineno-3-142"></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="__span-3-143"><a id="__codelineno-3-143" name="__codelineno-3-143" href="#__codelineno-3-143"></a>                <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="__span-3-144"><a id="__codelineno-3-144" name="__codelineno-3-144" href="#__codelineno-3-144"></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span><span id="__span-3-145"><a id="__codelineno-3-145" name="__codelineno-3-145" href="#__codelineno-3-145"></a>                <span class="k">return</span> <span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</span><span id="__span-3-146"><a id="__codelineno-3-146" name="__codelineno-3-146" href="#__codelineno-3-146"></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Example:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="n">t</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">t</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># Resets the gradient to zero</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>  <span class="c1"># Output: Tensor([1 2 3], requires_grad=True, shape=(3,))</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a><span class="n">t</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Output: (2, 2)</span>
</span></code></pre></div>
<h3 id="first-operation-transpose">First operation - Transpose<a class="headerlink" href="#first-operation-transpose" title="Permanent link">&para;</a></h3>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/xUkEKzq7XeQ?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Building PyTorch from Scratch: Tensor Operations, Transpose &amp; Backward Method Explained" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p>The backbone of the <code>Tensor</code> is basically useless - it just a mechanizm above the <code>numpy.array</code>. But now we can track the dependencies in the list and compute the gradient for the whole list of the dependencies!</p>
<p>I can show you the example of the foundamental tensor operation - the <code>transpose</code> method. The <code>transpose</code> operation reorders the dimensions of a tensor. If no axes are specified, automatically reverses the dimensions (<code>[::-1]</code>). For example, given a 3D tensor (shape <code>[2, 3, 4]</code>):</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># Output: (4, 3, 2)</span>
</span></code></pre></div>
<p>The dimensions are flipped:
<code>(2, 3, 4)  (4, 3, 2)</code>.</p>
<p>However, if specific axes are provided, permutes the dimensions accordingly.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># Changes order of dimensions</span>
</span></code></pre></div>
<p>In forward pass, <code>np.transpose(self.data, axes=axes)</code> swaps the tensor dimensions. In backward pass, we must apply the inverse permutation to propagate gradients correctly. If <code>axes=None</code>, the gradient reverses dimensions back with the transpose operation (default case). If <code>axes</code> is provided (i.e., custom permutation), we must invert that permutation. <code>np.argsort(axes)</code> finds the inverse order to revert the transpose. For example, if we permute <code>(0,1,2)  (1,2,0)</code>, we need the inverse mapping to undo it:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="n">axes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>   <span class="c1"># Forward permutation</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="n">inv_axes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span>  <span class="c1"># Output: (2, 0, 1)   This restores original order</span>
</span></code></pre></div>
<p><strong>Implementation:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>    <span class="c1"># Perform the transpose operation</span>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">)</span>
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>
</span><span id="__span-8-5"><a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>    <span class="c1"># Handle dependencies for autograd</span>
</span><span id="__span-8-6"><a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-8-7"><a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>
</span><span id="__span-8-8"><a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-8-9"><a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-8-10"><a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a>            <span class="c1"># Compute the inverse permutation of axes for the backward function</span>
</span><span id="__span-8-11"><a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a>            <span class="k">if</span> <span class="n">axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-8-12"><a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a>                <span class="c1"># Implicitly reverses dimensions</span>
</span><span id="__span-8-13"><a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a>                <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>  
</span><span id="__span-8-14"><a id="__codelineno-8-14" name="__codelineno-8-14" href="#__codelineno-8-14"></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="__span-8-15"><a id="__codelineno-8-15" name="__codelineno-8-15" href="#__codelineno-8-15"></a>                <span class="c1"># Compute the inverse permutation of axes</span>
</span><span id="__span-8-16"><a id="__codelineno-8-16" name="__codelineno-8-16" href="#__codelineno-8-16"></a>                <span class="n">inv_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">axes</span><span class="p">))</span>
</span><span id="__span-8-17"><a id="__codelineno-8-17" name="__codelineno-8-17" href="#__codelineno-8-17"></a>                <span class="c1"># Transpose the gradient back using the inverse permutation</span>
</span><span id="__span-8-18"><a id="__codelineno-8-18" name="__codelineno-8-18" href="#__codelineno-8-18"></a>                <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">inv_axes</span><span class="p">)</span>
</span><span id="__span-8-19"><a id="__codelineno-8-19" name="__codelineno-8-19" href="#__codelineno-8-19"></a>
</span><span id="__span-8-20"><a id="__codelineno-8-20" name="__codelineno-8-20" href="#__codelineno-8-20"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-8-21"><a id="__codelineno-8-21" name="__codelineno-8-21" href="#__codelineno-8-21"></a>            <span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span><span class="p">)</span>
</span><span id="__span-8-22"><a id="__codelineno-8-22" name="__codelineno-8-22" href="#__codelineno-8-22"></a>        <span class="p">)</span>
</span><span id="__span-8-23"><a id="__codelineno-8-23" name="__codelineno-8-23" href="#__codelineno-8-23"></a>
</span><span id="__span-8-24"><a id="__codelineno-8-24" name="__codelineno-8-24" href="#__codelineno-8-24"></a>    <span class="c1"># Return the new tensor with the transposed data</span>
</span><span id="__span-8-25"><a id="__codelineno-8-25" name="__codelineno-8-25" href="#__codelineno-8-25"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span>
</span><span id="__span-8-26"><a id="__codelineno-8-26" name="__codelineno-8-26" href="#__codelineno-8-26"></a>        <span class="n">output</span><span class="p">,</span>
</span><span id="__span-8-27"><a id="__codelineno-8-27" name="__codelineno-8-27" href="#__codelineno-8-27"></a>        <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
</span><span id="__span-8-28"><a id="__codelineno-8-28" name="__codelineno-8-28" href="#__codelineno-8-28"></a>        <span class="n">dependencies</span><span class="o">=</span><span class="n">dependencies</span>
</span><span id="__span-8-29"><a id="__codelineno-8-29" name="__codelineno-8-29" href="#__codelineno-8-29"></a>    <span class="p">)</span>
</span></code></pre></div>
<h3 id="the-backward-method">The <code>backward</code> Method<a class="headerlink" href="#the-backward-method" title="Permanent link">&para;</a></h3>
<p>The <code>transpose</code> method is the first operation that the <code>Tensor</code> class supports. To fully showcase the power of our simple implementation, let's implement the <code>backward</code> method.</p>
<p>The <code>backward</code> method implements <strong>reverse-mode automatic differentiation</strong> using the <strong>chain rule of calculus</strong>.</p>
<p><strong>Chain Rule:</strong></p>
<div class="arithmatex">\[
\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}
\]</div>
<p>If we have a function composition:  </p>
<div class="arithmatex">\[
f(x) = g(h(x))
\]</div>
<p>Then, by the chain rule:</p>
<div class="arithmatex">\[
f'(x) = g'(h(x)) \cdot h'(x)
\]</div>
<p>In the context of our <code>Tensor</code> class, this method is responsible for <strong>propagating gradients backward</strong> through a computation graph, ensuring that each node in the graph correctly accumulates its contribution to the final gradient.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="c1"># Backward Propagation: Computes gradients using backpropagation</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>    <span class="c1"># Step 1: Checking If Gradient Tracking is Enabled</span>
</span><span id="__span-9-4"><a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-9-5"><a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
</span><span id="__span-9-6"><a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a>            <span class="s2">&quot;Cannot call backward() on a tensor that does not require gradients. &quot;</span>
</span><span id="__span-9-7"><a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>            <span class="s2">&quot;If you need gradients, ensure that requires_grad=True when creating the tensor.&quot;</span>
</span><span id="__span-9-8"><a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>        <span class="p">)</span>
</span><span id="__span-9-9"><a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a>
</span><span id="__span-9-10"><a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a>    <span class="c1"># Step 2: Initializing the Gradient If Not Provided</span>
</span><span id="__span-9-11"><a id="__codelineno-9-11" name="__codelineno-9-11" href="#__codelineno-9-11"></a>    <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-9-12"><a id="__codelineno-9-12" name="__codelineno-9-12" href="#__codelineno-9-12"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">():</span>
</span><span id="__span-9-13"><a id="__codelineno-9-13" name="__codelineno-9-13" href="#__codelineno-9-13"></a>            <span class="c1"># The gradient of a scalar itself is 1</span>
</span><span id="__span-9-14"><a id="__codelineno-9-14" name="__codelineno-9-14" href="#__codelineno-9-14"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</span><span id="__span-9-15"><a id="__codelineno-9-15" name="__codelineno-9-15" href="#__codelineno-9-15"></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="__span-9-16"><a id="__codelineno-9-16" name="__codelineno-9-16" href="#__codelineno-9-16"></a>            <span class="c1"># If the tensor is not a scalar, `grad` must be provided explicitly.</span>
</span><span id="__span-9-17"><a id="__codelineno-9-17" name="__codelineno-9-17" href="#__codelineno-9-17"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Grad must be provided if tensor has shape&quot;</span><span class="p">)</span>
</span><span id="__span-9-18"><a id="__codelineno-9-18" name="__codelineno-9-18" href="#__codelineno-9-18"></a>
</span><span id="__span-9-19"><a id="__codelineno-9-19" name="__codelineno-9-19" href="#__codelineno-9-19"></a>    <span class="c1"># Step 3: Accumulating the Gradient</span>
</span><span id="__span-9-20"><a id="__codelineno-9-20" name="__codelineno-9-20" href="#__codelineno-9-20"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">grad</span>
</span><span id="__span-9-21"><a id="__codelineno-9-21" name="__codelineno-9-21" href="#__codelineno-9-21"></a>
</span><span id="__span-9-22"><a id="__codelineno-9-22" name="__codelineno-9-22" href="#__codelineno-9-22"></a>    <span class="c1"># The Chain Rule in Action</span>
</span><span id="__span-9-23"><a id="__codelineno-9-23" name="__codelineno-9-23" href="#__codelineno-9-23"></a>    <span class="k">for</span> <span class="n">dependency</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dependencies</span><span class="p">:</span>
</span><span id="__span-9-24"><a id="__codelineno-9-24" name="__codelineno-9-24" href="#__codelineno-9-24"></a>        <span class="c1"># Step 4: Applying the Chain Rule</span>
</span><span id="__span-9-25"><a id="__codelineno-9-25" name="__codelineno-9-25" href="#__codelineno-9-25"></a>        <span class="c1"># Propagates the gradient through the computation graph using `grad_fn`</span>
</span><span id="__span-9-26"><a id="__codelineno-9-26" name="__codelineno-9-26" href="#__codelineno-9-26"></a>        <span class="n">backward_grad</span> <span class="o">=</span> <span class="n">dependency</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span><span id="__span-9-27"><a id="__codelineno-9-27" name="__codelineno-9-27" href="#__codelineno-9-27"></a>        <span class="c1"># Step 5: Recursively Propagating Gradients</span>
</span><span id="__span-9-28"><a id="__codelineno-9-28" name="__codelineno-9-28" href="#__codelineno-9-28"></a>        <span class="n">dependency</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">backward_grad</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Step 3: Accumulating the Gradient</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+</span> <span class="n">grad</span>
</span></code></pre></div>
<p><strong>Mathematically, this represents:</strong> <span class="arithmatex">\(\text{self.grad} \gets \text{self.grad} + \text{grad}\)</span></p>
<p><strong>The Chain Rule in Action.</strong> This part implements the <strong>chain rule</strong>:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="k">for</span> <span class="n">dependency</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dependencies</span><span class="p">:</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>    <span class="n">backward_grad</span> <span class="o">=</span> <span class="n">dependency</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>    <span class="n">dependency</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">backward_grad</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Step 4: Applying the Chain Rule</strong></p>
<p>For each dependency (i.e., an operation that contributed to this tensor), we compute the gradient contribution using the chain rule:</p>
<div class="arithmatex">\[\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}\]</div>
<p>where <span class="arithmatex">\(dz/dy\)</span> is <code>grad</code> (gradient of the current tensor with respect to its output), <span class="arithmatex">\(dy/dx\)</span> is <code>dependency.grad_fn</code> (gradient of the dependency with respect to its input).</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="n">backward_grad</span> <span class="o">=</span> <span class="n">dependency</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span></code></pre></div>
<ul>
<li>This calls the stored gradient function (<code>grad_fn</code>) for this dependency.</li>
<li>It effectively computes: <span class="arithmatex">\(\text{backward_grad} = \frac{dz}{dy} \cdot \frac{dy}{dx}\)</span></li>
</ul>
<p><strong>Step 5: Recursively Propagating Gradients</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="n">dependency</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">backward_grad</span><span class="p">)</span>
</span></code></pre></div>
<ul>
<li>This recursively calls <code>backward</code> on the dependency.</li>
<li>It ensures that gradients are <strong>propagated through the entire computation graph</strong>.</li>
</ul>
<p><strong>Why Does This Work?</strong></p>
<p>When we compute gradients <strong>backward</strong>, we need to apply the <strong>chain rule</strong> in reverse order from the output back to the inputs.</p>
<ul>
<li><strong>Forward Pass:</strong> Builds a <strong>directed acyclic graph (DAG)</strong> where each tensor stores dependencies (operations that produced it).</li>
<li><strong>Backward Pass:</strong> Uses <strong>recursive calls</strong>, which implicitly use a <strong>stack</strong>, ensuring the <strong>last dependency is processed first</strong>.</li>
</ul>
<p>This is crucial because the last computed tensor (final output, e.g., loss) is at the top of the graph. Gradients flow backward through dependencies <strong>(from output to input).</strong> Recursive calls unwind the computation graph in the correct order <strong>(LIFO - Last In, First Out).</strong></p>
<h3 id="parameter-class-foundation-for-neural-network-parameters">Parameter Class: Foundation for Neural Network Parameters<a class="headerlink" href="#parameter-class-foundation-for-neural-network-parameters" title="Permanent link">&para;</a></h3>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/b16qKLmp2ro?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Building PyTorch: A Hands-On Guide to the Core Foundations of a Training Framework" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p>The <code>Parameter</code> class handles the initialization and management of model parameters like weights and biases in neural networks. This class simplifies defining and managing weights and biases, ensuring efficient model optimization. It supports multiple initialization methods like <a href="../../../01/14/weight-initialization-methods-in-neural-networks/">"xavier", "he", "normal", "uniform"</a> to set the right starting values, preventing issues like vanishing or exploding gradients. The class also ensures parameters are ready for optimization by setting <code>requires_grad=True</code> for backpropagation and includes a <code>gain</code> parameter to fine-tune initialization. </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>
</span><span id="__span-14-5"><a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="kn">from</span><span class="w"> </span><span class="nn">au2grad.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
</span><span id="__span-14-6"><a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>
</span><span id="__span-14-7"><a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a><span class="nb">type</span> <span class="n">InitMethod</span> <span class="o">=</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;xavier&quot;</span><span class="p">,</span> <span class="s2">&quot;he&quot;</span><span class="p">,</span> <span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">]</span>
</span><span id="__span-14-8"><a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a>
</span><span id="__span-14-9"><a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a>
</span><span id="__span-14-10"><a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a><span class="k">class</span><span class="w"> </span><span class="nc">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">):</span>
</span><span id="__span-14-11"><a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-14-12"><a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a><span class="sd">    Foundation for models parameters.</span>
</span><span id="__span-14-13"><a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-14-14"><a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a>
</span><span id="__span-14-15"><a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-14-16"><a id="__codelineno-14-16" name="__codelineno-14-16" href="#__codelineno-14-16"></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-14-17"><a id="__codelineno-14-17" name="__codelineno-14-17" href="#__codelineno-14-17"></a>        <span class="o">*</span><span class="n">shape</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="__span-14-18"><a id="__codelineno-14-18" name="__codelineno-14-18" href="#__codelineno-14-18"></a>        <span class="n">data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-14-19"><a id="__codelineno-14-19" name="__codelineno-14-19" href="#__codelineno-14-19"></a>        <span class="n">init_method</span><span class="p">:</span> <span class="n">InitMethod</span> <span class="o">=</span> <span class="s2">&quot;normal&quot;</span><span class="p">,</span>
</span><span id="__span-14-20"><a id="__codelineno-14-20" name="__codelineno-14-20" href="#__codelineno-14-20"></a>        <span class="n">gain</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
</span><span id="__span-14-21"><a id="__codelineno-14-21" name="__codelineno-14-21" href="#__codelineno-14-21"></a>        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
</span><span id="__span-14-22"><a id="__codelineno-14-22" name="__codelineno-14-22" href="#__codelineno-14-22"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-14-23"><a id="__codelineno-14-23" name="__codelineno-14-23" href="#__codelineno-14-23"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-14-24"><a id="__codelineno-14-24" name="__codelineno-14-24" href="#__codelineno-14-24"></a><span class="sd">        Initialize the parameter.</span>
</span><span id="__span-14-25"><a id="__codelineno-14-25" name="__codelineno-14-25" href="#__codelineno-14-25"></a>
</span><span id="__span-14-26"><a id="__codelineno-14-26" name="__codelineno-14-26" href="#__codelineno-14-26"></a><span class="sd">        Args:</span>
</span><span id="__span-14-27"><a id="__codelineno-14-27" name="__codelineno-14-27" href="#__codelineno-14-27"></a><span class="sd">            shape (tuple of int): The shape of the parameter.</span>
</span><span id="__span-14-28"><a id="__codelineno-14-28" name="__codelineno-14-28" href="#__codelineno-14-28"></a><span class="sd">            data (np.ndarray, optional): The data of the parameter. If not \</span>
</span><span id="__span-14-29"><a id="__codelineno-14-29" name="__codelineno-14-29" href="#__codelineno-14-29"></a><span class="sd">                provided, the parameter is initialized using the initialization \</span>
</span><span id="__span-14-30"><a id="__codelineno-14-30" name="__codelineno-14-30" href="#__codelineno-14-30"></a><span class="sd">                method.</span>
</span><span id="__span-14-31"><a id="__codelineno-14-31" name="__codelineno-14-31" href="#__codelineno-14-31"></a><span class="sd">            init_method (str): The initialization method. Defaults to &#39;normal&#39;. \</span>
</span><span id="__span-14-32"><a id="__codelineno-14-32" name="__codelineno-14-32" href="#__codelineno-14-32"></a><span class="sd">                Possible values are &#39;xavier&#39;, &#39;he&#39;, &#39;normal&#39;, &#39;uniform&#39;.</span>
</span><span id="__span-14-33"><a id="__codelineno-14-33" name="__codelineno-14-33" href="#__codelineno-14-33"></a><span class="sd">            gain (float): The gain for the initialization method. Defaults to 1.0.</span>
</span><span id="__span-14-34"><a id="__codelineno-14-34" name="__codelineno-14-34" href="#__codelineno-14-34"></a><span class="sd">            alpha (float): Slope for Leaky ReLU in &quot;he_leaky&quot; initialization.</span>
</span><span id="__span-14-35"><a id="__codelineno-14-35" name="__codelineno-14-35" href="#__codelineno-14-35"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-14-36"><a id="__codelineno-14-36" name="__codelineno-14-36" href="#__codelineno-14-36"></a>
</span><span id="__span-14-37"><a id="__codelineno-14-37" name="__codelineno-14-37" href="#__codelineno-14-37"></a>        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-14-38"><a id="__codelineno-14-38" name="__codelineno-14-38" href="#__codelineno-14-38"></a>            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialize</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">init_method</span><span class="p">,</span> <span class="n">gain</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span id="__span-14-39"><a id="__codelineno-14-39" name="__codelineno-14-39" href="#__codelineno-14-39"></a>
</span><span id="__span-14-40"><a id="__codelineno-14-40" name="__codelineno-14-40" href="#__codelineno-14-40"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-14-41"><a id="__codelineno-14-41" name="__codelineno-14-41" href="#__codelineno-14-41"></a>
</span><span id="__span-14-42"><a id="__codelineno-14-42" name="__codelineno-14-42" href="#__codelineno-14-42"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_initialize</span><span class="p">(</span>
</span><span id="__span-14-43"><a id="__codelineno-14-43" name="__codelineno-14-43" href="#__codelineno-14-43"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">method</span><span class="p">:</span> <span class="n">InitMethod</span> <span class="o">|</span> <span class="n">Any</span><span class="p">,</span> <span class="n">gain</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span>
</span><span id="__span-14-44"><a id="__codelineno-14-44" name="__codelineno-14-44" href="#__codelineno-14-44"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-14-45"><a id="__codelineno-14-45" name="__codelineno-14-45" href="#__codelineno-14-45"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-14-46"><a id="__codelineno-14-46" name="__codelineno-14-46" href="#__codelineno-14-46"></a><span class="sd">        Initialize the parameter data.</span>
</span><span id="__span-14-47"><a id="__codelineno-14-47" name="__codelineno-14-47" href="#__codelineno-14-47"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-14-48"><a id="__codelineno-14-48" name="__codelineno-14-48" href="#__codelineno-14-48"></a>
</span><span id="__span-14-49"><a id="__codelineno-14-49" name="__codelineno-14-49" href="#__codelineno-14-49"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span>
</span><span id="__span-14-50"><a id="__codelineno-14-50" name="__codelineno-14-50" href="#__codelineno-14-50"></a>
</span><span id="__span-14-51"><a id="__codelineno-14-51" name="__codelineno-14-51" href="#__codelineno-14-51"></a>        <span class="k">if</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s2">&quot;xavier&quot;</span><span class="p">:</span>
</span><span id="__span-14-52"><a id="__codelineno-14-52" name="__codelineno-14-52" href="#__codelineno-14-52"></a>            <span class="n">std</span> <span class="o">=</span> <span class="n">gain</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span id="__span-14-53"><a id="__codelineno-14-53" name="__codelineno-14-53" href="#__codelineno-14-53"></a>            <span class="k">return</span> <span class="n">std</span> <span class="o">*</span> <span class="n">weights</span>
</span><span id="__span-14-54"><a id="__codelineno-14-54" name="__codelineno-14-54" href="#__codelineno-14-54"></a>        <span class="k">if</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s2">&quot;he&quot;</span><span class="p">:</span>
</span><span id="__span-14-55"><a id="__codelineno-14-55" name="__codelineno-14-55" href="#__codelineno-14-55"></a>            <span class="n">std</span> <span class="o">=</span> <span class="n">gain</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span id="__span-14-56"><a id="__codelineno-14-56" name="__codelineno-14-56" href="#__codelineno-14-56"></a>            <span class="k">return</span> <span class="n">std</span> <span class="o">*</span> <span class="n">weights</span>
</span><span id="__span-14-57"><a id="__codelineno-14-57" name="__codelineno-14-57" href="#__codelineno-14-57"></a>        <span class="k">if</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s2">&quot;he_leaky&quot;</span><span class="p">:</span>
</span><span id="__span-14-58"><a id="__codelineno-14-58" name="__codelineno-14-58" href="#__codelineno-14-58"></a>            <span class="n">std</span> <span class="o">=</span> <span class="n">gain</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</span><span id="__span-14-59"><a id="__codelineno-14-59" name="__codelineno-14-59" href="#__codelineno-14-59"></a>            <span class="k">return</span> <span class="n">std</span> <span class="o">*</span> <span class="n">weights</span>
</span><span id="__span-14-60"><a id="__codelineno-14-60" name="__codelineno-14-60" href="#__codelineno-14-60"></a>        <span class="k">if</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
</span><span id="__span-14-61"><a id="__codelineno-14-61" name="__codelineno-14-61" href="#__codelineno-14-61"></a>            <span class="k">return</span> <span class="n">gain</span> <span class="o">*</span> <span class="n">weights</span>
</span><span id="__span-14-62"><a id="__codelineno-14-62" name="__codelineno-14-62" href="#__codelineno-14-62"></a>        <span class="k">if</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
</span><span id="__span-14-63"><a id="__codelineno-14-63" name="__codelineno-14-63" href="#__codelineno-14-63"></a>            <span class="k">return</span> <span class="n">gain</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
</span><span id="__span-14-64"><a id="__codelineno-14-64" name="__codelineno-14-64" href="#__codelineno-14-64"></a>
</span><span id="__span-14-65"><a id="__codelineno-14-65" name="__codelineno-14-65" href="#__codelineno-14-65"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown initialization method: </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="module-class-base-for-all-neural-network-modules">Module Class: Base for All Neural Network Modules<a class="headerlink" href="#module-class-base-for-all-neural-network-modules" title="Permanent link">&para;</a></h3>
<p>The <code>Module</code> class serves as the foundation for building neural network components, like layers and models. It defines essential methods like <code>forward</code>, which must be implemented in subclasses to process inputs and generate outputs. The class also provides functionality to switch between training (<code>train</code>) and evaluation (<code>eval</code>) modes, ensuring that all submodules are properly updated. </p>
<p>The <code>parameters</code> method recursively collects all parameters from the module and its submodules, while <code>zero_grad</code> resets gradients for all parameters. The <code>params_count</code> method returns the total number of parameters in the module. </p>
<p>In neural network development, the <code>Module</code> class simplifies handling the structure, state, and parameters of layers and models, making it easier to implement and train complex architectures.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="k">class</span><span class="w"> </span><span class="nc">Module</span><span class="p">:</span>
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-15-5"><a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a><span class="sd">    Base class for all modules.</span>
</span><span id="__span-15-6"><a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-15-7"><a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a>
</span><span id="__span-15-8"><a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-15-9"><a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
</span><span id="__span-15-10"><a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>
</span><span id="__span-15-11"><a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-15-12"><a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-15-13"><a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a><span class="sd">        Forward method to be implemented in children class</span>
</span><span id="__span-15-14"><a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a>
</span><span id="__span-15-15"><a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a><span class="sd">        Args:</span>
</span><span id="__span-15-16"><a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a><span class="sd">            input (Tensor or different object): Inputs</span>
</span><span id="__span-15-17"><a id="__codelineno-15-17" name="__codelineno-15-17" href="#__codelineno-15-17"></a>
</span><span id="__span-15-18"><a id="__codelineno-15-18" name="__codelineno-15-18" href="#__codelineno-15-18"></a><span class="sd">        Returns:</span>
</span><span id="__span-15-19"><a id="__codelineno-15-19" name="__codelineno-15-19" href="#__codelineno-15-19"></a><span class="sd">            Tensor: Outputs</span>
</span><span id="__span-15-20"><a id="__codelineno-15-20" name="__codelineno-15-20" href="#__codelineno-15-20"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-15-21"><a id="__codelineno-15-21" name="__codelineno-15-21" href="#__codelineno-15-21"></a>        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</span><span id="__span-15-22"><a id="__codelineno-15-22" name="__codelineno-15-22" href="#__codelineno-15-22"></a>
</span><span id="__span-15-23"><a id="__codelineno-15-23" name="__codelineno-15-23" href="#__codelineno-15-23"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
</span><span id="__span-15-24"><a id="__codelineno-15-24" name="__codelineno-15-24" href="#__codelineno-15-24"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-15-25"><a id="__codelineno-15-25" name="__codelineno-15-25" href="#__codelineno-15-25"></a><span class="sd">        Returns:</span>
</span><span id="__span-15-26"><a id="__codelineno-15-26" name="__codelineno-15-26" href="#__codelineno-15-26"></a><span class="sd">            List[Parameter]: Iterator of parameters</span>
</span><span id="__span-15-27"><a id="__codelineno-15-27" name="__codelineno-15-27" href="#__codelineno-15-27"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-15-28"><a id="__codelineno-15-28" name="__codelineno-15-28" href="#__codelineno-15-28"></a>
</span><span id="__span-15-29"><a id="__codelineno-15-29" name="__codelineno-15-29" href="#__codelineno-15-29"></a>        <span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-15-30"><a id="__codelineno-15-30" name="__codelineno-15-30" href="#__codelineno-15-30"></a>        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span id="__span-15-31"><a id="__codelineno-15-31" name="__codelineno-15-31" href="#__codelineno-15-31"></a>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
</span><span id="__span-15-32"><a id="__codelineno-15-32" name="__codelineno-15-32" href="#__codelineno-15-32"></a>                <span class="n">params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span id="__span-15-33"><a id="__codelineno-15-33" name="__codelineno-15-33" href="#__codelineno-15-33"></a>            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">Module</span><span class="p">):</span>
</span><span id="__span-15-34"><a id="__codelineno-15-34" name="__codelineno-15-34" href="#__codelineno-15-34"></a>                <span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</span><span id="__span-15-35"><a id="__codelineno-15-35" name="__codelineno-15-35" href="#__codelineno-15-35"></a>        <span class="k">return</span> <span class="n">params</span>
</span><span id="__span-15-36"><a id="__codelineno-15-36" name="__codelineno-15-36" href="#__codelineno-15-36"></a>
</span><span id="__span-15-37"><a id="__codelineno-15-37" name="__codelineno-15-37" href="#__codelineno-15-37"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-15-38"><a id="__codelineno-15-38" name="__codelineno-15-38" href="#__codelineno-15-38"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-15-39"><a id="__codelineno-15-39" name="__codelineno-15-39" href="#__codelineno-15-39"></a><span class="sd">        Zero the gradients of all parameters</span>
</span><span id="__span-15-40"><a id="__codelineno-15-40" name="__codelineno-15-40" href="#__codelineno-15-40"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-15-41"><a id="__codelineno-15-41" name="__codelineno-15-41" href="#__codelineno-15-41"></a>
</span><span id="__span-15-42"><a id="__codelineno-15-42" name="__codelineno-15-42" href="#__codelineno-15-42"></a>        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span><span id="__span-15-43"><a id="__codelineno-15-43" name="__codelineno-15-43" href="#__codelineno-15-43"></a>            <span class="n">param</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span id="__span-15-44"><a id="__codelineno-15-44" name="__codelineno-15-44" href="#__codelineno-15-44"></a>
</span><span id="__span-15-45"><a id="__codelineno-15-45" name="__codelineno-15-45" href="#__codelineno-15-45"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">params_count</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="__span-15-46"><a id="__codelineno-15-46" name="__codelineno-15-46" href="#__codelineno-15-46"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-15-47"><a id="__codelineno-15-47" name="__codelineno-15-47" href="#__codelineno-15-47"></a><span class="sd">        Returns:</span>
</span><span id="__span-15-48"><a id="__codelineno-15-48" name="__codelineno-15-48" href="#__codelineno-15-48"></a><span class="sd">            int: Number of parameters</span>
</span><span id="__span-15-49"><a id="__codelineno-15-49" name="__codelineno-15-49" href="#__codelineno-15-49"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-15-50"><a id="__codelineno-15-50" name="__codelineno-15-50" href="#__codelineno-15-50"></a>
</span><span id="__span-15-51"><a id="__codelineno-15-51" name="__codelineno-15-51" href="#__codelineno-15-51"></a>        <span class="n">num_parameters</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</span><span id="__span-15-52"><a id="__codelineno-15-52" name="__codelineno-15-52" href="#__codelineno-15-52"></a>        <span class="k">return</span> <span class="n">num_parameters</span>
</span></code></pre></div>
<h2 id="sequential-class-chaining-modules-in-order">Sequential Class: Chaining Modules in Order<a class="headerlink" href="#sequential-class-chaining-modules-in-order" title="Permanent link">&para;</a></h2>
<p>The <code>Sequential</code> class provides a simple way to stack multiple <code>Module</code> instances in a defined order. It automates the forward pass by passing the input tensor through each module sequentially, making it useful for building feedforward networks.  </p>
<p>The <code>parameters</code> method collects all parameters from the contained modules, ensuring easy access for optimization. The <code>forward</code> method iterates through the sequence, applying each module to the input.  </p>
<p>By structuring models in a linear fashion, <code>Sequential</code> simplifies neural network construction, reducing boilerplate and improving code clarity.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">Sequential</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">modules</span><span class="p">:</span> <span class="n">Module</span><span class="p">):</span>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">modules</span> <span class="o">=</span> <span class="n">modules</span>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]:</span>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a><span class="sd">        Returns a list of all parameters in the sequential module and its submodules.</span>
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-16-9"><a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a>        <span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-16-10"><a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a>        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
</span><span id="__span-16-11"><a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a>            <span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
</span><span id="__span-16-12"><a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>        <span class="k">return</span> <span class="n">params</span>
</span><span id="__span-16-13"><a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a>
</span><span id="__span-16-14"><a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-16-15"><a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-16-16"><a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a><span class="sd">        Passes the input through all modules in sequence.</span>
</span><span id="__span-16-17"><a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-16-18"><a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a>        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
</span><span id="__span-16-19"><a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-16-20"><a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a>        <span class="k">return</span> <span class="n">x</span>
</span></code></pre></div>
<h2 id="linear-layer-matrix-matrix-dot-product">Linear Layer: Matrix-Matrix Dot Product<a class="headerlink" href="#linear-layer-matrix-matrix-dot-product" title="Permanent link">&para;</a></h2>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/F9GH3nF4nkM?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Building PyTorch: Crafting Linear Layers and Parameter Counting in MicroTorch" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p>The mathematics behind the linear layer rely on matrix-matrix multiplication instead of vector operations. This allows efficient computation when processing multiple input samples simultaneously.</p>
<p>At layer <span class="arithmatex">\(i\)</span>, the transformation is defined as:  </p>
<div class="arithmatex">\[\tag{linear step}
\label{eq:linear_step}
A_i(\mathbf{X}) = \mathbf{X} \mathbf{W}_i + \mathbf{B}_i\]</div>
<p>Where <span class="arithmatex">\(\mathbf{X}\)</span> is the input matrix (batch of samples), <span class="arithmatex">\(\mathbf{W}_i\)</span> represents the weight matrix, and <span class="arithmatex">\(\mathbf{B}_i\)</span> is the bias matrix, typically broadcasted across the batch. The activation function <span class="arithmatex">\(\sigma\)</span> introduces non-linearity after this transformation.</p>
<p>For a single layer:  </p>
<div class="arithmatex">\[F_i(\mathbf{X}) = \sigma(A_i(\mathbf{X}))\]</div>
<p>where <span class="arithmatex">\(A_i(\mathbf{X})\)</span> is the linear transformation at layer <span class="arithmatex">\(i\)</span>.  </p>
<p>A deep neural network applies these transformations layer by layer, leading to the final output:  </p>
<div class="arithmatex">\[F(\mathbf{X}) = \sigma(A_L(\sigma(A_{L-1}(\dots \sigma(A_1(\mathbf{X})) \dots )))\]</div>
<p>Using <strong>functional composition</strong>, this process is compactly written as:  </p>
<div class="arithmatex">\[\tag{deep neural net}
\label{eq:deep_nn}
F(\mathbf{X}) = A_L \circ \sigma \circ A_{L-1} \circ \dots \circ \sigma \circ A_1 (\mathbf{X})\]</div>
<p>The <strong>forward pass</strong> computes these transformations, storing intermediate values for the <strong>backward pass</strong>. We can implement the <code>Linear</code> layer's <code>forward</code> method directly based on these equations.</p>
<p>The <code>tensor</code> implementation must support all necessary operations since it tracks dependencies within the gradient graph and accumulates gradients for the <code>backward</code> pass. This ensures automatic differentiation works seamlessly.</p>
<p>For the <code>Linear</code> layer, we only need to implement the <code>forward</code> step, as all gradient computations are handled within the <code>tensor</code> itself. However, inside <code>tensor</code>, we must implement <code>backward</code> for every operation used in the <code>Linear</code> layers <code>forward</code> step to enable proper gradient propagation during backpropagation.</p>
<p>As the step number one, let's implement the <strong>matrix dot product</strong> inside the tensor class.</p>
<h3 id="dot-product">Dot product<a class="headerlink" href="#dot-product" title="Permanent link">&para;</a></h3>
<p>Matrix multiplication follows the <strong>chain rule</strong> during backpropagation. Let's break it down step by step.</p>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/UQIGmdXZd_U?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Building PyTorch: Mastering Matrix Multiplication and Linear Layers in MicroTorch" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p><strong>1. Forward Pass (MatMul Operation)</strong></p>
<p>Given two tensors <strong>A</strong> and <strong>B</strong>, matrix multiplication is:</p>
<div class="arithmatex">\[Z = A \times B\]</div>
<p>Where <span class="arithmatex">\(A\)</span> has shape <span class="arithmatex">\((m, n)\)</span>, <span class="arithmatex">\(B\)</span> has shape <span class="arithmatex">\((n, p)\)</span> and the result <span class="arithmatex">\(Z\)</span> has shape <span class="arithmatex">\((m, p)\)</span>. You can find more details here: <a href="../../../../2024/11/20/matrix-multiplication-and-broadcasting/#exploring-matrix-multiplication-in-detail">Matrix Multiplication in Detail</a></p>
<p>This is implemented in the forward pass:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="c1"># Matrix multiplication</span>
</span><span id="__span-17-2"><a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a><span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">@</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
</span></code></pre></div>
<p><strong>2. Backward Pass (Gradients Computation)</strong></p>
<p>For backpropagation, we need to compute <span class="arithmatex">\(\frac{\partial L}{\partial A}\)</span> and <span class="arithmatex">\(\frac{\partial L}{\partial B}\)</span> using the chain rule.</p>
<p><strong>Gradient w.r.t. A</strong>
The gradient of the loss <span class="arithmatex">\(L\)</span> with respect to <span class="arithmatex">\(A\)</span> is given by:</p>
<div class="arithmatex">\[\frac{\partial L}{\partial A} = \frac{\partial L}{\partial Z} \times B^T\]</div>
<p>Where: <span class="arithmatex">\(\frac{\partial L}{\partial Z}\)</span> is the <strong>incoming gradient</strong> (represented as <code>grad</code>) and <span class="arithmatex">\(B^T\)</span> is the <strong>transpose of B</strong>.</p>
<p>This is implemented as:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-18-2"><a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-18-3"><a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>        <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="__span-18-4"><a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a>            <span class="k">return</span> <span class="n">grad</span> <span class="o">@</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># grad * B^T</span>
</span><span id="__span-18-5"><a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>  <span class="c1"># Handles 1D case</span>
</span></code></pre></div>
<ul>
<li>If <span class="arithmatex">\(B\)</span> is 2D, we use <code>b.data.swapaxes(-1, -2)</code> to compute <span class="arithmatex">\(B^T\)</span>.</li>
<li>If <span class="arithmatex">\(B\)</span> is 1D, we use <code>np.outer(grad, b.data.T)</code> to ensure correct shape.</li>
</ul>
<p><strong>Gradient w.r.t. B</strong></p>
<p>The gradient of the loss <span class="arithmatex">\(L\)</span> with respect to <span class="arithmatex">\(B\)</span> is given by:</p>
<div class="arithmatex">\[\frac{\partial L}{\partial B} = A^T \times \frac{\partial L}{\partial Z}\]</div>
<p>Where <span class="arithmatex">\(A^T\)</span> is the <strong>transpose of A</strong>.</p>
<p>This is implemented as:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-19-3"><a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a>        <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="__span-19-4"><a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a>            <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="n">grad</span>  <span class="c1"># A^T * grad</span>
</span><span id="__span-19-5"><a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>  <span class="c1"># Handles 1D case</span>
</span></code></pre></div>
<ul>
<li>If <span class="arithmatex">\(A\)</span> is 2D, we use <code>a.data.swapaxes(-1, -2)</code> to compute <span class="arithmatex">\(A^T\)</span>.</li>
<li>If <span class="arithmatex">\(A\)</span> is 1D, we use <code>np.outer(a.data.T, grad)</code>.</li>
</ul>
<p><strong>3. Why Do We Use <code>swapaxes(-1, -2)</code> Instead of <code>.T</code>?</strong></p>
<p><code>swapaxes(-1, -2)</code> is a <strong>general approach</strong> for transposing the last two dimensions. This ensures compatibility with <strong>both 2D matrices and higher-dimensional tensors</strong> (e.g., batches of matrices).</p>
<ul>
<li><code>.T</code> works <strong>only for 2D matrices</strong>, affecting all axes in higher dimensions.</li>
<li><code>swapaxes(-1, -2)</code> <strong>preserves batch and other leading dimensions</strong>, modifying only the last two.</li>
</ul>
<p>Example:</p>
<table>
<thead>
<tr>
<th>Shape of Tensor</th>
<th><code>.T</code> Output</th>
<th><code>swapaxes(-1, -2)</code> Output</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>(m, n)</code></td>
<td><code>(n, m)</code></td>
<td><code>(n, m)</code></td>
</tr>
<tr>
<td><code>(batch, m, n)</code></td>
<td><code>(n, m, batch)</code> (incorrect)</td>
<td><code>(batch, n, m)</code> (correct)</td>
</tr>
<tr>
<td><code>(batch, time, m, n)</code></td>
<td><code>(n, m, time, batch)</code> (incorrect)</td>
<td><code>(batch, time, n, m)</code> (correct)</td>
</tr>
</tbody>
</table>
<p><strong>4. How Does This Work in Backpropagation?</strong></p>
<ul>
<li><strong>During backpropagation</strong>, when a gradient <strong>flows back</strong> through the <code>matmul</code> operation, it needs to be <strong>properly propagated</strong> to both <code>A</code> and <code>B</code>.</li>
<li>The <strong>gradient computation follows the chain rule</strong> and ensures that the gradients for both matrices are computed <strong>correctly</strong>.</li>
</ul>
<p><strong>5. Summary</strong></p>
<p>Matrix multiplication follows the chain rule. The backward pass computes gradients for both <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span> using transposes. Uses <code>swapaxes(-1, -2)</code> to generalize for higher-dimensional cases.</p>
<table>
<thead>
<tr>
<th>Tensor</th>
<th>Gradient Formula</th>
<th>Code Implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(A\)</span></td>
<td><span class="arithmatex">\(\frac{\partial L}{\partial A} = \frac{\partial L}{\partial Z} \times B^T\)</span></td>
<td><code>grad @ b.data.swapaxes(-1, -2)</code></td>
</tr>
<tr>
<td><span class="arithmatex">\(B\)</span></td>
<td><span class="arithmatex">\(\frac{\partial L}{\partial B} = A^T \times \frac{\partial L}{\partial Z}\)</span></td>
<td><code>a.data.swapaxes(-1, -2) @ grad</code></td>
</tr>
</tbody>
</table>
<p><strong>Implementation</strong></p>
<p>To perform matrix-matrix multiplication, we first implement the static method <code>matmul</code> in the <code>Tensor</code> class. This method computes the dot product of two matrices <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span>, tracks dependencies, and sets up gradient functions for backpropagation.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="nd">@staticmethod</span>
</span><span id="__span-20-2"><a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-20-3"><a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-20-4"><a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a><span class="sd">    Static method to perform matrix multiplication of two tensors.</span>
</span><span id="__span-20-5"><a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a>
</span><span id="__span-20-6"><a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a><span class="sd">    Args:</span>
</span><span id="__span-20-7"><a id="__codelineno-20-7" name="__codelineno-20-7" href="#__codelineno-20-7"></a><span class="sd">        a (Tensor): First matrix.</span>
</span><span id="__span-20-8"><a id="__codelineno-20-8" name="__codelineno-20-8" href="#__codelineno-20-8"></a><span class="sd">        b (Tensor): Second matrix.</span>
</span><span id="__span-20-9"><a id="__codelineno-20-9" name="__codelineno-20-9" href="#__codelineno-20-9"></a>
</span><span id="__span-20-10"><a id="__codelineno-20-10" name="__codelineno-20-10" href="#__codelineno-20-10"></a><span class="sd">    Returns:</span>
</span><span id="__span-20-11"><a id="__codelineno-20-11" name="__codelineno-20-11" href="#__codelineno-20-11"></a><span class="sd">        Tensor: Resulting tensor with tracked dependencies.</span>
</span><span id="__span-20-12"><a id="__codelineno-20-12" name="__codelineno-20-12" href="#__codelineno-20-12"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-20-13"><a id="__codelineno-20-13" name="__codelineno-20-13" href="#__codelineno-20-13"></a>
</span><span id="__span-20-14"><a id="__codelineno-20-14" name="__codelineno-20-14" href="#__codelineno-20-14"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">@</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
</span><span id="__span-20-15"><a id="__codelineno-20-15" name="__codelineno-20-15" href="#__codelineno-20-15"></a>    <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span>
</span><span id="__span-20-16"><a id="__codelineno-20-16" name="__codelineno-20-16" href="#__codelineno-20-16"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-20-17"><a id="__codelineno-20-17" name="__codelineno-20-17" href="#__codelineno-20-17"></a>
</span><span id="__span-20-18"><a id="__codelineno-20-18" name="__codelineno-20-18" href="#__codelineno-20-18"></a>    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-20-19"><a id="__codelineno-20-19" name="__codelineno-20-19" href="#__codelineno-20-19"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-20-20"><a id="__codelineno-20-20" name="__codelineno-20-20" href="#__codelineno-20-20"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-20-21"><a id="__codelineno-20-21" name="__codelineno-20-21" href="#__codelineno-20-21"></a><span class="sd">            Backward gradient function for matmul with respect to a.</span>
</span><span id="__span-20-22"><a id="__codelineno-20-22" name="__codelineno-20-22" href="#__codelineno-20-22"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-20-23"><a id="__codelineno-20-23" name="__codelineno-20-23" href="#__codelineno-20-23"></a>
</span><span id="__span-20-24"><a id="__codelineno-20-24" name="__codelineno-20-24" href="#__codelineno-20-24"></a>            <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="__span-20-25"><a id="__codelineno-20-25" name="__codelineno-20-25" href="#__codelineno-20-25"></a>                <span class="k">return</span> <span class="n">grad</span> <span class="o">@</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-20-26"><a id="__codelineno-20-26" name="__codelineno-20-26" href="#__codelineno-20-26"></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="__span-20-27"><a id="__codelineno-20-27" name="__codelineno-20-27" href="#__codelineno-20-27"></a>
</span><span id="__span-20-28"><a id="__codelineno-20-28" name="__codelineno-20-28" href="#__codelineno-20-28"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-20-29"><a id="__codelineno-20-29" name="__codelineno-20-29" href="#__codelineno-20-29"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-20-30"><a id="__codelineno-20-30" name="__codelineno-20-30" href="#__codelineno-20-30"></a>                <span class="n">value</span><span class="o">=</span><span class="n">a</span><span class="p">,</span>
</span><span id="__span-20-31"><a id="__codelineno-20-31" name="__codelineno-20-31" href="#__codelineno-20-31"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span>
</span><span id="__span-20-32"><a id="__codelineno-20-32" name="__codelineno-20-32" href="#__codelineno-20-32"></a>            <span class="p">)</span>
</span><span id="__span-20-33"><a id="__codelineno-20-33" name="__codelineno-20-33" href="#__codelineno-20-33"></a>        <span class="p">)</span>
</span><span id="__span-20-34"><a id="__codelineno-20-34" name="__codelineno-20-34" href="#__codelineno-20-34"></a>
</span><span id="__span-20-35"><a id="__codelineno-20-35" name="__codelineno-20-35" href="#__codelineno-20-35"></a>    <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-20-36"><a id="__codelineno-20-36" name="__codelineno-20-36" href="#__codelineno-20-36"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-20-37"><a id="__codelineno-20-37" name="__codelineno-20-37" href="#__codelineno-20-37"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-20-38"><a id="__codelineno-20-38" name="__codelineno-20-38" href="#__codelineno-20-38"></a><span class="sd">            Backward gradient function for matmul with respect to b.</span>
</span><span id="__span-20-39"><a id="__codelineno-20-39" name="__codelineno-20-39" href="#__codelineno-20-39"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-20-40"><a id="__codelineno-20-40" name="__codelineno-20-40" href="#__codelineno-20-40"></a>
</span><span id="__span-20-41"><a id="__codelineno-20-41" name="__codelineno-20-41" href="#__codelineno-20-41"></a>            <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="__span-20-42"><a id="__codelineno-20-42" name="__codelineno-20-42" href="#__codelineno-20-42"></a>                <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">@</span> <span class="n">grad</span>
</span><span id="__span-20-43"><a id="__codelineno-20-43" name="__codelineno-20-43" href="#__codelineno-20-43"></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span><span id="__span-20-44"><a id="__codelineno-20-44" name="__codelineno-20-44" href="#__codelineno-20-44"></a>
</span><span id="__span-20-45"><a id="__codelineno-20-45" name="__codelineno-20-45" href="#__codelineno-20-45"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-20-46"><a id="__codelineno-20-46" name="__codelineno-20-46" href="#__codelineno-20-46"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-20-47"><a id="__codelineno-20-47" name="__codelineno-20-47" href="#__codelineno-20-47"></a>                <span class="n">value</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
</span><span id="__span-20-48"><a id="__codelineno-20-48" name="__codelineno-20-48" href="#__codelineno-20-48"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span>
</span><span id="__span-20-49"><a id="__codelineno-20-49" name="__codelineno-20-49" href="#__codelineno-20-49"></a>            <span class="p">)</span>
</span><span id="__span-20-50"><a id="__codelineno-20-50" name="__codelineno-20-50" href="#__codelineno-20-50"></a>        <span class="p">)</span>
</span><span id="__span-20-51"><a id="__codelineno-20-51" name="__codelineno-20-51" href="#__codelineno-20-51"></a>
</span><span id="__span-20-52"><a id="__codelineno-20-52" name="__codelineno-20-52" href="#__codelineno-20-52"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="ensuring-data-consistency-with-data_gate">Ensuring Data Consistency with <code>data_gate</code><a class="headerlink" href="#ensuring-data-consistency-with-data_gate" title="Permanent link">&para;</a></h3>
<p>When performing matrix multiplication or other tensor operations, we must ensure that the data types are compatible. For example, attempting to multiply a <code>Tensor</code> with a <code>numpy.ndarray</code> directly may lead to unexpected behavior. To prevent such issues, we can create a <code>data_gate</code> method that automatically converts inputs to the <code>Tensor</code> type if they are not already.  </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="nd">@staticmethod</span>
</span><span id="__span-21-2"><a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">data_gate</span><span class="p">(</span><span class="n">data_object</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-21-3"><a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-21-4"><a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a><span class="sd">    Ensures the input is a Tensor.</span>
</span><span id="__span-21-5"><a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a>
</span><span id="__span-21-6"><a id="__codelineno-21-6" name="__codelineno-21-6" href="#__codelineno-21-6"></a><span class="sd">    This method checks if the provided object is already a Tensor. </span>
</span><span id="__span-21-7"><a id="__codelineno-21-7" name="__codelineno-21-7" href="#__codelineno-21-7"></a><span class="sd">    If not, it converts it into a Tensor before proceeding with operations.</span>
</span><span id="__span-21-8"><a id="__codelineno-21-8" name="__codelineno-21-8" href="#__codelineno-21-8"></a>
</span><span id="__span-21-9"><a id="__codelineno-21-9" name="__codelineno-21-9" href="#__codelineno-21-9"></a><span class="sd">    Args:</span>
</span><span id="__span-21-10"><a id="__codelineno-21-10" name="__codelineno-21-10" href="#__codelineno-21-10"></a><span class="sd">        data_object (Data): The input data, which can be a Tensor or a compatible type.</span>
</span><span id="__span-21-11"><a id="__codelineno-21-11" name="__codelineno-21-11" href="#__codelineno-21-11"></a>
</span><span id="__span-21-12"><a id="__codelineno-21-12" name="__codelineno-21-12" href="#__codelineno-21-12"></a><span class="sd">    Returns:</span>
</span><span id="__span-21-13"><a id="__codelineno-21-13" name="__codelineno-21-13" href="#__codelineno-21-13"></a><span class="sd">        Tensor: The input converted to a Tensor if necessary.</span>
</span><span id="__span-21-14"><a id="__codelineno-21-14" name="__codelineno-21-14" href="#__codelineno-21-14"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-21-15"><a id="__codelineno-21-15" name="__codelineno-21-15" href="#__codelineno-21-15"></a>    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data_object</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="__span-21-16"><a id="__codelineno-21-16" name="__codelineno-21-16" href="#__codelineno-21-16"></a>        <span class="k">return</span> <span class="n">data_object</span>  <span class="c1"># Return as-is if already a Tensor</span>
</span><span id="__span-21-17"><a id="__codelineno-21-17" name="__codelineno-21-17" href="#__codelineno-21-17"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">data_object</span><span class="p">)</span>  <span class="c1"># Convert to Tensor if not</span>
</span></code></pre></div>
<p>This function acts as a safeguard, ensuring that all operations are performed with the correct data type. Simple but effective, preventing potential errors when working with mixed data types.</p>
<h3 id="matmul-operator">Matmul operator <code>@</code><a class="headerlink" href="#matmul-operator" title="Permanent link">&para;</a></h3>
<p>Next, we define the dot method as the standard interface for matrix dot products. To enable the <code>@</code> operator for matrix multiplication, we overload the <code>__matmul__</code> method.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-22-3"><a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a><span class="sd">    Perform matrix dot product with another tensor or data.</span>
</span><span id="__span-22-4"><a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a>
</span><span id="__span-22-5"><a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a><span class="sd">    Args:</span>
</span><span id="__span-22-6"><a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a><span class="sd">        other (Data): The other operand.</span>
</span><span id="__span-22-7"><a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a>
</span><span id="__span-22-8"><a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a><span class="sd">    Returns:</span>
</span><span id="__span-22-9"><a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a><span class="sd">        Tensor: Result of the dot product.</span>
</span><span id="__span-22-10"><a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-22-11"><a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a>
</span><span id="__span-22-12"><a id="__codelineno-22-12" name="__codelineno-22-12" href="#__codelineno-22-12"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>
</span><span id="__span-22-13"><a id="__codelineno-22-13" name="__codelineno-22-13" href="#__codelineno-22-13"></a>
</span><span id="__span-22-14"><a id="__codelineno-22-14" name="__codelineno-22-14" href="#__codelineno-22-14"></a>
</span><span id="__span-22-15"><a id="__codelineno-22-15" name="__codelineno-22-15" href="#__codelineno-22-15"></a><span class="k">def</span><span class="w"> </span><span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-22-16"><a id="__codelineno-22-16" name="__codelineno-22-16" href="#__codelineno-22-16"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-22-17"><a id="__codelineno-22-17" name="__codelineno-22-17" href="#__codelineno-22-17"></a><span class="sd">    Overload the `@` operator for matrix multiplication.</span>
</span><span id="__span-22-18"><a id="__codelineno-22-18" name="__codelineno-22-18" href="#__codelineno-22-18"></a>
</span><span id="__span-22-19"><a id="__codelineno-22-19" name="__codelineno-22-19" href="#__codelineno-22-19"></a><span class="sd">    Args:</span>
</span><span id="__span-22-20"><a id="__codelineno-22-20" name="__codelineno-22-20" href="#__codelineno-22-20"></a><span class="sd">        other (Data): The other operand.</span>
</span><span id="__span-22-21"><a id="__codelineno-22-21" name="__codelineno-22-21" href="#__codelineno-22-21"></a>
</span><span id="__span-22-22"><a id="__codelineno-22-22" name="__codelineno-22-22" href="#__codelineno-22-22"></a><span class="sd">    Returns:</span>
</span><span id="__span-22-23"><a id="__codelineno-22-23" name="__codelineno-22-23" href="#__codelineno-22-23"></a><span class="sd">        Tensor: Result of the matrix multiplication.</span>
</span><span id="__span-22-24"><a id="__codelineno-22-24" name="__codelineno-22-24" href="#__codelineno-22-24"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-22-25"><a id="__codelineno-22-25" name="__codelineno-22-25" href="#__codelineno-22-25"></a>
</span><span id="__span-22-26"><a id="__codelineno-22-26" name="__codelineno-22-26" href="#__codelineno-22-26"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span></code></pre></div>
<p>This implementation ensures that the <code>Tensor</code> class handles both forward and backward computations for matrix multiplication, integrating smoothly into the automatic differentiation framework.</p>
<h2 id="linear-layer-implementation">Linear Layer Implementation<a class="headerlink" href="#linear-layer-implementation" title="Permanent link">&para;</a></h2>
<p>The <code>Linear</code> layer applies a linear transformation to the input tensor, mapping it from <code>in_features</code> to <code>out_features</code>. It consists of learnable weight parameters and an optional bias.  </p>
<p>During initialization, the weights are assigned based on the specified <code>init_method</code>, and the bias is included if enabled. The <code>forward</code> method performs matrix multiplication between the input tensor and the weight matrix. If the bias is present, it is reshaped accordingly and added to the output.  </p>
<p>To support both 2D and 3D inputs, the implementation ensures that matrix dimensions align properly before performing operations. The result is a transformed tensor, ready for further processing in the neural network.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">Linear</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-23-2"><a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="__span-23-3"><a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-23-4"><a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a>        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="__span-23-5"><a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a>        <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="__span-23-6"><a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a>        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="__span-23-7"><a id="__codelineno-23-7" name="__codelineno-23-7" href="#__codelineno-23-7"></a>        <span class="n">init_method</span><span class="p">:</span> <span class="n">InitMethod</span> <span class="o">=</span> <span class="s2">&quot;xavier&quot;</span><span class="p">,</span>
</span><span id="__span-23-8"><a id="__codelineno-23-8" name="__codelineno-23-8" href="#__codelineno-23-8"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-23-9"><a id="__codelineno-23-9" name="__codelineno-23-9" href="#__codelineno-23-9"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-23-10"><a id="__codelineno-23-10" name="__codelineno-23-10" href="#__codelineno-23-10"></a>
</span><span id="__span-23-11"><a id="__codelineno-23-11" name="__codelineno-23-11" href="#__codelineno-23-11"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
</span><span id="__span-23-12"><a id="__codelineno-23-12" name="__codelineno-23-12" href="#__codelineno-23-12"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
</span><span id="__span-23-13"><a id="__codelineno-23-13" name="__codelineno-23-13" href="#__codelineno-23-13"></a>
</span><span id="__span-23-14"><a id="__codelineno-23-14" name="__codelineno-23-14" href="#__codelineno-23-14"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">)</span>
</span><span id="__span-23-15"><a id="__codelineno-23-15" name="__codelineno-23-15" href="#__codelineno-23-15"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span>
</span><span id="__span-23-16"><a id="__codelineno-23-16" name="__codelineno-23-16" href="#__codelineno-23-16"></a>
</span><span id="__span-23-17"><a id="__codelineno-23-17" name="__codelineno-23-17" href="#__codelineno-23-17"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-23-18"><a id="__codelineno-23-18" name="__codelineno-23-18" href="#__codelineno-23-18"></a>        <span class="c1"># Check dimensions of input tensors</span>
</span><span id="__span-23-19"><a id="__codelineno-23-19" name="__codelineno-23-19" href="#__codelineno-23-19"></a>        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Input must be 2D or 3D Tensor! x.ndim=</span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-23-20"><a id="__codelineno-23-20" name="__codelineno-23-20" href="#__codelineno-23-20"></a>
</span><span id="__span-23-21"><a id="__codelineno-23-21" name="__codelineno-23-21" href="#__codelineno-23-21"></a>        <span class="c1"># Check if the last dimension of input matches in_features</span>
</span><span id="__span-23-22"><a id="__codelineno-23-22" name="__codelineno-23-22" href="#__codelineno-23-22"></a>        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">:</span>
</span><span id="__span-23-23"><a id="__codelineno-23-23" name="__codelineno-23-23" href="#__codelineno-23-23"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span id="__span-23-24"><a id="__codelineno-23-24" name="__codelineno-23-24" href="#__codelineno-23-24"></a>                <span class="sa">f</span><span class="s2">&quot;Last dimension of input: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> does not match in_features: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-23-25"><a id="__codelineno-23-25" name="__codelineno-23-25" href="#__codelineno-23-25"></a>            <span class="p">)</span>
</span><span id="__span-23-26"><a id="__codelineno-23-26" name="__codelineno-23-26" href="#__codelineno-23-26"></a>
</span><span id="__span-23-27"><a id="__codelineno-23-27" name="__codelineno-23-27" href="#__codelineno-23-27"></a>        <span class="c1"># Compute matrix multiplication: x @ weight^T</span>
</span><span id="__span-23-28"><a id="__codelineno-23-28" name="__codelineno-23-28" href="#__codelineno-23-28"></a>        <span class="n">output</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span>
</span><span id="__span-23-29"><a id="__codelineno-23-29" name="__codelineno-23-29" href="#__codelineno-23-29"></a>
</span><span id="__span-23-30"><a id="__codelineno-23-30" name="__codelineno-23-30" href="#__codelineno-23-30"></a>        <span class="c1"># Add the bias directly. Broadcasting will handle it!</span>
</span><span id="__span-23-31"><a id="__codelineno-23-31" name="__codelineno-23-31" href="#__codelineno-23-31"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-23-32"><a id="__codelineno-23-32" name="__codelineno-23-32" href="#__codelineno-23-32"></a>            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</span><span id="__span-23-33"><a id="__codelineno-23-33" name="__codelineno-23-33" href="#__codelineno-23-33"></a>
</span><span id="__span-23-34"><a id="__codelineno-23-34" name="__codelineno-23-34" href="#__codelineno-23-34"></a>        <span class="k">return</span> <span class="n">output</span>
</span></code></pre></div>
<p>But here, we have the <code>+</code> operation: <code>output = output + self.bias</code>, which is not implemented inside the <code>Tensor</code> class. To make the <code>Linear</code> implementation work, we need to handling this operation correctly.</p>
<h2 id="broadcasting-in-backward-mode">Broadcasting in backward mode<a class="headerlink" href="#broadcasting-in-backward-mode" title="Permanent link">&para;</a></h2>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/pdZij4qj2WQ?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Building PyTorch: Adding Broadcasting and Addition Operations to MicroTorch" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p>Gradients must be correctly propagated across dimensions that may differ between tensors. <em>Broadcasting</em> allows tensors of different shapes to interact, but when computing gradients, we need to handle these differences in dimensions.</p>
<p>Operations like <code>+</code>, <code>-</code>, or <code>*</code> are straightforward to implement in the <code>forward</code> pass. However, during the <code>backward</code> pass, we must account for broadcasting rules when computing gradients. To handle this, we need to introduce an additional method in the <code>Tensor</code> class.</p>
<p><strong>Broadcasting</strong> is a method used by most scientific computing libraries like PyTorch or NumPy to handle operations between arrays of different shapes. <strong>Broadcasting Rules</strong> - when performing an operation, compare the dimensions from right to left side. If the dimensions do not match, the shape with a size of 1 is stretched to match the other shape.</p>
<p>In our example, if we were to broadcast:</p>
<ul>
<li><strong>Matrix <code>A</code></strong> with shape <code>(3, 1)</code> (our <code>X * W</code> result)</li>
<li><strong>Matrix <code>B</code></strong> with shape <code>(1, 4)</code> (our bias <code>B</code> expanded to match <code>X * W</code> for broadcasting)</li>
</ul>
<p>For the addition, we:</p>
<ol>
<li>Stretch <code>A</code> to match <code>B</code> by duplicating the column four times.</li>
<li>Stretch <code>B</code> to match <code>A</code> by duplicating the row three times.</li>
</ol>
<p>Thus, both matrices would be aligned to have dimensions of <code>3x4</code>, allowing for element-wise addition.</p>
<p>Let's see this in code using NumPy:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-24-1"><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-24-2"><a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a>
</span><span id="__span-24-3"><a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a><span class="c1"># Define array A with shape (3, 1)</span>
</span><span id="__span-24-4"><a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span><span id="__span-24-5"><a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a>    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
</span><span id="__span-24-6"><a id="__codelineno-24-6" name="__codelineno-24-6" href="#__codelineno-24-6"></a>    <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
</span><span id="__span-24-7"><a id="__codelineno-24-7" name="__codelineno-24-7" href="#__codelineno-24-7"></a>    <span class="p">[</span><span class="mi">3</span><span class="p">],</span>
</span><span id="__span-24-8"><a id="__codelineno-24-8" name="__codelineno-24-8" href="#__codelineno-24-8"></a><span class="p">])</span>
</span><span id="__span-24-9"><a id="__codelineno-24-9" name="__codelineno-24-9" href="#__codelineno-24-9"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Array A shape: </span><span class="si">{</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-24-10"><a id="__codelineno-24-10" name="__codelineno-24-10" href="#__codelineno-24-10"></a>
</span><span id="__span-24-11"><a id="__codelineno-24-11" name="__codelineno-24-11" href="#__codelineno-24-11"></a><span class="c1"># Define array B with shape (1, 4)</span>
</span><span id="__span-24-12"><a id="__codelineno-24-12" name="__codelineno-24-12" href="#__codelineno-24-12"></a><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span><span id="__span-24-13"><a id="__codelineno-24-13" name="__codelineno-24-13" href="#__codelineno-24-13"></a>    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
</span><span id="__span-24-14"><a id="__codelineno-24-14" name="__codelineno-24-14" href="#__codelineno-24-14"></a><span class="p">])</span>
</span><span id="__span-24-15"><a id="__codelineno-24-15" name="__codelineno-24-15" href="#__codelineno-24-15"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Array B shape: </span><span class="si">{</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-24-16"><a id="__codelineno-24-16" name="__codelineno-24-16" href="#__codelineno-24-16"></a>
</span><span id="__span-24-17"><a id="__codelineno-24-17" name="__codelineno-24-17" href="#__codelineno-24-17"></a><span class="c1"># Perform broadcasting addition</span>
</span><span id="__span-24-18"><a id="__codelineno-24-18" name="__codelineno-24-18" href="#__codelineno-24-18"></a><span class="n">result</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">B</span>
</span><span id="__span-24-19"><a id="__codelineno-24-19" name="__codelineno-24-19" href="#__codelineno-24-19"></a>
</span><span id="__span-24-20"><a id="__codelineno-24-20" name="__codelineno-24-20" href="#__codelineno-24-20"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A + B result: &quot;</span><span class="p">)</span>
</span><span id="__span-24-21"><a id="__codelineno-24-21" name="__codelineno-24-21" href="#__codelineno-24-21"></a><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span><span id="__span-24-22"><a id="__codelineno-24-22" name="__codelineno-24-22" href="#__codelineno-24-22"></a>
</span><span id="__span-24-23"><a id="__codelineno-24-23" name="__codelineno-24-23" href="#__codelineno-24-23"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Result of A + B shape: </span><span class="si">{</span><span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p><em>Output:</em></p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-25-1"><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a>Array A shape: (3, 1)
</span><span id="__span-25-2"><a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>Array B shape: (1, 4)
</span><span id="__span-25-3"><a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a>A + B result: 
</span><span id="__span-25-4"><a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>[[2 3 4 5]
</span><span id="__span-25-5"><a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a> [3 4 5 6]
</span><span id="__span-25-6"><a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a> [4 5 6 7]]
</span><span id="__span-25-7"><a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a>Result of A + B shape: (3, 4)
</span></code></pre></div>
<p>Matrix multiplication example:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-26-1"><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a><span class="c1"># Broadcasting the same for the matrix multiplication</span>
</span><span id="__span-26-2"><a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a><span class="n">matmul</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">B</span>
</span><span id="__span-26-3"><a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Matmul A @ B shape: </span><span class="si">{</span><span class="n">matmul</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-26-4"><a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a>
</span><span id="__span-26-5"><a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Matmul result: &quot;</span><span class="p">)</span>
</span><span id="__span-26-6"><a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">matmul</span><span class="p">)</span>
</span></code></pre></div>
<p><em>Output:</em></p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-27-1"><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a>Matmul A @ B shape: (3, 4)
</span><span id="__span-27-2"><a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a>Matmul result: 
</span><span id="__span-27-3"><a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a>[[ 1  2  3  4]
</span><span id="__span-27-4"><a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a> [ 2  4  6  8]
</span><span id="__span-27-5"><a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a> [ 3  6  9 12]]
</span></code></pre></div>
<p>The <code>bkwd_broadcast</code> method ensures gradients are correctly summed across broadcasted dimensions in <code>backward</code> mode. When tensors of different shapes interact, broadcasting aligns them by repeating elements. The method handles <em>gradient propagation</em> by summing over the extra dimensions created by broadcasting, ensuring consistency and preventing errors in gradient calculations. This is crucial for element-wise operations with mismatched tensor shapes, maintaining correct backpropagation.</p>
<p>In <strong>Scenario 1</strong>, <code>b</code> has shape <code>(1,)</code>, meaning it was <strong>expanded to match both dimensions</strong> of <code>a</code>. The backward pass gives <code>grad_c</code> shape <code>(2,2)</code>, but <code>b</code> originally had no explicit dimensions. We <strong>sum over all extra axes <code>(0,1)</code></strong> (<code>keepdims=False</code>) to return to shape <code>(1,)</code>.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-28-1"><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> 
</span><span id="__span-28-2"><a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a>              <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>  <span class="c1"># Shape: (2, 2)</span>
</span><span id="__span-28-3"><a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a>
</span><span id="__span-28-4"><a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>      <span class="c1"># Shape: (1,)  (Broadcasted across both axis)</span>
</span><span id="__span-28-5"><a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a>
</span><span id="__span-28-6"><a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a><span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</span><span id="__span-28-7"><a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-28-8"><a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a>
</span><span id="__span-28-9"><a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a><span class="n">grad_c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</span><span id="__span-28-10"><a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;grad_c: </span><span class="si">{</span><span class="n">grad_c</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-28-11"><a id="__codelineno-28-11" name="__codelineno-28-11" href="#__codelineno-28-11"></a>
</span><span id="__span-28-12"><a id="__codelineno-28-12" name="__codelineno-28-12" href="#__codelineno-28-12"></a><span class="c1"># Since `a` was not broadcasted, the gradient just passes through</span>
</span><span id="__span-28-13"><a id="__codelineno-28-13" name="__codelineno-28-13" href="#__codelineno-28-13"></a><span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_c</span>
</span><span id="__span-28-14"><a id="__codelineno-28-14" name="__codelineno-28-14" href="#__codelineno-28-14"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;grad_a: </span><span class="si">{</span><span class="n">grad_a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-28-15"><a id="__codelineno-28-15" name="__codelineno-28-15" href="#__codelineno-28-15"></a>
</span><span id="__span-28-16"><a id="__codelineno-28-16" name="__codelineno-28-16" href="#__codelineno-28-16"></a><span class="c1"># Since `b` was expanded to match both dimensions</span>
</span><span id="__span-28-17"><a id="__codelineno-28-17" name="__codelineno-28-17" href="#__codelineno-28-17"></a><span class="c1"># We **sum over all extra axes `(0,1)`** (`keepdims=False`)</span>
</span><span id="__span-28-18"><a id="__codelineno-28-18" name="__codelineno-28-18" href="#__codelineno-28-18"></a><span class="c1"># to return to shape `(1,)`.</span>
</span><span id="__span-28-19"><a id="__codelineno-28-19" name="__codelineno-28-19" href="#__codelineno-28-19"></a><span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_c</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-28-20"><a id="__codelineno-28-20" name="__codelineno-28-20" href="#__codelineno-28-20"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;grad_b: </span><span class="si">{</span><span class="n">grad_b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p><em>Output:</em></p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-29-1"><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a>c: [[11 12]
</span><span id="__span-29-2"><a id="__codelineno-29-2" name="__codelineno-29-2" href="#__codelineno-29-2"></a>    [13 14]]
</span><span id="__span-29-3"><a id="__codelineno-29-3" name="__codelineno-29-3" href="#__codelineno-29-3"></a>
</span><span id="__span-29-4"><a id="__codelineno-29-4" name="__codelineno-29-4" href="#__codelineno-29-4"></a>grad_c: [[1 1]
</span><span id="__span-29-5"><a id="__codelineno-29-5" name="__codelineno-29-5" href="#__codelineno-29-5"></a>         [1 1]]
</span><span id="__span-29-6"><a id="__codelineno-29-6" name="__codelineno-29-6" href="#__codelineno-29-6"></a>
</span><span id="__span-29-7"><a id="__codelineno-29-7" name="__codelineno-29-7" href="#__codelineno-29-7"></a>grad_a: [[1 1]
</span><span id="__span-29-8"><a id="__codelineno-29-8" name="__codelineno-29-8" href="#__codelineno-29-8"></a>         [1 1]]
</span><span id="__span-29-9"><a id="__codelineno-29-9" name="__codelineno-29-9" href="#__codelineno-29-9"></a>
</span><span id="__span-29-10"><a id="__codelineno-29-10" name="__codelineno-29-10" href="#__codelineno-29-10"></a>grad_b: 4
</span></code></pre></div>
<p>In <strong>Scenario 2</strong>, <code>b</code> has shape <code>(2,1)</code>, meaning it was broadcasted along axis <code>1</code> to match <code>a</code>'s shape <code>(2,2)</code>. During the backward pass, <code>grad_c</code> has shape <code>(2,2)</code>, so we <strong>sum over axis 1</strong> (<code>keepdims=True</code>) to restore <code>b</code>'s original shape <code>(2,1)</code>.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-30-1"><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> 
</span><span id="__span-30-2"><a id="__codelineno-30-2" name="__codelineno-30-2" href="#__codelineno-30-2"></a>              <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>      <span class="c1"># Shape: (2, 2)</span>
</span><span id="__span-30-3"><a id="__codelineno-30-3" name="__codelineno-30-3" href="#__codelineno-30-3"></a>
</span><span id="__span-30-4"><a id="__codelineno-30-4" name="__codelineno-30-4" href="#__codelineno-30-4"></a><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">10</span><span class="p">],</span> 
</span><span id="__span-30-5"><a id="__codelineno-30-5" name="__codelineno-30-5" href="#__codelineno-30-5"></a>              <span class="p">[</span><span class="mi">20</span><span class="p">]])</span>        <span class="c1"># Shape: (2, 1)  (Broadcasted across axis 1)</span>
</span><span id="__span-30-6"><a id="__codelineno-30-6" name="__codelineno-30-6" href="#__codelineno-30-6"></a>
</span><span id="__span-30-7"><a id="__codelineno-30-7" name="__codelineno-30-7" href="#__codelineno-30-7"></a><span class="c1"># element-wise addition</span>
</span><span id="__span-30-8"><a id="__codelineno-30-8" name="__codelineno-30-8" href="#__codelineno-30-8"></a><span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>                   <span class="c1"># Shape: (2, 2) (Broadcasting rules)</span>
</span><span id="__span-30-9"><a id="__codelineno-30-9" name="__codelineno-30-9" href="#__codelineno-30-9"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-30-10"><a id="__codelineno-30-10" name="__codelineno-30-10" href="#__codelineno-30-10"></a>
</span><span id="__span-30-11"><a id="__codelineno-30-11" name="__codelineno-30-11" href="#__codelineno-30-11"></a><span class="c1"># generate the initial gradient</span>
</span><span id="__span-30-12"><a id="__codelineno-30-12" name="__codelineno-30-12" href="#__codelineno-30-12"></a><span class="c1"># Shape: (2, 2)</span>
</span><span id="__span-30-13"><a id="__codelineno-30-13" name="__codelineno-30-13" href="#__codelineno-30-13"></a><span class="n">grad_c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</span><span id="__span-30-14"><a id="__codelineno-30-14" name="__codelineno-30-14" href="#__codelineno-30-14"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;grad_c: </span><span class="si">{</span><span class="n">grad_c</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-30-15"><a id="__codelineno-30-15" name="__codelineno-30-15" href="#__codelineno-30-15"></a>
</span><span id="__span-30-16"><a id="__codelineno-30-16" name="__codelineno-30-16" href="#__codelineno-30-16"></a><span class="c1"># Since `a` was not broadcasted, the gradient just passes through</span>
</span><span id="__span-30-17"><a id="__codelineno-30-17" name="__codelineno-30-17" href="#__codelineno-30-17"></a><span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_c</span>
</span><span id="__span-30-18"><a id="__codelineno-30-18" name="__codelineno-30-18" href="#__codelineno-30-18"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;grad_a: </span><span class="si">{</span><span class="n">grad_a</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-30-19"><a id="__codelineno-30-19" name="__codelineno-30-19" href="#__codelineno-30-19"></a>
</span><span id="__span-30-20"><a id="__codelineno-30-20" name="__codelineno-30-20" href="#__codelineno-30-20"></a><span class="c1"># Since `b` was **broadcasted along axis 1**, we must **sum** over </span>
</span><span id="__span-30-21"><a id="__codelineno-30-21" name="__codelineno-30-21" href="#__codelineno-30-21"></a><span class="c1"># that axis to reduce it back to `b`&#39;s original shape `(2,1)`</span>
</span><span id="__span-30-22"><a id="__codelineno-30-22" name="__codelineno-30-22" href="#__codelineno-30-22"></a><span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_c</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-30-23"><a id="__codelineno-30-23" name="__codelineno-30-23" href="#__codelineno-30-23"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;grad_b: </span><span class="si">{</span><span class="n">grad_b</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p><em>Output:</em></p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-31-1"><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a>c: [[11 12]
</span><span id="__span-31-2"><a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a>    [23 24]]
</span><span id="__span-31-3"><a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a>
</span><span id="__span-31-4"><a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a>grad_c: [[1 1]
</span><span id="__span-31-5"><a id="__codelineno-31-5" name="__codelineno-31-5" href="#__codelineno-31-5"></a>         [1 1]]
</span><span id="__span-31-6"><a id="__codelineno-31-6" name="__codelineno-31-6" href="#__codelineno-31-6"></a>
</span><span id="__span-31-7"><a id="__codelineno-31-7" name="__codelineno-31-7" href="#__codelineno-31-7"></a>grad_a: [[1 1]
</span><span id="__span-31-8"><a id="__codelineno-31-8" name="__codelineno-31-8" href="#__codelineno-31-8"></a>         [1 1]]
</span><span id="__span-31-9"><a id="__codelineno-31-9" name="__codelineno-31-9" href="#__codelineno-31-9"></a>
</span><span id="__span-31-10"><a id="__codelineno-31-10" name="__codelineno-31-10" href="#__codelineno-31-10"></a>grad_b: [[2]
</span><span id="__span-31-11"><a id="__codelineno-31-11" name="__codelineno-31-11" href="#__codelineno-31-11"></a>         [2]]
</span></code></pre></div>
<p>We need to compute <strong>gradients for A and B</strong> in the gradient tree.</p>
<p>Since <code>a</code> was not broadcasted, the gradient just passes through:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-32-1"><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a><span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_c</span>  <span class="c1"># Same shape as a (2, 2)</span>
</span></code></pre></div>
<p>Since <code>b</code> was <strong>broadcasted along axis 1</strong>, we must <strong>sum</strong> over that axis to reduce it back to <code>b</code>'s original shape <code>(2,1)</code>.  </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-33-1"><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a><span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_c</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div>
<p>The <code>bkwd_broadcast</code> function ensures that gradients are correctly summed over broadcasted dimensions during backpropagation. When an operation involves tensors of different shapes, broadcasting aligns them by expanding dimensions as needed. If extra dimensions were added during this process, they must be summed over in the backward pass to maintain consistency with the original tensor shape. In this case, since <code>B</code> was originally <code>(2,1)</code>, no additional dimensions were introduced (<code>ndim_added = 0</code>), so this step is skipped.  </p>
<p>To correctly compute the gradient for <code>B</code>, we must sum over the broadcasted axis. Since <code>B</code> was expanded along axis <code>1</code> to match the shape of <code>A</code>, its corresponding gradient <code>grad_Z</code> retains this extra information across all columns. To revert the gradient to <code>B</code>s original shape <code>(2,1)</code>, we sum over axis <code>1</code>, ensuring that the total contribution from each row is preserved while eliminating the artificially expanded dimension.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-34-1"><a id="__codelineno-34-1" name="__codelineno-34-1" href="#__codelineno-34-1"></a><span class="nd">@staticmethod</span>
</span><span id="__span-34-2"><a id="__codelineno-34-2" name="__codelineno-34-2" href="#__codelineno-34-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">bkwd_broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">):</span>
</span><span id="__span-34-3"><a id="__codelineno-34-3" name="__codelineno-34-3" href="#__codelineno-34-3"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-34-4"><a id="__codelineno-34-4" name="__codelineno-34-4" href="#__codelineno-34-4"></a><span class="sd">    Backward closure function to sum across broadcasted dimensions.</span>
</span><span id="__span-34-5"><a id="__codelineno-34-5" name="__codelineno-34-5" href="#__codelineno-34-5"></a>
</span><span id="__span-34-6"><a id="__codelineno-34-6" name="__codelineno-34-6" href="#__codelineno-34-6"></a><span class="sd">    When performing operations between tensors of different shapes, broadcasting is used</span>
</span><span id="__span-34-7"><a id="__codelineno-34-7" name="__codelineno-34-7" href="#__codelineno-34-7"></a><span class="sd">    to align their shapes. This function ensures that the gradients are correctly summed</span>
</span><span id="__span-34-8"><a id="__codelineno-34-8" name="__codelineno-34-8" href="#__codelineno-34-8"></a><span class="sd">    over the broadcasted dimensions during the backward pass.</span>
</span><span id="__span-34-9"><a id="__codelineno-34-9" name="__codelineno-34-9" href="#__codelineno-34-9"></a>
</span><span id="__span-34-10"><a id="__codelineno-34-10" name="__codelineno-34-10" href="#__codelineno-34-10"></a><span class="sd">    Args:</span>
</span><span id="__span-34-11"><a id="__codelineno-34-11" name="__codelineno-34-11" href="#__codelineno-34-11"></a><span class="sd">        tensor (Tensor): The tensor involved in the operation, used to handle its shape</span>
</span><span id="__span-34-12"><a id="__codelineno-34-12" name="__codelineno-34-12" href="#__codelineno-34-12"></a><span class="sd">                         during backward gradient computation.</span>
</span><span id="__span-34-13"><a id="__codelineno-34-13" name="__codelineno-34-13" href="#__codelineno-34-13"></a><span class="sd">    Returns:</span>
</span><span id="__span-34-14"><a id="__codelineno-34-14" name="__codelineno-34-14" href="#__codelineno-34-14"></a><span class="sd">        _bkwd (function): A function that computes the gradient, summing over broadcasted</span>
</span><span id="__span-34-15"><a id="__codelineno-34-15" name="__codelineno-34-15" href="#__codelineno-34-15"></a><span class="sd">                          dimensions to match the original tensor&#39;s shape.</span>
</span><span id="__span-34-16"><a id="__codelineno-34-16" name="__codelineno-34-16" href="#__codelineno-34-16"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-34-17"><a id="__codelineno-34-17" name="__codelineno-34-17" href="#__codelineno-34-17"></a>
</span><span id="__span-34-18"><a id="__codelineno-34-18" name="__codelineno-34-18" href="#__codelineno-34-18"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-34-19"><a id="__codelineno-34-19" name="__codelineno-34-19" href="#__codelineno-34-19"></a>        <span class="c1"># Handle scalar tensor case:</span>
</span><span id="__span-34-20"><a id="__codelineno-34-20" name="__codelineno-34-20" href="#__codelineno-34-20"></a>        <span class="c1"># Original tensor was a scalar: sum all gradients</span>
</span><span id="__span-34-21"><a id="__codelineno-34-21" name="__codelineno-34-21" href="#__codelineno-34-21"></a>        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="__span-34-22"><a id="__codelineno-34-22" name="__codelineno-34-22" href="#__codelineno-34-22"></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span><span id="__span-34-23"><a id="__codelineno-34-23" name="__codelineno-34-23" href="#__codelineno-34-23"></a>
</span><span id="__span-34-24"><a id="__codelineno-34-24" name="__codelineno-34-24" href="#__codelineno-34-24"></a>        <span class="c1"># Handle scalar grad case</span>
</span><span id="__span-34-25"><a id="__codelineno-34-25" name="__codelineno-34-25" href="#__codelineno-34-25"></a>        <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="__span-34-26"><a id="__codelineno-34-26" name="__codelineno-34-26" href="#__codelineno-34-26"></a>            <span class="k">return</span> <span class="n">grad</span>
</span><span id="__span-34-27"><a id="__codelineno-34-27" name="__codelineno-34-27" href="#__codelineno-34-27"></a>
</span><span id="__span-34-28"><a id="__codelineno-34-28" name="__codelineno-34-28" href="#__codelineno-34-28"></a>        <span class="c1"># Calculate the number of dimensions *added* to the tensor to achieve</span>
</span><span id="__span-34-29"><a id="__codelineno-34-29" name="__codelineno-34-29" href="#__codelineno-34-29"></a>        <span class="c1"># the grad shape. This is where broadcasting might have &quot;prepended&quot;</span>
</span><span id="__span-34-30"><a id="__codelineno-34-30" name="__codelineno-34-30" href="#__codelineno-34-30"></a>        <span class="c1"># dimensions.</span>
</span><span id="__span-34-31"><a id="__codelineno-34-31" name="__codelineno-34-31" href="#__codelineno-34-31"></a>        <span class="n">ndim_added</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
</span><span id="__span-34-32"><a id="__codelineno-34-32" name="__codelineno-34-32" href="#__codelineno-34-32"></a>
</span><span id="__span-34-33"><a id="__codelineno-34-33" name="__codelineno-34-33" href="#__codelineno-34-33"></a>        <span class="k">if</span> <span class="n">ndim_added</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="__span-34-34"><a id="__codelineno-34-34" name="__codelineno-34-34" href="#__codelineno-34-34"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ndim_added</span><span class="p">)),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span><span id="__span-34-35"><a id="__codelineno-34-35" name="__codelineno-34-35" href="#__codelineno-34-35"></a>
</span><span id="__span-34-36"><a id="__codelineno-34-36" name="__codelineno-34-36" href="#__codelineno-34-36"></a>        <span class="c1"># Sum over dimensions where tensor was broadcasted (size 1)</span>
</span><span id="__span-34-37"><a id="__codelineno-34-37" name="__codelineno-34-37" href="#__codelineno-34-37"></a>        <span class="n">reduce_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="__span-34-38"><a id="__codelineno-34-38" name="__codelineno-34-38" href="#__codelineno-34-38"></a>            <span class="n">dim</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
</span><span id="__span-34-39"><a id="__codelineno-34-39" name="__codelineno-34-39" href="#__codelineno-34-39"></a>            <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span>
</span><span id="__span-34-40"><a id="__codelineno-34-40" name="__codelineno-34-40" href="#__codelineno-34-40"></a>        <span class="p">)</span>
</span><span id="__span-34-41"><a id="__codelineno-34-41" name="__codelineno-34-41" href="#__codelineno-34-41"></a>
</span><span id="__span-34-42"><a id="__codelineno-34-42" name="__codelineno-34-42" href="#__codelineno-34-42"></a>        <span class="k">if</span> <span class="n">reduce_axes</span><span class="p">:</span>
</span><span id="__span-34-43"><a id="__codelineno-34-43" name="__codelineno-34-43" href="#__codelineno-34-43"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-34-44"><a id="__codelineno-34-44" name="__codelineno-34-44" href="#__codelineno-34-44"></a>
</span><span id="__span-34-45"><a id="__codelineno-34-45" name="__codelineno-34-45" href="#__codelineno-34-45"></a>        <span class="c1"># Ensure the final shape matches the tensor shape exactly</span>
</span><span id="__span-34-46"><a id="__codelineno-34-46" name="__codelineno-34-46" href="#__codelineno-34-46"></a>        <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
</span><span id="__span-34-47"><a id="__codelineno-34-47" name="__codelineno-34-47" href="#__codelineno-34-47"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="__span-34-48"><a id="__codelineno-34-48" name="__codelineno-34-48" href="#__codelineno-34-48"></a>
</span><span id="__span-34-49"><a id="__codelineno-34-49" name="__codelineno-34-49" href="#__codelineno-34-49"></a>        <span class="k">return</span> <span class="n">grad</span>
</span><span id="__span-34-50"><a id="__codelineno-34-50" name="__codelineno-34-50" href="#__codelineno-34-50"></a>
</span><span id="__span-34-51"><a id="__codelineno-34-51" name="__codelineno-34-51" href="#__codelineno-34-51"></a>    <span class="k">return</span> <span class="n">_bkwd</span>
</span></code></pre></div>
<p><strong>Handle Scalar Tensor Case</strong></p>
<p>If the original tensor is a scalar (<code>tensor.ndim == 0</code>), it means that during the forward pass, this scalar was broadcasted to match the shape of another tensor. To compute the gradient for a scalar tensor, we need to sum up all the gradients from the larger tensor (e.g., matrix or vector) because the scalar contributes to every element of the result.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-35-1"><a id="__codelineno-35-1" name="__codelineno-35-1" href="#__codelineno-35-1"></a><span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="__span-35-2"><a id="__codelineno-35-2" name="__codelineno-35-2" href="#__codelineno-35-2"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Handle Scalar Gradient Case</strong></p>
<p>If the gradient itself is a scalar (<code>grad.ndim == 0</code>), no broadcasting occurred during the forward pass. In this case, the gradient can be returned as-is because there are no dimensions to reduce.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-36-1"><a id="__codelineno-36-1" name="__codelineno-36-1" href="#__codelineno-36-1"></a><span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="__span-36-2"><a id="__codelineno-36-2" name="__codelineno-36-2" href="#__codelineno-36-2"></a>    <span class="k">return</span> <span class="n">grad</span>
</span></code></pre></div>
<p><strong>Calculate Dimensions Added by Broadcasting</strong></p>
<p>During broadcasting, NumPy may prepend dimensions to the smaller tensor to align its shape with the larger tensor. For example: Forward shapes: <code>(3,) + (5, 3) -&gt; (5, 3)</code> - a new dimension is prepended to the first tensor. And we calculate in <code>ndim_added</code> how many such dimensions were added to the original tensor to match the gradient's shape.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-37-1"><a id="__codelineno-37-1" name="__codelineno-37-1" href="#__codelineno-37-1"></a><span class="n">ndim_added</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Scenario 1 - Sum Over Added Dimensions:</strong> These are collapsed using <code>keepdims=False</code> because they don't exist in the original tensor.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-38-1"><a id="__codelineno-38-1" name="__codelineno-38-1" href="#__codelineno-38-1"></a><span class="k">if</span> <span class="n">ndim_added</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="__span-38-2"><a id="__codelineno-38-2" name="__codelineno-38-2" href="#__codelineno-38-2"></a>    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ndim_added</span><span class="p">)),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Scenario 2 - Sum Over Broadcasted Dimensions:</strong> These are summed while retaining their size as <code>1</code> using <code>keepdims=True</code> to preserve the original tensor's structure.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-39-1"><a id="__codelineno-39-1" name="__codelineno-39-1" href="#__codelineno-39-1"></a><span class="n">reduce_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
</span><span id="__span-39-2"><a id="__codelineno-39-2" name="__codelineno-39-2" href="#__codelineno-39-2"></a>    <span class="n">dim</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
</span><span id="__span-39-3"><a id="__codelineno-39-3" name="__codelineno-39-3" href="#__codelineno-39-3"></a>    <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span>
</span><span id="__span-39-4"><a id="__codelineno-39-4" name="__codelineno-39-4" href="#__codelineno-39-4"></a><span class="p">)</span>
</span><span id="__span-39-5"><a id="__codelineno-39-5" name="__codelineno-39-5" href="#__codelineno-39-5"></a>
</span><span id="__span-39-6"><a id="__codelineno-39-6" name="__codelineno-39-6" href="#__codelineno-39-6"></a><span class="k">if</span> <span class="n">reduce_axes</span><span class="p">:</span>
</span><span id="__span-39-7"><a id="__codelineno-39-7" name="__codelineno-39-7" href="#__codelineno-39-7"></a>    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Ensure Final Shape Matches:</strong> This is a safeguard to ensure that the gradient's shape exactly matches the original tensor's shape. While the previous steps should handle most cases, this ensures correctness in edge cases.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-40-1"><a id="__codelineno-40-1" name="__codelineno-40-1" href="#__codelineno-40-1"></a><span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
</span><span id="__span-40-2"><a id="__codelineno-40-2" name="__codelineno-40-2" href="#__codelineno-40-2"></a>    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="add-sub-and-their-friends"><code>add</code>, <code>sub</code> and their friends<a class="headerlink" href="#add-sub-and-their-friends" title="Permanent link">&para;</a></h3>
<p>The first one is the <code>add</code> method, which will handle element-wise addition of two <code>Tensor</code> objects.</p>
<p>The <strong>addition</strong> operation computes the element-wise sum of two tensors.</p>
<div class="arithmatex">\[f(a, b) = a + b\]</div>
<p>The derivative of <span class="arithmatex">\(a + b\)</span> with respect to <span class="arithmatex">\(a\)</span> and <span class="arithmatex">\(b\)</span> is 1:</p>
<div class="arithmatex">\[\frac{d}{da} (a + b) = 1\]</div>
<div class="arithmatex">\[\frac{d}{db} (a + b) = 1\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-41-1"><a id="__codelineno-41-1" name="__codelineno-41-1" href="#__codelineno-41-1"></a><span class="nd">@staticmethod</span>
</span><span id="__span-41-2"><a id="__codelineno-41-2" name="__codelineno-41-2" href="#__codelineno-41-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-41-3"><a id="__codelineno-41-3" name="__codelineno-41-3" href="#__codelineno-41-3"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-41-4"><a id="__codelineno-41-4" name="__codelineno-41-4" href="#__codelineno-41-4"></a><span class="sd">    Add two tensors and return a new tensor containing the result.</span>
</span><span id="__span-41-5"><a id="__codelineno-41-5" name="__codelineno-41-5" href="#__codelineno-41-5"></a>
</span><span id="__span-41-6"><a id="__codelineno-41-6" name="__codelineno-41-6" href="#__codelineno-41-6"></a><span class="sd">    This method performs element-wise addition of two tensors, handling broadcasting </span>
</span><span id="__span-41-7"><a id="__codelineno-41-7" name="__codelineno-41-7" href="#__codelineno-41-7"></a><span class="sd">    if necessary. If either tensor requires gradients, the resulting tensor will also </span>
</span><span id="__span-41-8"><a id="__codelineno-41-8" name="__codelineno-41-8" href="#__codelineno-41-8"></a><span class="sd">    track gradients and backpropagate them correctly.</span>
</span><span id="__span-41-9"><a id="__codelineno-41-9" name="__codelineno-41-9" href="#__codelineno-41-9"></a>
</span><span id="__span-41-10"><a id="__codelineno-41-10" name="__codelineno-41-10" href="#__codelineno-41-10"></a><span class="sd">    Args:</span>
</span><span id="__span-41-11"><a id="__codelineno-41-11" name="__codelineno-41-11" href="#__codelineno-41-11"></a><span class="sd">        a (Tensor): The first tensor to be added.</span>
</span><span id="__span-41-12"><a id="__codelineno-41-12" name="__codelineno-41-12" href="#__codelineno-41-12"></a><span class="sd">        b (Tensor): The second tensor to be added.</span>
</span><span id="__span-41-13"><a id="__codelineno-41-13" name="__codelineno-41-13" href="#__codelineno-41-13"></a>
</span><span id="__span-41-14"><a id="__codelineno-41-14" name="__codelineno-41-14" href="#__codelineno-41-14"></a><span class="sd">    Returns:</span>
</span><span id="__span-41-15"><a id="__codelineno-41-15" name="__codelineno-41-15" href="#__codelineno-41-15"></a><span class="sd">        Tensor: A new tensor that contains the element-wise sum of a and b.</span>
</span><span id="__span-41-16"><a id="__codelineno-41-16" name="__codelineno-41-16" href="#__codelineno-41-16"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-41-17"><a id="__codelineno-41-17" name="__codelineno-41-17" href="#__codelineno-41-17"></a>
</span><span id="__span-41-18"><a id="__codelineno-41-18" name="__codelineno-41-18" href="#__codelineno-41-18"></a>    <span class="c1"># Perform element-wise addition of the data of tensors a and b</span>
</span><span id="__span-41-19"><a id="__codelineno-41-19" name="__codelineno-41-19" href="#__codelineno-41-19"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
</span><span id="__span-41-20"><a id="__codelineno-41-20" name="__codelineno-41-20" href="#__codelineno-41-20"></a>
</span><span id="__span-41-21"><a id="__codelineno-41-21" name="__codelineno-41-21" href="#__codelineno-41-21"></a>    <span class="c1"># Determine if the result requires gradients (if any input tensor requires it)</span>
</span><span id="__span-41-22"><a id="__codelineno-41-22" name="__codelineno-41-22" href="#__codelineno-41-22"></a>    <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span>
</span><span id="__span-41-23"><a id="__codelineno-41-23" name="__codelineno-41-23" href="#__codelineno-41-23"></a>
</span><span id="__span-41-24"><a id="__codelineno-41-24" name="__codelineno-41-24" href="#__codelineno-41-24"></a>    <span class="c1"># List to store dependencies (grad functions) for backpropagation</span>
</span><span id="__span-41-25"><a id="__codelineno-41-25" name="__codelineno-41-25" href="#__codelineno-41-25"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-41-26"><a id="__codelineno-41-26" name="__codelineno-41-26" href="#__codelineno-41-26"></a>
</span><span id="__span-41-27"><a id="__codelineno-41-27" name="__codelineno-41-27" href="#__codelineno-41-27"></a>    <span class="c1"># If tensor a requires gradients, add its gradient function to dependencies</span>
</span><span id="__span-41-28"><a id="__codelineno-41-28" name="__codelineno-41-28" href="#__codelineno-41-28"></a>    <span class="c1"># Apply bkwd_broadcast to the tensor a</span>
</span><span id="__span-41-29"><a id="__codelineno-41-29" name="__codelineno-41-29" href="#__codelineno-41-29"></a>    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-41-30"><a id="__codelineno-41-30" name="__codelineno-41-30" href="#__codelineno-41-30"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-41-31"><a id="__codelineno-41-31" name="__codelineno-41-31" href="#__codelineno-41-31"></a>            <span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_bkwd_broadcast</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
</span><span id="__span-41-32"><a id="__codelineno-41-32" name="__codelineno-41-32" href="#__codelineno-41-32"></a>        <span class="p">)</span>
</span><span id="__span-41-33"><a id="__codelineno-41-33" name="__codelineno-41-33" href="#__codelineno-41-33"></a>
</span><span id="__span-41-34"><a id="__codelineno-41-34" name="__codelineno-41-34" href="#__codelineno-41-34"></a>    <span class="c1"># If tensor b requires gradients, add its gradient function to dependencies</span>
</span><span id="__span-41-35"><a id="__codelineno-41-35" name="__codelineno-41-35" href="#__codelineno-41-35"></a>    <span class="c1"># Apply bkwd_broadcast to the tensor b</span>
</span><span id="__span-41-36"><a id="__codelineno-41-36" name="__codelineno-41-36" href="#__codelineno-41-36"></a>    <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-41-37"><a id="__codelineno-41-37" name="__codelineno-41-37" href="#__codelineno-41-37"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-41-38"><a id="__codelineno-41-38" name="__codelineno-41-38" href="#__codelineno-41-38"></a>            <span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_bkwd_broadcast</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
</span><span id="__span-41-39"><a id="__codelineno-41-39" name="__codelineno-41-39" href="#__codelineno-41-39"></a>        <span class="p">)</span>
</span><span id="__span-41-40"><a id="__codelineno-41-40" name="__codelineno-41-40" href="#__codelineno-41-40"></a>
</span><span id="__span-41-41"><a id="__codelineno-41-41" name="__codelineno-41-41" href="#__codelineno-41-41"></a>    <span class="c1"># Return a new tensor with the result, gradient flag, and dependencies</span>
</span><span id="__span-41-42"><a id="__codelineno-41-42" name="__codelineno-41-42" href="#__codelineno-41-42"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>Now, we are ready to overload the <code>+</code> and <code>-</code> operations! By implementing these operator overloads, we make tensor arithmetic more intuitive and user-friendly. </p>
<p>Additionally, we implement <strong>in-place addition (<code>+=</code>) and subtraction (<code>-=</code>)</strong> to modify tensors directly without creating new ones. However, note that <strong>in-place operations do not track gradients</strong> for automatic differentiation.</p>
<p>To simplify subtraction, we introduce the <code>__neg__</code> method (<code>-</code> operator), which multiplies the tensor by <code>-1</code>. This allows us to redefine subtraction as <strong>adding the negated tensor</strong>, replacing <code>a - b</code> with <code>a + (-b)</code>, keeping the logic clean and consistent.</p>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/kPRDyKfLYlA?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Building PyTorch: Overloading Operators for Subtraction and Multiplication in MicroTorch" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<div class="language-python highlight"><pre><span></span><code><span id="__span-42-1"><a id="__codelineno-42-1" name="__codelineno-42-1" href="#__codelineno-42-1"></a><span class="k">def</span><span class="w"> </span><span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-42-2"><a id="__codelineno-42-2" name="__codelineno-42-2" href="#__codelineno-42-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-42-3"><a id="__codelineno-42-3" name="__codelineno-42-3" href="#__codelineno-42-3"></a><span class="sd">    Overload the `+` operator to perform element-wise tensor addition.</span>
</span><span id="__span-42-4"><a id="__codelineno-42-4" name="__codelineno-42-4" href="#__codelineno-42-4"></a>
</span><span id="__span-42-5"><a id="__codelineno-42-5" name="__codelineno-42-5" href="#__codelineno-42-5"></a><span class="sd">    Args:</span>
</span><span id="__span-42-6"><a id="__codelineno-42-6" name="__codelineno-42-6" href="#__codelineno-42-6"></a><span class="sd">        other (Data): Another tensor or scalar to add.</span>
</span><span id="__span-42-7"><a id="__codelineno-42-7" name="__codelineno-42-7" href="#__codelineno-42-7"></a>
</span><span id="__span-42-8"><a id="__codelineno-42-8" name="__codelineno-42-8" href="#__codelineno-42-8"></a><span class="sd">    Returns:</span>
</span><span id="__span-42-9"><a id="__codelineno-42-9" name="__codelineno-42-9" href="#__codelineno-42-9"></a><span class="sd">        Tensor: The result of element-wise addition.</span>
</span><span id="__span-42-10"><a id="__codelineno-42-10" name="__codelineno-42-10" href="#__codelineno-42-10"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-42-11"><a id="__codelineno-42-11" name="__codelineno-42-11" href="#__codelineno-42-11"></a>
</span><span id="__span-42-12"><a id="__codelineno-42-12" name="__codelineno-42-12" href="#__codelineno-42-12"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>
</span><span id="__span-42-13"><a id="__codelineno-42-13" name="__codelineno-42-13" href="#__codelineno-42-13"></a>
</span><span id="__span-42-14"><a id="__codelineno-42-14" name="__codelineno-42-14" href="#__codelineno-42-14"></a><span class="k">def</span><span class="w"> </span><span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-42-15"><a id="__codelineno-42-15" name="__codelineno-42-15" href="#__codelineno-42-15"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-42-16"><a id="__codelineno-42-16" name="__codelineno-42-16" href="#__codelineno-42-16"></a><span class="sd">    Overload the right-hand `+` operator (other + self).</span>
</span><span id="__span-42-17"><a id="__codelineno-42-17" name="__codelineno-42-17" href="#__codelineno-42-17"></a>
</span><span id="__span-42-18"><a id="__codelineno-42-18" name="__codelineno-42-18" href="#__codelineno-42-18"></a><span class="sd">    Args:</span>
</span><span id="__span-42-19"><a id="__codelineno-42-19" name="__codelineno-42-19" href="#__codelineno-42-19"></a><span class="sd">        other (Data): Another tensor or scalar to add.</span>
</span><span id="__span-42-20"><a id="__codelineno-42-20" name="__codelineno-42-20" href="#__codelineno-42-20"></a>
</span><span id="__span-42-21"><a id="__codelineno-42-21" name="__codelineno-42-21" href="#__codelineno-42-21"></a><span class="sd">    Returns:</span>
</span><span id="__span-42-22"><a id="__codelineno-42-22" name="__codelineno-42-22" href="#__codelineno-42-22"></a><span class="sd">        Tensor: The result of element-wise addition.</span>
</span><span id="__span-42-23"><a id="__codelineno-42-23" name="__codelineno-42-23" href="#__codelineno-42-23"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-42-24"><a id="__codelineno-42-24" name="__codelineno-42-24" href="#__codelineno-42-24"></a>
</span><span id="__span-42-25"><a id="__codelineno-42-25" name="__codelineno-42-25" href="#__codelineno-42-25"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span>
</span><span id="__span-42-26"><a id="__codelineno-42-26" name="__codelineno-42-26" href="#__codelineno-42-26"></a>
</span><span id="__span-42-27"><a id="__codelineno-42-27" name="__codelineno-42-27" href="#__codelineno-42-27"></a><span class="k">def</span><span class="w"> </span><span class="fm">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-42-28"><a id="__codelineno-42-28" name="__codelineno-42-28" href="#__codelineno-42-28"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-42-29"><a id="__codelineno-42-29" name="__codelineno-42-29" href="#__codelineno-42-29"></a><span class="sd">    Overload the `+=` operator for in-place addition.</span>
</span><span id="__span-42-30"><a id="__codelineno-42-30" name="__codelineno-42-30" href="#__codelineno-42-30"></a><span class="sd">    WARNING: In-place operations do not track gradients!</span>
</span><span id="__span-42-31"><a id="__codelineno-42-31" name="__codelineno-42-31" href="#__codelineno-42-31"></a>
</span><span id="__span-42-32"><a id="__codelineno-42-32" name="__codelineno-42-32" href="#__codelineno-42-32"></a><span class="sd">    Args:</span>
</span><span id="__span-42-33"><a id="__codelineno-42-33" name="__codelineno-42-33" href="#__codelineno-42-33"></a><span class="sd">        other (Data): Another tensor or scalar to add in-place.</span>
</span><span id="__span-42-34"><a id="__codelineno-42-34" name="__codelineno-42-34" href="#__codelineno-42-34"></a>
</span><span id="__span-42-35"><a id="__codelineno-42-35" name="__codelineno-42-35" href="#__codelineno-42-35"></a><span class="sd">    Returns:</span>
</span><span id="__span-42-36"><a id="__codelineno-42-36" name="__codelineno-42-36" href="#__codelineno-42-36"></a><span class="sd">        Tensor: The updated tensor after in-place addition.</span>
</span><span id="__span-42-37"><a id="__codelineno-42-37" name="__codelineno-42-37" href="#__codelineno-42-37"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-42-38"><a id="__codelineno-42-38" name="__codelineno-42-38" href="#__codelineno-42-38"></a>
</span><span id="__span-42-39"><a id="__codelineno-42-39" name="__codelineno-42-39" href="#__codelineno-42-39"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">build_ndarray</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-42-40"><a id="__codelineno-42-40" name="__codelineno-42-40" href="#__codelineno-42-40"></a>    <span class="k">return</span> <span class="bp">self</span>
</span><span id="__span-42-41"><a id="__codelineno-42-41" name="__codelineno-42-41" href="#__codelineno-42-41"></a>
</span><span id="__span-42-42"><a id="__codelineno-42-42" name="__codelineno-42-42" href="#__codelineno-42-42"></a><span class="k">def</span><span class="w"> </span><span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-42-43"><a id="__codelineno-42-43" name="__codelineno-42-43" href="#__codelineno-42-43"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-42-44"><a id="__codelineno-42-44" name="__codelineno-42-44" href="#__codelineno-42-44"></a><span class="sd">    Overload the unary `-` operator to negate a tensor.</span>
</span><span id="__span-42-45"><a id="__codelineno-42-45" name="__codelineno-42-45" href="#__codelineno-42-45"></a><span class="sd">    This allows defining subtraction as addition with negation.</span>
</span><span id="__span-42-46"><a id="__codelineno-42-46" name="__codelineno-42-46" href="#__codelineno-42-46"></a>
</span><span id="__span-42-47"><a id="__codelineno-42-47" name="__codelineno-42-47" href="#__codelineno-42-47"></a><span class="sd">    Returns:</span>
</span><span id="__span-42-48"><a id="__codelineno-42-48" name="__codelineno-42-48" href="#__codelineno-42-48"></a><span class="sd">        Tensor: The negated tensor (-self).</span>
</span><span id="__span-42-49"><a id="__codelineno-42-49" name="__codelineno-42-49" href="#__codelineno-42-49"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-42-50"><a id="__codelineno-42-50" name="__codelineno-42-50" href="#__codelineno-42-50"></a>
</span><span id="__span-42-51"><a id="__codelineno-42-51" name="__codelineno-42-51" href="#__codelineno-42-51"></a>    <span class="n">output</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span>
</span><span id="__span-42-52"><a id="__codelineno-42-52" name="__codelineno-42-52" href="#__codelineno-42-52"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-42-53"><a id="__codelineno-42-53" name="__codelineno-42-53" href="#__codelineno-42-53"></a>
</span><span id="__span-42-54"><a id="__codelineno-42-54" name="__codelineno-42-54" href="#__codelineno-42-54"></a>    <span class="c1"># Define the backward function: gradient negation</span>
</span><span id="__span-42-55"><a id="__codelineno-42-55" name="__codelineno-42-55" href="#__codelineno-42-55"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-42-56"><a id="__codelineno-42-56" name="__codelineno-42-56" href="#__codelineno-42-56"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-42-57"><a id="__codelineno-42-57" name="__codelineno-42-57" href="#__codelineno-42-57"></a>            <span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">grad</span><span class="p">:</span> <span class="o">-</span><span class="n">grad</span><span class="p">)</span>
</span><span id="__span-42-58"><a id="__codelineno-42-58" name="__codelineno-42-58" href="#__codelineno-42-58"></a>        <span class="p">)</span>
</span><span id="__span-42-59"><a id="__codelineno-42-59" name="__codelineno-42-59" href="#__codelineno-42-59"></a>
</span><span id="__span-42-60"><a id="__codelineno-42-60" name="__codelineno-42-60" href="#__codelineno-42-60"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span><span id="__span-42-61"><a id="__codelineno-42-61" name="__codelineno-42-61" href="#__codelineno-42-61"></a>
</span><span id="__span-42-62"><a id="__codelineno-42-62" name="__codelineno-42-62" href="#__codelineno-42-62"></a><span class="k">def</span><span class="w"> </span><span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-42-63"><a id="__codelineno-42-63" name="__codelineno-42-63" href="#__codelineno-42-63"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-42-64"><a id="__codelineno-42-64" name="__codelineno-42-64" href="#__codelineno-42-64"></a><span class="sd">    Overload the `-` operator for element-wise subtraction.</span>
</span><span id="__span-42-65"><a id="__codelineno-42-65" name="__codelineno-42-65" href="#__codelineno-42-65"></a><span class="sd">    Uses addition with negation: a - b  a + (-b).</span>
</span><span id="__span-42-66"><a id="__codelineno-42-66" name="__codelineno-42-66" href="#__codelineno-42-66"></a>
</span><span id="__span-42-67"><a id="__codelineno-42-67" name="__codelineno-42-67" href="#__codelineno-42-67"></a><span class="sd">    Args:</span>
</span><span id="__span-42-68"><a id="__codelineno-42-68" name="__codelineno-42-68" href="#__codelineno-42-68"></a><span class="sd">        other (Data): Another tensor or scalar to subtract.</span>
</span><span id="__span-42-69"><a id="__codelineno-42-69" name="__codelineno-42-69" href="#__codelineno-42-69"></a>
</span><span id="__span-42-70"><a id="__codelineno-42-70" name="__codelineno-42-70" href="#__codelineno-42-70"></a><span class="sd">    Returns:</span>
</span><span id="__span-42-71"><a id="__codelineno-42-71" name="__codelineno-42-71" href="#__codelineno-42-71"></a><span class="sd">        Tensor: The result of element-wise subtraction.</span>
</span><span id="__span-42-72"><a id="__codelineno-42-72" name="__codelineno-42-72" href="#__codelineno-42-72"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-42-73"><a id="__codelineno-42-73" name="__codelineno-42-73" href="#__codelineno-42-73"></a>
</span><span id="__span-42-74"><a id="__codelineno-42-74" name="__codelineno-42-74" href="#__codelineno-42-74"></a>    <span class="k">return</span> <span class="bp">self</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>
</span><span id="__span-42-75"><a id="__codelineno-42-75" name="__codelineno-42-75" href="#__codelineno-42-75"></a>
</span><span id="__span-42-76"><a id="__codelineno-42-76" name="__codelineno-42-76" href="#__codelineno-42-76"></a><span class="k">def</span><span class="w"> </span><span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-42-77"><a id="__codelineno-42-77" name="__codelineno-42-77" href="#__codelineno-42-77"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-42-78"><a id="__codelineno-42-78" name="__codelineno-42-78" href="#__codelineno-42-78"></a><span class="sd">    Overload the right-hand `-` operator (other - self).</span>
</span><span id="__span-42-79"><a id="__codelineno-42-79" name="__codelineno-42-79" href="#__codelineno-42-79"></a><span class="sd">    Uses addition with negation: b - a  b + (-a).</span>
</span><span id="__span-42-80"><a id="__codelineno-42-80" name="__codelineno-42-80" href="#__codelineno-42-80"></a>
</span><span id="__span-42-81"><a id="__codelineno-42-81" name="__codelineno-42-81" href="#__codelineno-42-81"></a><span class="sd">    Args:</span>
</span><span id="__span-42-82"><a id="__codelineno-42-82" name="__codelineno-42-82" href="#__codelineno-42-82"></a><span class="sd">        other (Data): Another tensor or scalar.</span>
</span><span id="__span-42-83"><a id="__codelineno-42-83" name="__codelineno-42-83" href="#__codelineno-42-83"></a>
</span><span id="__span-42-84"><a id="__codelineno-42-84" name="__codelineno-42-84" href="#__codelineno-42-84"></a><span class="sd">    Returns:</span>
</span><span id="__span-42-85"><a id="__codelineno-42-85" name="__codelineno-42-85" href="#__codelineno-42-85"></a><span class="sd">        Tensor: The result of element-wise subtraction.</span>
</span><span id="__span-42-86"><a id="__codelineno-42-86" name="__codelineno-42-86" href="#__codelineno-42-86"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-42-87"><a id="__codelineno-42-87" name="__codelineno-42-87" href="#__codelineno-42-87"></a>
</span><span id="__span-42-88"><a id="__codelineno-42-88" name="__codelineno-42-88" href="#__codelineno-42-88"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">)</span>
</span><span id="__span-42-89"><a id="__codelineno-42-89" name="__codelineno-42-89" href="#__codelineno-42-89"></a>
</span><span id="__span-42-90"><a id="__codelineno-42-90" name="__codelineno-42-90" href="#__codelineno-42-90"></a><span class="k">def</span><span class="w"> </span><span class="fm">__isub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-42-91"><a id="__codelineno-42-91" name="__codelineno-42-91" href="#__codelineno-42-91"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-42-92"><a id="__codelineno-42-92" name="__codelineno-42-92" href="#__codelineno-42-92"></a><span class="sd">    Overload the `-=` operator for in-place subtraction.</span>
</span><span id="__span-42-93"><a id="__codelineno-42-93" name="__codelineno-42-93" href="#__codelineno-42-93"></a><span class="sd">    WARNING: In-place operations do not track gradients!</span>
</span><span id="__span-42-94"><a id="__codelineno-42-94" name="__codelineno-42-94" href="#__codelineno-42-94"></a>
</span><span id="__span-42-95"><a id="__codelineno-42-95" name="__codelineno-42-95" href="#__codelineno-42-95"></a><span class="sd">    Args:</span>
</span><span id="__span-42-96"><a id="__codelineno-42-96" name="__codelineno-42-96" href="#__codelineno-42-96"></a><span class="sd">        other (Data): Another tensor or scalar to subtract in-place.</span>
</span><span id="__span-42-97"><a id="__codelineno-42-97" name="__codelineno-42-97" href="#__codelineno-42-97"></a>
</span><span id="__span-42-98"><a id="__codelineno-42-98" name="__codelineno-42-98" href="#__codelineno-42-98"></a><span class="sd">    Returns:</span>
</span><span id="__span-42-99"><a id="__codelineno-42-99" name="__codelineno-42-99" href="#__codelineno-42-99"></a><span class="sd">        Tensor: The updated tensor after in-place subtraction.</span>
</span><span id="__span-42-100"><a id="__codelineno-42-100" name="__codelineno-42-100" href="#__codelineno-42-100"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-42-101"><a id="__codelineno-42-101" name="__codelineno-42-101" href="#__codelineno-42-101"></a>
</span><span id="__span-42-102"><a id="__codelineno-42-102" name="__codelineno-42-102" href="#__codelineno-42-102"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">build_ndarray</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-42-103"><a id="__codelineno-42-103" name="__codelineno-42-103" href="#__codelineno-42-103"></a>    <span class="k">return</span> <span class="bp">self</span>
</span></code></pre></div>
<h3 id="mul"><code>mul</code><a class="headerlink" href="#mul" title="Permanent link">&para;</a></h3>
<p>The <strong>multiplication</strong> operation computes the element-wise product of two tensors.</p>
<div class="arithmatex">\[f(a, b) = a \cdot b\]</div>
<p>The derivative of <span class="arithmatex">\(a \cdot b\)</span> with respect to <span class="arithmatex">\(a\)</span> and <span class="arithmatex">\(b\)</span> is:</p>
<div class="arithmatex">\[\frac{d}{da} (a \cdot b) = b\]</div>
<div class="arithmatex">\[\frac{d}{db} (a \cdot b) = a\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-43-1"><a id="__codelineno-43-1" name="__codelineno-43-1" href="#__codelineno-43-1"></a><span class="nd">@staticmethod</span>
</span><span id="__span-43-2"><a id="__codelineno-43-2" name="__codelineno-43-2" href="#__codelineno-43-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">mul</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-43-3"><a id="__codelineno-43-3" name="__codelineno-43-3" href="#__codelineno-43-3"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-43-4"><a id="__codelineno-43-4" name="__codelineno-43-4" href="#__codelineno-43-4"></a><span class="sd">    Performs element-wise multiplication between two tensors and returns the result.</span>
</span><span id="__span-43-5"><a id="__codelineno-43-5" name="__codelineno-43-5" href="#__codelineno-43-5"></a><span class="sd">    Handles tensors that require gradients by defining the backward pass for backpropagation.</span>
</span><span id="__span-43-6"><a id="__codelineno-43-6" name="__codelineno-43-6" href="#__codelineno-43-6"></a>
</span><span id="__span-43-7"><a id="__codelineno-43-7" name="__codelineno-43-7" href="#__codelineno-43-7"></a><span class="sd">    Args:</span>
</span><span id="__span-43-8"><a id="__codelineno-43-8" name="__codelineno-43-8" href="#__codelineno-43-8"></a><span class="sd">        a (Tensor): First tensor to be multiplied.</span>
</span><span id="__span-43-9"><a id="__codelineno-43-9" name="__codelineno-43-9" href="#__codelineno-43-9"></a><span class="sd">        b (Tensor): Second tensor to be multiplied.</span>
</span><span id="__span-43-10"><a id="__codelineno-43-10" name="__codelineno-43-10" href="#__codelineno-43-10"></a>
</span><span id="__span-43-11"><a id="__codelineno-43-11" name="__codelineno-43-11" href="#__codelineno-43-11"></a><span class="sd">    Returns:</span>
</span><span id="__span-43-12"><a id="__codelineno-43-12" name="__codelineno-43-12" href="#__codelineno-43-12"></a><span class="sd">        Tensor: A new tensor containing the result of the element-wise multiplication.</span>
</span><span id="__span-43-13"><a id="__codelineno-43-13" name="__codelineno-43-13" href="#__codelineno-43-13"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-43-14"><a id="__codelineno-43-14" name="__codelineno-43-14" href="#__codelineno-43-14"></a>    <span class="c1"># Ensure both tensors contain their data correctly, handling any potential gates</span>
</span><span id="__span-43-15"><a id="__codelineno-43-15" name="__codelineno-43-15" href="#__codelineno-43-15"></a>    <span class="n">a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</span><span id="__span-43-16"><a id="__codelineno-43-16" name="__codelineno-43-16" href="#__codelineno-43-16"></a>    <span class="n">b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span><span id="__span-43-17"><a id="__codelineno-43-17" name="__codelineno-43-17" href="#__codelineno-43-17"></a>
</span><span id="__span-43-18"><a id="__codelineno-43-18" name="__codelineno-43-18" href="#__codelineno-43-18"></a>    <span class="c1"># Perform element-wise multiplication on the tensor data</span>
</span><span id="__span-43-19"><a id="__codelineno-43-19" name="__codelineno-43-19" href="#__codelineno-43-19"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
</span><span id="__span-43-20"><a id="__codelineno-43-20" name="__codelineno-43-20" href="#__codelineno-43-20"></a>
</span><span id="__span-43-21"><a id="__codelineno-43-21" name="__codelineno-43-21" href="#__codelineno-43-21"></a>    <span class="c1"># Determine if the resulting tensor should require gradients</span>
</span><span id="__span-43-22"><a id="__codelineno-43-22" name="__codelineno-43-22" href="#__codelineno-43-22"></a>    <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span>
</span><span id="__span-43-23"><a id="__codelineno-43-23" name="__codelineno-43-23" href="#__codelineno-43-23"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-43-24"><a id="__codelineno-43-24" name="__codelineno-43-24" href="#__codelineno-43-24"></a>
</span><span id="__span-43-25"><a id="__codelineno-43-25" name="__codelineno-43-25" href="#__codelineno-43-25"></a>    <span class="c1"># Define the backward pass function for multiplication</span>
</span><span id="__span-43-26"><a id="__codelineno-43-26" name="__codelineno-43-26" href="#__codelineno-43-26"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_backward</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
</span><span id="__span-43-27"><a id="__codelineno-43-27" name="__codelineno-43-27" href="#__codelineno-43-27"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-43-28"><a id="__codelineno-43-28" name="__codelineno-43-28" href="#__codelineno-43-28"></a><span class="sd">        Backward closure function for Mul operation.</span>
</span><span id="__span-43-29"><a id="__codelineno-43-29" name="__codelineno-43-29" href="#__codelineno-43-29"></a><span class="sd">        Computes the gradient of the multiplication operation.</span>
</span><span id="__span-43-30"><a id="__codelineno-43-30" name="__codelineno-43-30" href="#__codelineno-43-30"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-43-31"><a id="__codelineno-43-31" name="__codelineno-43-31" href="#__codelineno-43-31"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-43-32"><a id="__codelineno-43-32" name="__codelineno-43-32" href="#__codelineno-43-32"></a><span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-43-33"><a id="__codelineno-43-33" name="__codelineno-43-33" href="#__codelineno-43-33"></a><span class="sd">            The gradient of the multiplication operation.</span>
</span><span id="__span-43-34"><a id="__codelineno-43-34" name="__codelineno-43-34" href="#__codelineno-43-34"></a><span class="sd">            The gradient of a * b is grad * b for a and grad * a for b.</span>
</span><span id="__span-43-35"><a id="__codelineno-43-35" name="__codelineno-43-35" href="#__codelineno-43-35"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-43-36"><a id="__codelineno-43-36" name="__codelineno-43-36" href="#__codelineno-43-36"></a>            <span class="c1"># Multiply the gradient by tensor b&#39;s data for the gradient w.r.t a</span>
</span><span id="__span-43-37"><a id="__codelineno-43-37" name="__codelineno-43-37" href="#__codelineno-43-37"></a>            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>
</span><span id="__span-43-38"><a id="__codelineno-43-38" name="__codelineno-43-38" href="#__codelineno-43-38"></a>            <span class="c1"># Ensure the gradient is properly reshaped using broadcasting</span>
</span><span id="__span-43-39"><a id="__codelineno-43-39" name="__codelineno-43-39" href="#__codelineno-43-39"></a>            <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">_bkwd_broadcast</span><span class="p">(</span><span class="n">a</span><span class="p">)(</span><span class="n">grad</span><span class="p">)</span>
</span><span id="__span-43-40"><a id="__codelineno-43-40" name="__codelineno-43-40" href="#__codelineno-43-40"></a>
</span><span id="__span-43-41"><a id="__codelineno-43-41" name="__codelineno-43-41" href="#__codelineno-43-41"></a>        <span class="k">return</span> <span class="n">_bkwd</span>
</span><span id="__span-43-42"><a id="__codelineno-43-42" name="__codelineno-43-42" href="#__codelineno-43-42"></a>
</span><span id="__span-43-43"><a id="__codelineno-43-43" name="__codelineno-43-43" href="#__codelineno-43-43"></a>    <span class="c1"># If tensor a requires gradients, add the backward function to the dependencies</span>
</span><span id="__span-43-44"><a id="__codelineno-43-44" name="__codelineno-43-44" href="#__codelineno-43-44"></a>    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-43-45"><a id="__codelineno-43-45" name="__codelineno-43-45" href="#__codelineno-43-45"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-43-46"><a id="__codelineno-43-46" name="__codelineno-43-46" href="#__codelineno-43-46"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-43-47"><a id="__codelineno-43-47" name="__codelineno-43-47" href="#__codelineno-43-47"></a>                <span class="n">value</span><span class="o">=</span><span class="n">a</span><span class="p">,</span>
</span><span id="__span-43-48"><a id="__codelineno-43-48" name="__codelineno-43-48" href="#__codelineno-43-48"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_backward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># Link tensor a&#39;s backward pass</span>
</span><span id="__span-43-49"><a id="__codelineno-43-49" name="__codelineno-43-49" href="#__codelineno-43-49"></a>            <span class="p">)</span>
</span><span id="__span-43-50"><a id="__codelineno-43-50" name="__codelineno-43-50" href="#__codelineno-43-50"></a>        <span class="p">)</span>
</span><span id="__span-43-51"><a id="__codelineno-43-51" name="__codelineno-43-51" href="#__codelineno-43-51"></a>
</span><span id="__span-43-52"><a id="__codelineno-43-52" name="__codelineno-43-52" href="#__codelineno-43-52"></a>    <span class="c1"># If tensor b requires gradients, add the backward function to the dependencies</span>
</span><span id="__span-43-53"><a id="__codelineno-43-53" name="__codelineno-43-53" href="#__codelineno-43-53"></a>    <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-43-54"><a id="__codelineno-43-54" name="__codelineno-43-54" href="#__codelineno-43-54"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-43-55"><a id="__codelineno-43-55" name="__codelineno-43-55" href="#__codelineno-43-55"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-43-56"><a id="__codelineno-43-56" name="__codelineno-43-56" href="#__codelineno-43-56"></a>                <span class="n">value</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
</span><span id="__span-43-57"><a id="__codelineno-43-57" name="__codelineno-43-57" href="#__codelineno-43-57"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_backward</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>  <span class="c1"># Link tensor b&#39;s backward pass</span>
</span><span id="__span-43-58"><a id="__codelineno-43-58" name="__codelineno-43-58" href="#__codelineno-43-58"></a>            <span class="p">)</span>
</span><span id="__span-43-59"><a id="__codelineno-43-59" name="__codelineno-43-59" href="#__codelineno-43-59"></a>        <span class="p">)</span>
</span><span id="__span-43-60"><a id="__codelineno-43-60" name="__codelineno-43-60" href="#__codelineno-43-60"></a>
</span><span id="__span-43-61"><a id="__codelineno-43-61" name="__codelineno-43-61" href="#__codelineno-43-61"></a>    <span class="c1"># Return the result as a new tensor, with the appropriate gradient information</span>
</span><span id="__span-43-62"><a id="__codelineno-43-62" name="__codelineno-43-62" href="#__codelineno-43-62"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>Now, we are ready to overload the multiplication operators. This allows us to use <code>*</code> for element-wise multiplication of tensors, additionally, we implement in-place multiplication (<code>*=</code>), which modifies the tensor directly but does not support gradient tracking.  </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-44-1"><a id="__codelineno-44-1" name="__codelineno-44-1" href="#__codelineno-44-1"></a><span class="k">def</span><span class="w"> </span><span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-44-2"><a id="__codelineno-44-2" name="__codelineno-44-2" href="#__codelineno-44-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-44-3"><a id="__codelineno-44-3" name="__codelineno-44-3" href="#__codelineno-44-3"></a><span class="sd">    Overloads the `*` operator for element-wise multiplication.</span>
</span><span id="__span-44-4"><a id="__codelineno-44-4" name="__codelineno-44-4" href="#__codelineno-44-4"></a>
</span><span id="__span-44-5"><a id="__codelineno-44-5" name="__codelineno-44-5" href="#__codelineno-44-5"></a><span class="sd">    This method ensures that Tensor multiplication can be performed seamlessly </span>
</span><span id="__span-44-6"><a id="__codelineno-44-6" name="__codelineno-44-6" href="#__codelineno-44-6"></a><span class="sd">    with both other Tensors and scalar values.</span>
</span><span id="__span-44-7"><a id="__codelineno-44-7" name="__codelineno-44-7" href="#__codelineno-44-7"></a>
</span><span id="__span-44-8"><a id="__codelineno-44-8" name="__codelineno-44-8" href="#__codelineno-44-8"></a><span class="sd">    Args:</span>
</span><span id="__span-44-9"><a id="__codelineno-44-9" name="__codelineno-44-9" href="#__codelineno-44-9"></a><span class="sd">        other (Data): The other operand, which can be a Tensor or a compatible scalar.</span>
</span><span id="__span-44-10"><a id="__codelineno-44-10" name="__codelineno-44-10" href="#__codelineno-44-10"></a>
</span><span id="__span-44-11"><a id="__codelineno-44-11" name="__codelineno-44-11" href="#__codelineno-44-11"></a><span class="sd">    Returns:</span>
</span><span id="__span-44-12"><a id="__codelineno-44-12" name="__codelineno-44-12" href="#__codelineno-44-12"></a><span class="sd">        Tensor: A new Tensor representing the element-wise product.</span>
</span><span id="__span-44-13"><a id="__codelineno-44-13" name="__codelineno-44-13" href="#__codelineno-44-13"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-44-14"><a id="__codelineno-44-14" name="__codelineno-44-14" href="#__codelineno-44-14"></a>
</span><span id="__span-44-15"><a id="__codelineno-44-15" name="__codelineno-44-15" href="#__codelineno-44-15"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">))</span>
</span><span id="__span-44-16"><a id="__codelineno-44-16" name="__codelineno-44-16" href="#__codelineno-44-16"></a>
</span><span id="__span-44-17"><a id="__codelineno-44-17" name="__codelineno-44-17" href="#__codelineno-44-17"></a><span class="k">def</span><span class="w"> </span><span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-44-18"><a id="__codelineno-44-18" name="__codelineno-44-18" href="#__codelineno-44-18"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-44-19"><a id="__codelineno-44-19" name="__codelineno-44-19" href="#__codelineno-44-19"></a><span class="sd">    Overloads the right-hand `*` operator.</span>
</span><span id="__span-44-20"><a id="__codelineno-44-20" name="__codelineno-44-20" href="#__codelineno-44-20"></a>
</span><span id="__span-44-21"><a id="__codelineno-44-21" name="__codelineno-44-21" href="#__codelineno-44-21"></a><span class="sd">    This ensures that multiplication works correctly when a scalar or another</span>
</span><span id="__span-44-22"><a id="__codelineno-44-22" name="__codelineno-44-22" href="#__codelineno-44-22"></a><span class="sd">    compatible type appears on the left side of the `*` operator.</span>
</span><span id="__span-44-23"><a id="__codelineno-44-23" name="__codelineno-44-23" href="#__codelineno-44-23"></a>
</span><span id="__span-44-24"><a id="__codelineno-44-24" name="__codelineno-44-24" href="#__codelineno-44-24"></a><span class="sd">    Args:</span>
</span><span id="__span-44-25"><a id="__codelineno-44-25" name="__codelineno-44-25" href="#__codelineno-44-25"></a><span class="sd">        other (Data): The left-hand operand, which can be a scalar or Tensor.</span>
</span><span id="__span-44-26"><a id="__codelineno-44-26" name="__codelineno-44-26" href="#__codelineno-44-26"></a>
</span><span id="__span-44-27"><a id="__codelineno-44-27" name="__codelineno-44-27" href="#__codelineno-44-27"></a><span class="sd">    Returns:</span>
</span><span id="__span-44-28"><a id="__codelineno-44-28" name="__codelineno-44-28" href="#__codelineno-44-28"></a><span class="sd">        Tensor: A new Tensor representing the element-wise product.</span>
</span><span id="__span-44-29"><a id="__codelineno-44-29" name="__codelineno-44-29" href="#__codelineno-44-29"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-44-30"><a id="__codelineno-44-30" name="__codelineno-44-30" href="#__codelineno-44-30"></a>
</span><span id="__span-44-31"><a id="__codelineno-44-31" name="__codelineno-44-31" href="#__codelineno-44-31"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span>
</span><span id="__span-44-32"><a id="__codelineno-44-32" name="__codelineno-44-32" href="#__codelineno-44-32"></a>
</span><span id="__span-44-33"><a id="__codelineno-44-33" name="__codelineno-44-33" href="#__codelineno-44-33"></a><span class="k">def</span><span class="w"> </span><span class="fm">__imul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-44-34"><a id="__codelineno-44-34" name="__codelineno-44-34" href="#__codelineno-44-34"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-44-35"><a id="__codelineno-44-35" name="__codelineno-44-35" href="#__codelineno-44-35"></a><span class="sd">    Overloads the `*=` operator for in-place multiplication.</span>
</span><span id="__span-44-36"><a id="__codelineno-44-36" name="__codelineno-44-36" href="#__codelineno-44-36"></a>
</span><span id="__span-44-37"><a id="__codelineno-44-37" name="__codelineno-44-37" href="#__codelineno-44-37"></a><span class="sd">    This modifies the Tensors data directly, which improves efficiency.</span>
</span><span id="__span-44-38"><a id="__codelineno-44-38" name="__codelineno-44-38" href="#__codelineno-44-38"></a><span class="sd">    However, in-place operations do not support automatic differentiation</span>
</span><span id="__span-44-39"><a id="__codelineno-44-39" name="__codelineno-44-39" href="#__codelineno-44-39"></a><span class="sd">    (i.e., gradients will not be tracked).</span>
</span><span id="__span-44-40"><a id="__codelineno-44-40" name="__codelineno-44-40" href="#__codelineno-44-40"></a>
</span><span id="__span-44-41"><a id="__codelineno-44-41" name="__codelineno-44-41" href="#__codelineno-44-41"></a><span class="sd">    Args:</span>
</span><span id="__span-44-42"><a id="__codelineno-44-42" name="__codelineno-44-42" href="#__codelineno-44-42"></a><span class="sd">        other (Data): The operand to multiply with.</span>
</span><span id="__span-44-43"><a id="__codelineno-44-43" name="__codelineno-44-43" href="#__codelineno-44-43"></a>
</span><span id="__span-44-44"><a id="__codelineno-44-44" name="__codelineno-44-44" href="#__codelineno-44-44"></a><span class="sd">    Returns:</span>
</span><span id="__span-44-45"><a id="__codelineno-44-45" name="__codelineno-44-45" href="#__codelineno-44-45"></a><span class="sd">        Tensor: The modified Tensor after in-place multiplication.</span>
</span><span id="__span-44-46"><a id="__codelineno-44-46" name="__codelineno-44-46" href="#__codelineno-44-46"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-44-47"><a id="__codelineno-44-47" name="__codelineno-44-47" href="#__codelineno-44-47"></a>
</span><span id="__span-44-48"><a id="__codelineno-44-48" name="__codelineno-44-48" href="#__codelineno-44-48"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">build_ndarray</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-44-49"><a id="__codelineno-44-49" name="__codelineno-44-49" href="#__codelineno-44-49"></a>    <span class="k">return</span> <span class="bp">self</span>
</span></code></pre></div>
<h2 id="logs-exponents-and-activation-functions">Logs, Exponents, and Activation Functions<a class="headerlink" href="#logs-exponents-and-activation-functions" title="Permanent link">&para;</a></h2>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/IM_W-RGMTVc?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Building PyTorch: Enriching MicroTorch with Logs, Exponents, and Activation Functions" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<h3 id="log"><code>log</code><a class="headerlink" href="#log" title="Permanent link">&para;</a></h3>
<p>The <strong>logarithmic</strong> operation computes the natural logarithm of each element in the tensor.</p>
<div class="arithmatex">\[f(x) = \log(x)\]</div>
<p>The derivative of <span class="arithmatex">\(\log(x)\)</span> is:</p>
<div class="arithmatex">\[\frac{d}{dx} \log(x) = \frac{1}{x}\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-45-1"><a id="__codelineno-45-1" name="__codelineno-45-1" href="#__codelineno-45-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-45-2"><a id="__codelineno-45-2" name="__codelineno-45-2" href="#__codelineno-45-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-45-3"><a id="__codelineno-45-3" name="__codelineno-45-3" href="#__codelineno-45-3"></a><span class="sd">    Computes the natural logarithm of all elements in the tensor.</span>
</span><span id="__span-45-4"><a id="__codelineno-45-4" name="__codelineno-45-4" href="#__codelineno-45-4"></a>
</span><span id="__span-45-5"><a id="__codelineno-45-5" name="__codelineno-45-5" href="#__codelineno-45-5"></a><span class="sd">    The logarithm is applied element-wise to the tensor&#39;s data. This function assumes that </span>
</span><span id="__span-45-6"><a id="__codelineno-45-6" name="__codelineno-45-6" href="#__codelineno-45-6"></a><span class="sd">    the data values are positive, as the logarithm of non-positive values is undefined.</span>
</span><span id="__span-45-7"><a id="__codelineno-45-7" name="__codelineno-45-7" href="#__codelineno-45-7"></a>
</span><span id="__span-45-8"><a id="__codelineno-45-8" name="__codelineno-45-8" href="#__codelineno-45-8"></a><span class="sd">    Returns:</span>
</span><span id="__span-45-9"><a id="__codelineno-45-9" name="__codelineno-45-9" href="#__codelineno-45-9"></a><span class="sd">        Tensor: A new tensor containing the element-wise natural logarithm of the input tensor.</span>
</span><span id="__span-45-10"><a id="__codelineno-45-10" name="__codelineno-45-10" href="#__codelineno-45-10"></a>
</span><span id="__span-45-11"><a id="__codelineno-45-11" name="__codelineno-45-11" href="#__codelineno-45-11"></a><span class="sd">    The natural logarithm of a value `x` is calculated as:</span>
</span><span id="__span-45-12"><a id="__codelineno-45-12" name="__codelineno-45-12" href="#__codelineno-45-12"></a><span class="sd">        log(x) = ln(x)</span>
</span><span id="__span-45-13"><a id="__codelineno-45-13" name="__codelineno-45-13" href="#__codelineno-45-13"></a>
</span><span id="__span-45-14"><a id="__codelineno-45-14" name="__codelineno-45-14" href="#__codelineno-45-14"></a><span class="sd">    The derivative of log(x) with respect to x is:</span>
</span><span id="__span-45-15"><a id="__codelineno-45-15" name="__codelineno-45-15" href="#__codelineno-45-15"></a><span class="sd">        d/dx log(x) = 1/x</span>
</span><span id="__span-45-16"><a id="__codelineno-45-16" name="__codelineno-45-16" href="#__codelineno-45-16"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-45-17"><a id="__codelineno-45-17" name="__codelineno-45-17" href="#__codelineno-45-17"></a>
</span><span id="__span-45-18"><a id="__codelineno-45-18" name="__codelineno-45-18" href="#__codelineno-45-18"></a>    <span class="c1"># Perform logarithmic operation on the data</span>
</span><span id="__span-45-19"><a id="__codelineno-45-19" name="__codelineno-45-19" href="#__codelineno-45-19"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-45-20"><a id="__codelineno-45-20" name="__codelineno-45-20" href="#__codelineno-45-20"></a>
</span><span id="__span-45-21"><a id="__codelineno-45-21" name="__codelineno-45-21" href="#__codelineno-45-21"></a>    <span class="c1"># Initialize an empty list for dependencies (used for backpropagation)</span>
</span><span id="__span-45-22"><a id="__codelineno-45-22" name="__codelineno-45-22" href="#__codelineno-45-22"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-45-23"><a id="__codelineno-45-23" name="__codelineno-45-23" href="#__codelineno-45-23"></a>
</span><span id="__span-45-24"><a id="__codelineno-45-24" name="__codelineno-45-24" href="#__codelineno-45-24"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-45-25"><a id="__codelineno-45-25" name="__codelineno-45-25" href="#__codelineno-45-25"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-45-26"><a id="__codelineno-45-26" name="__codelineno-45-26" href="#__codelineno-45-26"></a><span class="sd">        Backward pass for the logarithm operation.</span>
</span><span id="__span-45-27"><a id="__codelineno-45-27" name="__codelineno-45-27" href="#__codelineno-45-27"></a>
</span><span id="__span-45-28"><a id="__codelineno-45-28" name="__codelineno-45-28" href="#__codelineno-45-28"></a><span class="sd">        The derivative of the logarithm is 1/x, so we compute the gradient as:</span>
</span><span id="__span-45-29"><a id="__codelineno-45-29" name="__codelineno-45-29" href="#__codelineno-45-29"></a><span class="sd">            grad(x) = grad(x) / x</span>
</span><span id="__span-45-30"><a id="__codelineno-45-30" name="__codelineno-45-30" href="#__codelineno-45-30"></a>
</span><span id="__span-45-31"><a id="__codelineno-45-31" name="__codelineno-45-31" href="#__codelineno-45-31"></a><span class="sd">        Args:</span>
</span><span id="__span-45-32"><a id="__codelineno-45-32" name="__codelineno-45-32" href="#__codelineno-45-32"></a><span class="sd">            grad (np.ndarray): The gradient propagated from the next layer.</span>
</span><span id="__span-45-33"><a id="__codelineno-45-33" name="__codelineno-45-33" href="#__codelineno-45-33"></a>
</span><span id="__span-45-34"><a id="__codelineno-45-34" name="__codelineno-45-34" href="#__codelineno-45-34"></a><span class="sd">        Returns:</span>
</span><span id="__span-45-35"><a id="__codelineno-45-35" name="__codelineno-45-35" href="#__codelineno-45-35"></a><span class="sd">            np.ndarray: The gradient to propagate backward.</span>
</span><span id="__span-45-36"><a id="__codelineno-45-36" name="__codelineno-45-36" href="#__codelineno-45-36"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-45-37"><a id="__codelineno-45-37" name="__codelineno-45-37" href="#__codelineno-45-37"></a>
</span><span id="__span-45-38"><a id="__codelineno-45-38" name="__codelineno-45-38" href="#__codelineno-45-38"></a>        <span class="c1"># The derivative of log(x) is 1/x, so we divide the gradient by the data (x)</span>
</span><span id="__span-45-39"><a id="__codelineno-45-39" name="__codelineno-45-39" href="#__codelineno-45-39"></a>        <span class="k">return</span> <span class="n">grad</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
</span><span id="__span-45-40"><a id="__codelineno-45-40" name="__codelineno-45-40" href="#__codelineno-45-40"></a>
</span><span id="__span-45-41"><a id="__codelineno-45-41" name="__codelineno-45-41" href="#__codelineno-45-41"></a>    <span class="c1"># If the tensor requires gradients (i.e., it&#39;s part of the computation graph), </span>
</span><span id="__span-45-42"><a id="__codelineno-45-42" name="__codelineno-45-42" href="#__codelineno-45-42"></a>    <span class="c1"># we store the backward function in the dependencies.</span>
</span><span id="__span-45-43"><a id="__codelineno-45-43" name="__codelineno-45-43" href="#__codelineno-45-43"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-45-44"><a id="__codelineno-45-44" name="__codelineno-45-44" href="#__codelineno-45-44"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-45-45"><a id="__codelineno-45-45" name="__codelineno-45-45" href="#__codelineno-45-45"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-45-46"><a id="__codelineno-45-46" name="__codelineno-45-46" href="#__codelineno-45-46"></a>                <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
</span><span id="__span-45-47"><a id="__codelineno-45-47" name="__codelineno-45-47" href="#__codelineno-45-47"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span>
</span><span id="__span-45-48"><a id="__codelineno-45-48" name="__codelineno-45-48" href="#__codelineno-45-48"></a>            <span class="p">)</span>
</span><span id="__span-45-49"><a id="__codelineno-45-49" name="__codelineno-45-49" href="#__codelineno-45-49"></a>        <span class="p">)</span>
</span><span id="__span-45-50"><a id="__codelineno-45-50" name="__codelineno-45-50" href="#__codelineno-45-50"></a>
</span><span id="__span-45-51"><a id="__codelineno-45-51" name="__codelineno-45-51" href="#__codelineno-45-51"></a>    <span class="c1"># Return a new Tensor containing the result of the log operation and the necessary dependencies.</span>
</span><span id="__span-45-52"><a id="__codelineno-45-52" name="__codelineno-45-52" href="#__codelineno-45-52"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="tanh"><code>tanh</code><a class="headerlink" href="#tanh" title="Permanent link">&para;</a></h3>
<p>The <strong>tanh</strong> operation computes the hyperbolic tangent of each element in the tensor.</p>
<div class="arithmatex">\[\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\]</div>
<p>The derivative of <span class="arithmatex">\(\tanh(x)\)</span> is:</p>
<div class="arithmatex">\[\frac{d}{dx} \tanh(x) = 1 - \tanh^2(x)\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-46-1"><a id="__codelineno-46-1" name="__codelineno-46-1" href="#__codelineno-46-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-46-2"><a id="__codelineno-46-2" name="__codelineno-46-2" href="#__codelineno-46-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-46-3"><a id="__codelineno-46-3" name="__codelineno-46-3" href="#__codelineno-46-3"></a><span class="sd">    Computes the hyperbolic tangent (tanh) of all elements in the tensor.</span>
</span><span id="__span-46-4"><a id="__codelineno-46-4" name="__codelineno-46-4" href="#__codelineno-46-4"></a>
</span><span id="__span-46-5"><a id="__codelineno-46-5" name="__codelineno-46-5" href="#__codelineno-46-5"></a><span class="sd">    The hyperbolic tangent function is applied element-wise to the tensor&#39;s data. The tanh </span>
</span><span id="__span-46-6"><a id="__codelineno-46-6" name="__codelineno-46-6" href="#__codelineno-46-6"></a><span class="sd">    function maps the input values to the range (-1, 1).</span>
</span><span id="__span-46-7"><a id="__codelineno-46-7" name="__codelineno-46-7" href="#__codelineno-46-7"></a>
</span><span id="__span-46-8"><a id="__codelineno-46-8" name="__codelineno-46-8" href="#__codelineno-46-8"></a><span class="sd">    Returns:</span>
</span><span id="__span-46-9"><a id="__codelineno-46-9" name="__codelineno-46-9" href="#__codelineno-46-9"></a><span class="sd">        Tensor: A new tensor containing the element-wise hyperbolic tangent of the input tensor.</span>
</span><span id="__span-46-10"><a id="__codelineno-46-10" name="__codelineno-46-10" href="#__codelineno-46-10"></a>
</span><span id="__span-46-11"><a id="__codelineno-46-11" name="__codelineno-46-11" href="#__codelineno-46-11"></a><span class="sd">    The hyperbolic tangent function is defined as:</span>
</span><span id="__span-46-12"><a id="__codelineno-46-12" name="__codelineno-46-12" href="#__codelineno-46-12"></a><span class="sd">        tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))</span>
</span><span id="__span-46-13"><a id="__codelineno-46-13" name="__codelineno-46-13" href="#__codelineno-46-13"></a>
</span><span id="__span-46-14"><a id="__codelineno-46-14" name="__codelineno-46-14" href="#__codelineno-46-14"></a><span class="sd">    The derivative of tanh(x) with respect to x is:</span>
</span><span id="__span-46-15"><a id="__codelineno-46-15" name="__codelineno-46-15" href="#__codelineno-46-15"></a><span class="sd">        d/dx tanh(x) = 1 - tanh(x)^2</span>
</span><span id="__span-46-16"><a id="__codelineno-46-16" name="__codelineno-46-16" href="#__codelineno-46-16"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-46-17"><a id="__codelineno-46-17" name="__codelineno-46-17" href="#__codelineno-46-17"></a>
</span><span id="__span-46-18"><a id="__codelineno-46-18" name="__codelineno-46-18" href="#__codelineno-46-18"></a>    <span class="c1"># Perform hyperbolic tangent operation on the data</span>
</span><span id="__span-46-19"><a id="__codelineno-46-19" name="__codelineno-46-19" href="#__codelineno-46-19"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-46-20"><a id="__codelineno-46-20" name="__codelineno-46-20" href="#__codelineno-46-20"></a>
</span><span id="__span-46-21"><a id="__codelineno-46-21" name="__codelineno-46-21" href="#__codelineno-46-21"></a>    <span class="c1"># Initialize an empty list for dependencies (used for backpropagation)</span>
</span><span id="__span-46-22"><a id="__codelineno-46-22" name="__codelineno-46-22" href="#__codelineno-46-22"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-46-23"><a id="__codelineno-46-23" name="__codelineno-46-23" href="#__codelineno-46-23"></a>
</span><span id="__span-46-24"><a id="__codelineno-46-24" name="__codelineno-46-24" href="#__codelineno-46-24"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-46-25"><a id="__codelineno-46-25" name="__codelineno-46-25" href="#__codelineno-46-25"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-46-26"><a id="__codelineno-46-26" name="__codelineno-46-26" href="#__codelineno-46-26"></a><span class="sd">        Backward pass for the tanh operation.</span>
</span><span id="__span-46-27"><a id="__codelineno-46-27" name="__codelineno-46-27" href="#__codelineno-46-27"></a>
</span><span id="__span-46-28"><a id="__codelineno-46-28" name="__codelineno-46-28" href="#__codelineno-46-28"></a><span class="sd">        The derivative of tanh(x) is 1 - tanh(x)^2, so we compute the gradient as:</span>
</span><span id="__span-46-29"><a id="__codelineno-46-29" name="__codelineno-46-29" href="#__codelineno-46-29"></a><span class="sd">            grad(x) = grad(x) * (1 - tanh(x)^2)</span>
</span><span id="__span-46-30"><a id="__codelineno-46-30" name="__codelineno-46-30" href="#__codelineno-46-30"></a>
</span><span id="__span-46-31"><a id="__codelineno-46-31" name="__codelineno-46-31" href="#__codelineno-46-31"></a><span class="sd">        Args:</span>
</span><span id="__span-46-32"><a id="__codelineno-46-32" name="__codelineno-46-32" href="#__codelineno-46-32"></a><span class="sd">            grad (np.ndarray): The gradient propagated from the next layer.</span>
</span><span id="__span-46-33"><a id="__codelineno-46-33" name="__codelineno-46-33" href="#__codelineno-46-33"></a>
</span><span id="__span-46-34"><a id="__codelineno-46-34" name="__codelineno-46-34" href="#__codelineno-46-34"></a><span class="sd">        Returns:</span>
</span><span id="__span-46-35"><a id="__codelineno-46-35" name="__codelineno-46-35" href="#__codelineno-46-35"></a><span class="sd">            np.ndarray: The gradient to propagate backward.</span>
</span><span id="__span-46-36"><a id="__codelineno-46-36" name="__codelineno-46-36" href="#__codelineno-46-36"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-46-37"><a id="__codelineno-46-37" name="__codelineno-46-37" href="#__codelineno-46-37"></a>
</span><span id="__span-46-38"><a id="__codelineno-46-38" name="__codelineno-46-38" href="#__codelineno-46-38"></a>        <span class="c1"># The derivative of tanh(x) is 1 - tanh(x)^2, so we multiply the gradient by this value</span>
</span><span id="__span-46-39"><a id="__codelineno-46-39" name="__codelineno-46-39" href="#__codelineno-46-39"></a>        <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-46-40"><a id="__codelineno-46-40" name="__codelineno-46-40" href="#__codelineno-46-40"></a>
</span><span id="__span-46-41"><a id="__codelineno-46-41" name="__codelineno-46-41" href="#__codelineno-46-41"></a>    <span class="c1"># If the tensor requires gradients (i.e., it&#39;s part of the computation graph), </span>
</span><span id="__span-46-42"><a id="__codelineno-46-42" name="__codelineno-46-42" href="#__codelineno-46-42"></a>    <span class="c1"># we store the backward function in the dependencies.</span>
</span><span id="__span-46-43"><a id="__codelineno-46-43" name="__codelineno-46-43" href="#__codelineno-46-43"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-46-44"><a id="__codelineno-46-44" name="__codelineno-46-44" href="#__codelineno-46-44"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-46-45"><a id="__codelineno-46-45" name="__codelineno-46-45" href="#__codelineno-46-45"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-46-46"><a id="__codelineno-46-46" name="__codelineno-46-46" href="#__codelineno-46-46"></a>                <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
</span><span id="__span-46-47"><a id="__codelineno-46-47" name="__codelineno-46-47" href="#__codelineno-46-47"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span>
</span><span id="__span-46-48"><a id="__codelineno-46-48" name="__codelineno-46-48" href="#__codelineno-46-48"></a>            <span class="p">)</span>
</span><span id="__span-46-49"><a id="__codelineno-46-49" name="__codelineno-46-49" href="#__codelineno-46-49"></a>        <span class="p">)</span>
</span><span id="__span-46-50"><a id="__codelineno-46-50" name="__codelineno-46-50" href="#__codelineno-46-50"></a>
</span><span id="__span-46-51"><a id="__codelineno-46-51" name="__codelineno-46-51" href="#__codelineno-46-51"></a>    <span class="c1"># Return a new Tensor containing the result of the tanh operation and the necessary dependencies.</span>
</span><span id="__span-46-52"><a id="__codelineno-46-52" name="__codelineno-46-52" href="#__codelineno-46-52"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="pow"><code>pow</code><a class="headerlink" href="#pow" title="Permanent link">&para;</a></h3>
<p>The <strong>power</strong> operation raises each element of the tensor to the specified power.</p>
<div class="arithmatex">\[f(x) = x^p\]</div>
<p>The derivative of <span class="arithmatex">\(x^p\)</span> is:</p>
<div class="arithmatex">\[\frac{d}{dx} x^p = p \cdot x^{p-1}\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-47-1"><a id="__codelineno-47-1" name="__codelineno-47-1" href="#__codelineno-47-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">pow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">pow</span><span class="p">:</span> <span class="n">Scalar</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-47-2"><a id="__codelineno-47-2" name="__codelineno-47-2" href="#__codelineno-47-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-47-3"><a id="__codelineno-47-3" name="__codelineno-47-3" href="#__codelineno-47-3"></a><span class="sd">    Computes the element-wise power of the tensor&#39;s data.</span>
</span><span id="__span-47-4"><a id="__codelineno-47-4" name="__codelineno-47-4" href="#__codelineno-47-4"></a>
</span><span id="__span-47-5"><a id="__codelineno-47-5" name="__codelineno-47-5" href="#__codelineno-47-5"></a><span class="sd">    The operation applies the power function element-wise, raising each element in the tensor </span>
</span><span id="__span-47-6"><a id="__codelineno-47-6" name="__codelineno-47-6" href="#__codelineno-47-6"></a><span class="sd">    to the given power `pow`.</span>
</span><span id="__span-47-7"><a id="__codelineno-47-7" name="__codelineno-47-7" href="#__codelineno-47-7"></a>
</span><span id="__span-47-8"><a id="__codelineno-47-8" name="__codelineno-47-8" href="#__codelineno-47-8"></a><span class="sd">    Args:</span>
</span><span id="__span-47-9"><a id="__codelineno-47-9" name="__codelineno-47-9" href="#__codelineno-47-9"></a><span class="sd">        pow (Scalar): The exponent to which each element in the tensor should be raised.</span>
</span><span id="__span-47-10"><a id="__codelineno-47-10" name="__codelineno-47-10" href="#__codelineno-47-10"></a>
</span><span id="__span-47-11"><a id="__codelineno-47-11" name="__codelineno-47-11" href="#__codelineno-47-11"></a><span class="sd">    Returns:</span>
</span><span id="__span-47-12"><a id="__codelineno-47-12" name="__codelineno-47-12" href="#__codelineno-47-12"></a><span class="sd">        Tensor: A new tensor where each element is raised to the specified power.</span>
</span><span id="__span-47-13"><a id="__codelineno-47-13" name="__codelineno-47-13" href="#__codelineno-47-13"></a>
</span><span id="__span-47-14"><a id="__codelineno-47-14" name="__codelineno-47-14" href="#__codelineno-47-14"></a><span class="sd">    The power function is defined as:</span>
</span><span id="__span-47-15"><a id="__codelineno-47-15" name="__codelineno-47-15" href="#__codelineno-47-15"></a><span class="sd">        y = x^pow, where `x` is the input tensor&#39;s element and `pow` is the given exponent.</span>
</span><span id="__span-47-16"><a id="__codelineno-47-16" name="__codelineno-47-16" href="#__codelineno-47-16"></a>
</span><span id="__span-47-17"><a id="__codelineno-47-17" name="__codelineno-47-17" href="#__codelineno-47-17"></a><span class="sd">    The derivative of x^pow with respect to x is:</span>
</span><span id="__span-47-18"><a id="__codelineno-47-18" name="__codelineno-47-18" href="#__codelineno-47-18"></a><span class="sd">        d/dx (x^pow) = pow * x^(pow - 1)</span>
</span><span id="__span-47-19"><a id="__codelineno-47-19" name="__codelineno-47-19" href="#__codelineno-47-19"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-47-20"><a id="__codelineno-47-20" name="__codelineno-47-20" href="#__codelineno-47-20"></a>
</span><span id="__span-47-21"><a id="__codelineno-47-21" name="__codelineno-47-21" href="#__codelineno-47-21"></a>    <span class="c1"># Perform element-wise power operation (raise each element to the given power)</span>
</span><span id="__span-47-22"><a id="__codelineno-47-22" name="__codelineno-47-22" href="#__codelineno-47-22"></a>    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="nb">pow</span>
</span><span id="__span-47-23"><a id="__codelineno-47-23" name="__codelineno-47-23" href="#__codelineno-47-23"></a>
</span><span id="__span-47-24"><a id="__codelineno-47-24" name="__codelineno-47-24" href="#__codelineno-47-24"></a>    <span class="c1"># Initialize an empty list for dependencies (used for backpropagation)</span>
</span><span id="__span-47-25"><a id="__codelineno-47-25" name="__codelineno-47-25" href="#__codelineno-47-25"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-47-26"><a id="__codelineno-47-26" name="__codelineno-47-26" href="#__codelineno-47-26"></a>
</span><span id="__span-47-27"><a id="__codelineno-47-27" name="__codelineno-47-27" href="#__codelineno-47-27"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-47-28"><a id="__codelineno-47-28" name="__codelineno-47-28" href="#__codelineno-47-28"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-47-29"><a id="__codelineno-47-29" name="__codelineno-47-29" href="#__codelineno-47-29"></a><span class="sd">        Backward pass for the power operation.</span>
</span><span id="__span-47-30"><a id="__codelineno-47-30" name="__codelineno-47-30" href="#__codelineno-47-30"></a>
</span><span id="__span-47-31"><a id="__codelineno-47-31" name="__codelineno-47-31" href="#__codelineno-47-31"></a><span class="sd">        The derivative of x^pow with respect to x is:</span>
</span><span id="__span-47-32"><a id="__codelineno-47-32" name="__codelineno-47-32" href="#__codelineno-47-32"></a><span class="sd">            d/dx (x^pow) = pow * x^(pow - 1)</span>
</span><span id="__span-47-33"><a id="__codelineno-47-33" name="__codelineno-47-33" href="#__codelineno-47-33"></a>
</span><span id="__span-47-34"><a id="__codelineno-47-34" name="__codelineno-47-34" href="#__codelineno-47-34"></a><span class="sd">        We multiply the gradient by the derivative to propagate the gradient backward.</span>
</span><span id="__span-47-35"><a id="__codelineno-47-35" name="__codelineno-47-35" href="#__codelineno-47-35"></a>
</span><span id="__span-47-36"><a id="__codelineno-47-36" name="__codelineno-47-36" href="#__codelineno-47-36"></a><span class="sd">        Args:</span>
</span><span id="__span-47-37"><a id="__codelineno-47-37" name="__codelineno-47-37" href="#__codelineno-47-37"></a><span class="sd">            grad (np.ndarray): The gradient propagated from the next layer.</span>
</span><span id="__span-47-38"><a id="__codelineno-47-38" name="__codelineno-47-38" href="#__codelineno-47-38"></a>
</span><span id="__span-47-39"><a id="__codelineno-47-39" name="__codelineno-47-39" href="#__codelineno-47-39"></a><span class="sd">        Returns:</span>
</span><span id="__span-47-40"><a id="__codelineno-47-40" name="__codelineno-47-40" href="#__codelineno-47-40"></a><span class="sd">            np.ndarray: The gradient to propagate backward.</span>
</span><span id="__span-47-41"><a id="__codelineno-47-41" name="__codelineno-47-41" href="#__codelineno-47-41"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-47-42"><a id="__codelineno-47-42" name="__codelineno-47-42" href="#__codelineno-47-42"></a>        <span class="c1"># The derivative of x^pow is pow * x^(pow - 1), so we multiply the gradient by this value</span>
</span><span id="__span-47-43"><a id="__codelineno-47-43" name="__codelineno-47-43" href="#__codelineno-47-43"></a>        <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="p">(</span><span class="nb">pow</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">**</span><span class="p">(</span><span class="nb">pow</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span>
</span><span id="__span-47-44"><a id="__codelineno-47-44" name="__codelineno-47-44" href="#__codelineno-47-44"></a>
</span><span id="__span-47-45"><a id="__codelineno-47-45" name="__codelineno-47-45" href="#__codelineno-47-45"></a>    <span class="c1"># If the tensor requires gradients (i.e., it&#39;s part of the computation graph), </span>
</span><span id="__span-47-46"><a id="__codelineno-47-46" name="__codelineno-47-46" href="#__codelineno-47-46"></a>    <span class="c1"># we store the backward function in the dependencies.</span>
</span><span id="__span-47-47"><a id="__codelineno-47-47" name="__codelineno-47-47" href="#__codelineno-47-47"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-47-48"><a id="__codelineno-47-48" name="__codelineno-47-48" href="#__codelineno-47-48"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-47-49"><a id="__codelineno-47-49" name="__codelineno-47-49" href="#__codelineno-47-49"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-47-50"><a id="__codelineno-47-50" name="__codelineno-47-50" href="#__codelineno-47-50"></a>                <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
</span><span id="__span-47-51"><a id="__codelineno-47-51" name="__codelineno-47-51" href="#__codelineno-47-51"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span>
</span><span id="__span-47-52"><a id="__codelineno-47-52" name="__codelineno-47-52" href="#__codelineno-47-52"></a>            <span class="p">)</span>
</span><span id="__span-47-53"><a id="__codelineno-47-53" name="__codelineno-47-53" href="#__codelineno-47-53"></a>        <span class="p">)</span>
</span><span id="__span-47-54"><a id="__codelineno-47-54" name="__codelineno-47-54" href="#__codelineno-47-54"></a>
</span><span id="__span-47-55"><a id="__codelineno-47-55" name="__codelineno-47-55" href="#__codelineno-47-55"></a>    <span class="c1"># Return a new Tensor containing the result of the power operation and the necessary dependencies.</span>
</span><span id="__span-47-56"><a id="__codelineno-47-56" name="__codelineno-47-56" href="#__codelineno-47-56"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>To enable exponentiation using the <code>**</code> operator, we overload the <code>__pow__</code> method. This allows us to perform element-wise power operations naturally, like:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-48-1"><a id="__codelineno-48-1" name="__codelineno-48-1" href="#__codelineno-48-1"></a><span class="n">tensor</span> <span class="o">**</span> <span class="mi">2</span>  <span class="c1"># Equivalent to tensor.pow(2)</span>
</span></code></pre></div>
<p><strong>This's more intuitive syntax:</strong> <code>tensor ** 2</code> instead of <code>tensor.pow(2)</code>.</p>
<p>Internally, <code>__pow__</code> simply calls the <code>pow</code> method, which handles the power operation while ensuring proper gradient computation.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-49-1"><a id="__codelineno-49-1" name="__codelineno-49-1" href="#__codelineno-49-1"></a><span class="k">def</span><span class="w"> </span><span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">pow</span><span class="p">:</span> <span class="n">Scalar</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-49-2"><a id="__codelineno-49-2" name="__codelineno-49-2" href="#__codelineno-49-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-49-3"><a id="__codelineno-49-3" name="__codelineno-49-3" href="#__codelineno-49-3"></a><span class="sd">    Overload the `**` operator for element-wise exponentiation.</span>
</span><span id="__span-49-4"><a id="__codelineno-49-4" name="__codelineno-49-4" href="#__codelineno-49-4"></a>
</span><span id="__span-49-5"><a id="__codelineno-49-5" name="__codelineno-49-5" href="#__codelineno-49-5"></a><span class="sd">    Args:</span>
</span><span id="__span-49-6"><a id="__codelineno-49-6" name="__codelineno-49-6" href="#__codelineno-49-6"></a><span class="sd">        pow (Scalar): The exponent to raise the tensor to.</span>
</span><span id="__span-49-7"><a id="__codelineno-49-7" name="__codelineno-49-7" href="#__codelineno-49-7"></a>
</span><span id="__span-49-8"><a id="__codelineno-49-8" name="__codelineno-49-8" href="#__codelineno-49-8"></a><span class="sd">    Returns:</span>
</span><span id="__span-49-9"><a id="__codelineno-49-9" name="__codelineno-49-9" href="#__codelineno-49-9"></a><span class="sd">        Tensor: A new tensor with each element raised to the given power.</span>
</span><span id="__span-49-10"><a id="__codelineno-49-10" name="__codelineno-49-10" href="#__codelineno-49-10"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-49-11"><a id="__codelineno-49-11" name="__codelineno-49-11" href="#__codelineno-49-11"></a>
</span><span id="__span-49-12"><a id="__codelineno-49-12" name="__codelineno-49-12" href="#__codelineno-49-12"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="nb">pow</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="division">division<a class="headerlink" href="#division" title="Permanent link">&para;</a></h3>
<p>The last set of operator overloads covers division. These methods ensure that the <code>/</code> operator works naturally with Tensors and scalars, handling both standard and in-place division. Since division can be expressed as multiplication by the reciprocal, we reuse the <code>**</code> operator to compute the inverse.  </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-50-1"><a id="__codelineno-50-1" name="__codelineno-50-1" href="#__codelineno-50-1"></a><span class="k">def</span><span class="w"> </span><span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-50-2"><a id="__codelineno-50-2" name="__codelineno-50-2" href="#__codelineno-50-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-50-3"><a id="__codelineno-50-3" name="__codelineno-50-3" href="#__codelineno-50-3"></a><span class="sd">    Overloads the `/` operator for element-wise division.</span>
</span><span id="__span-50-4"><a id="__codelineno-50-4" name="__codelineno-50-4" href="#__codelineno-50-4"></a>
</span><span id="__span-50-5"><a id="__codelineno-50-5" name="__codelineno-50-5" href="#__codelineno-50-5"></a><span class="sd">    Instead of direct division, this method multiplies the Tensor by the</span>
</span><span id="__span-50-6"><a id="__codelineno-50-6" name="__codelineno-50-6" href="#__codelineno-50-6"></a><span class="sd">    reciprocal of `other`, ensuring compatibility with automatic differentiation.</span>
</span><span id="__span-50-7"><a id="__codelineno-50-7" name="__codelineno-50-7" href="#__codelineno-50-7"></a>
</span><span id="__span-50-8"><a id="__codelineno-50-8" name="__codelineno-50-8" href="#__codelineno-50-8"></a><span class="sd">    Args:</span>
</span><span id="__span-50-9"><a id="__codelineno-50-9" name="__codelineno-50-9" href="#__codelineno-50-9"></a><span class="sd">        other (Data): The divisor, which can be a Tensor or a scalar.</span>
</span><span id="__span-50-10"><a id="__codelineno-50-10" name="__codelineno-50-10" href="#__codelineno-50-10"></a>
</span><span id="__span-50-11"><a id="__codelineno-50-11" name="__codelineno-50-11" href="#__codelineno-50-11"></a><span class="sd">    Returns:</span>
</span><span id="__span-50-12"><a id="__codelineno-50-12" name="__codelineno-50-12" href="#__codelineno-50-12"></a><span class="sd">        Tensor: A new Tensor representing the division result.</span>
</span><span id="__span-50-13"><a id="__codelineno-50-13" name="__codelineno-50-13" href="#__codelineno-50-13"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-50-14"><a id="__codelineno-50-14" name="__codelineno-50-14" href="#__codelineno-50-14"></a>
</span><span id="__span-50-15"><a id="__codelineno-50-15" name="__codelineno-50-15" href="#__codelineno-50-15"></a>    <span class="n">other</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-50-16"><a id="__codelineno-50-16" name="__codelineno-50-16" href="#__codelineno-50-16"></a>    <span class="k">return</span> <span class="bp">self</span> <span class="o">*</span> <span class="p">(</span><span class="n">other</span><span class="o">**-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-50-17"><a id="__codelineno-50-17" name="__codelineno-50-17" href="#__codelineno-50-17"></a>
</span><span id="__span-50-18"><a id="__codelineno-50-18" name="__codelineno-50-18" href="#__codelineno-50-18"></a><span class="k">def</span><span class="w"> </span><span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-50-19"><a id="__codelineno-50-19" name="__codelineno-50-19" href="#__codelineno-50-19"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-50-20"><a id="__codelineno-50-20" name="__codelineno-50-20" href="#__codelineno-50-20"></a><span class="sd">    Overloads the right-hand `/` operator.</span>
</span><span id="__span-50-21"><a id="__codelineno-50-21" name="__codelineno-50-21" href="#__codelineno-50-21"></a>
</span><span id="__span-50-22"><a id="__codelineno-50-22" name="__codelineno-50-22" href="#__codelineno-50-22"></a><span class="sd">    This ensures that division works correctly when a scalar or another</span>
</span><span id="__span-50-23"><a id="__codelineno-50-23" name="__codelineno-50-23" href="#__codelineno-50-23"></a><span class="sd">    compatible type appears on the left side of the `/` operator.</span>
</span><span id="__span-50-24"><a id="__codelineno-50-24" name="__codelineno-50-24" href="#__codelineno-50-24"></a>
</span><span id="__span-50-25"><a id="__codelineno-50-25" name="__codelineno-50-25" href="#__codelineno-50-25"></a><span class="sd">    Args:</span>
</span><span id="__span-50-26"><a id="__codelineno-50-26" name="__codelineno-50-26" href="#__codelineno-50-26"></a><span class="sd">        other (Data): The numerator, which can be a scalar or Tensor.</span>
</span><span id="__span-50-27"><a id="__codelineno-50-27" name="__codelineno-50-27" href="#__codelineno-50-27"></a>
</span><span id="__span-50-28"><a id="__codelineno-50-28" name="__codelineno-50-28" href="#__codelineno-50-28"></a><span class="sd">    Returns:</span>
</span><span id="__span-50-29"><a id="__codelineno-50-29" name="__codelineno-50-29" href="#__codelineno-50-29"></a><span class="sd">        Tensor: A new Tensor representing the division result.</span>
</span><span id="__span-50-30"><a id="__codelineno-50-30" name="__codelineno-50-30" href="#__codelineno-50-30"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-50-31"><a id="__codelineno-50-31" name="__codelineno-50-31" href="#__codelineno-50-31"></a>
</span><span id="__span-50-32"><a id="__codelineno-50-32" name="__codelineno-50-32" href="#__codelineno-50-32"></a>    <span class="n">other</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-50-33"><a id="__codelineno-50-33" name="__codelineno-50-33" href="#__codelineno-50-33"></a>    <span class="k">return</span> <span class="n">other</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">**-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-50-34"><a id="__codelineno-50-34" name="__codelineno-50-34" href="#__codelineno-50-34"></a>
</span><span id="__span-50-35"><a id="__codelineno-50-35" name="__codelineno-50-35" href="#__codelineno-50-35"></a><span class="k">def</span><span class="w"> </span><span class="fm">__itruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-50-36"><a id="__codelineno-50-36" name="__codelineno-50-36" href="#__codelineno-50-36"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-50-37"><a id="__codelineno-50-37" name="__codelineno-50-37" href="#__codelineno-50-37"></a><span class="sd">    Overloads the `/=` operator for in-place division.</span>
</span><span id="__span-50-38"><a id="__codelineno-50-38" name="__codelineno-50-38" href="#__codelineno-50-38"></a>
</span><span id="__span-50-39"><a id="__codelineno-50-39" name="__codelineno-50-39" href="#__codelineno-50-39"></a><span class="sd">    This modifies the Tensors data directly, improving efficiency.</span>
</span><span id="__span-50-40"><a id="__codelineno-50-40" name="__codelineno-50-40" href="#__codelineno-50-40"></a><span class="sd">    However, in-place operations do not support automatic differentiation</span>
</span><span id="__span-50-41"><a id="__codelineno-50-41" name="__codelineno-50-41" href="#__codelineno-50-41"></a><span class="sd">    (i.e., gradients will not be tracked).</span>
</span><span id="__span-50-42"><a id="__codelineno-50-42" name="__codelineno-50-42" href="#__codelineno-50-42"></a>
</span><span id="__span-50-43"><a id="__codelineno-50-43" name="__codelineno-50-43" href="#__codelineno-50-43"></a><span class="sd">    Args:</span>
</span><span id="__span-50-44"><a id="__codelineno-50-44" name="__codelineno-50-44" href="#__codelineno-50-44"></a><span class="sd">        other (Data): The divisor for in-place division.</span>
</span><span id="__span-50-45"><a id="__codelineno-50-45" name="__codelineno-50-45" href="#__codelineno-50-45"></a>
</span><span id="__span-50-46"><a id="__codelineno-50-46" name="__codelineno-50-46" href="#__codelineno-50-46"></a><span class="sd">    Returns:</span>
</span><span id="__span-50-47"><a id="__codelineno-50-47" name="__codelineno-50-47" href="#__codelineno-50-47"></a><span class="sd">        Tensor: The modified Tensor after in-place division.</span>
</span><span id="__span-50-48"><a id="__codelineno-50-48" name="__codelineno-50-48" href="#__codelineno-50-48"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-50-49"><a id="__codelineno-50-49" name="__codelineno-50-49" href="#__codelineno-50-49"></a>
</span><span id="__span-50-50"><a id="__codelineno-50-50" name="__codelineno-50-50" href="#__codelineno-50-50"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">build_ndarray</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-50-51"><a id="__codelineno-50-51" name="__codelineno-50-51" href="#__codelineno-50-51"></a>    <span class="k">return</span> <span class="bp">self</span>
</span></code></pre></div>
<h3 id="exp"><code>exp</code><a class="headerlink" href="#exp" title="Permanent link">&para;</a></h3>
<p>The <strong>exponential</strong> operation computes the exponent (base <span class="arithmatex">\(e\)</span>) of each element in the tensor.</p>
<div class="arithmatex">\[\exp(x) = e^x\]</div>
<p>The derivative of <span class="arithmatex">\(e^x\)</span> is:</p>
<div class="arithmatex">\[\frac{d}{dx} e^x = e^x\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-51-1"><a id="__codelineno-51-1" name="__codelineno-51-1" href="#__codelineno-51-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-51-2"><a id="__codelineno-51-2" name="__codelineno-51-2" href="#__codelineno-51-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-51-3"><a id="__codelineno-51-3" name="__codelineno-51-3" href="#__codelineno-51-3"></a><span class="sd">    Computes the element-wise exponential of the tensor&#39;s data.</span>
</span><span id="__span-51-4"><a id="__codelineno-51-4" name="__codelineno-51-4" href="#__codelineno-51-4"></a>
</span><span id="__span-51-5"><a id="__codelineno-51-5" name="__codelineno-51-5" href="#__codelineno-51-5"></a><span class="sd">    The operation applies the exponential function element-wise, raising the constant e </span>
</span><span id="__span-51-6"><a id="__codelineno-51-6" name="__codelineno-51-6" href="#__codelineno-51-6"></a><span class="sd">    (Euler&#39;s number) to the power of each element in the tensor.</span>
</span><span id="__span-51-7"><a id="__codelineno-51-7" name="__codelineno-51-7" href="#__codelineno-51-7"></a>
</span><span id="__span-51-8"><a id="__codelineno-51-8" name="__codelineno-51-8" href="#__codelineno-51-8"></a><span class="sd">    Returns:</span>
</span><span id="__span-51-9"><a id="__codelineno-51-9" name="__codelineno-51-9" href="#__codelineno-51-9"></a><span class="sd">        Tensor: A new tensor where each element is the exponential of the corresponding element </span>
</span><span id="__span-51-10"><a id="__codelineno-51-10" name="__codelineno-51-10" href="#__codelineno-51-10"></a><span class="sd">        in the input tensor.</span>
</span><span id="__span-51-11"><a id="__codelineno-51-11" name="__codelineno-51-11" href="#__codelineno-51-11"></a>
</span><span id="__span-51-12"><a id="__codelineno-51-12" name="__codelineno-51-12" href="#__codelineno-51-12"></a><span class="sd">    The exponential function is defined as:</span>
</span><span id="__span-51-13"><a id="__codelineno-51-13" name="__codelineno-51-13" href="#__codelineno-51-13"></a><span class="sd">        y = e^x, where `e` is Euler&#39;s number (approximately 2.71828), and `x` is the input tensor&#39;s element.</span>
</span><span id="__span-51-14"><a id="__codelineno-51-14" name="__codelineno-51-14" href="#__codelineno-51-14"></a>
</span><span id="__span-51-15"><a id="__codelineno-51-15" name="__codelineno-51-15" href="#__codelineno-51-15"></a><span class="sd">    The derivative of e^x with respect to x is:</span>
</span><span id="__span-51-16"><a id="__codelineno-51-16" name="__codelineno-51-16" href="#__codelineno-51-16"></a><span class="sd">        d/dx (e^x) = e^x</span>
</span><span id="__span-51-17"><a id="__codelineno-51-17" name="__codelineno-51-17" href="#__codelineno-51-17"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-51-18"><a id="__codelineno-51-18" name="__codelineno-51-18" href="#__codelineno-51-18"></a>
</span><span id="__span-51-19"><a id="__codelineno-51-19" name="__codelineno-51-19" href="#__codelineno-51-19"></a>    <span class="c1"># Perform element-wise exponential operation (raise e to the power of each element)</span>
</span><span id="__span-51-20"><a id="__codelineno-51-20" name="__codelineno-51-20" href="#__codelineno-51-20"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-51-21"><a id="__codelineno-51-21" name="__codelineno-51-21" href="#__codelineno-51-21"></a>
</span><span id="__span-51-22"><a id="__codelineno-51-22" name="__codelineno-51-22" href="#__codelineno-51-22"></a>    <span class="c1"># Initialize an empty list for dependencies (used for backpropagation)</span>
</span><span id="__span-51-23"><a id="__codelineno-51-23" name="__codelineno-51-23" href="#__codelineno-51-23"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-51-24"><a id="__codelineno-51-24" name="__codelineno-51-24" href="#__codelineno-51-24"></a>
</span><span id="__span-51-25"><a id="__codelineno-51-25" name="__codelineno-51-25" href="#__codelineno-51-25"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-51-26"><a id="__codelineno-51-26" name="__codelineno-51-26" href="#__codelineno-51-26"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-51-27"><a id="__codelineno-51-27" name="__codelineno-51-27" href="#__codelineno-51-27"></a><span class="sd">        Backward pass for the exponential operation.</span>
</span><span id="__span-51-28"><a id="__codelineno-51-28" name="__codelineno-51-28" href="#__codelineno-51-28"></a>
</span><span id="__span-51-29"><a id="__codelineno-51-29" name="__codelineno-51-29" href="#__codelineno-51-29"></a><span class="sd">        The derivative of e^x with respect to x is:</span>
</span><span id="__span-51-30"><a id="__codelineno-51-30" name="__codelineno-51-30" href="#__codelineno-51-30"></a><span class="sd">            d/dx (e^x) = e^x</span>
</span><span id="__span-51-31"><a id="__codelineno-51-31" name="__codelineno-51-31" href="#__codelineno-51-31"></a>
</span><span id="__span-51-32"><a id="__codelineno-51-32" name="__codelineno-51-32" href="#__codelineno-51-32"></a><span class="sd">        We multiply the gradient by e^x to propagate the gradient backward.</span>
</span><span id="__span-51-33"><a id="__codelineno-51-33" name="__codelineno-51-33" href="#__codelineno-51-33"></a>
</span><span id="__span-51-34"><a id="__codelineno-51-34" name="__codelineno-51-34" href="#__codelineno-51-34"></a><span class="sd">        Args:</span>
</span><span id="__span-51-35"><a id="__codelineno-51-35" name="__codelineno-51-35" href="#__codelineno-51-35"></a><span class="sd">            grad (np.ndarray): The gradient propagated from the next layer.</span>
</span><span id="__span-51-36"><a id="__codelineno-51-36" name="__codelineno-51-36" href="#__codelineno-51-36"></a>
</span><span id="__span-51-37"><a id="__codelineno-51-37" name="__codelineno-51-37" href="#__codelineno-51-37"></a><span class="sd">        Returns:</span>
</span><span id="__span-51-38"><a id="__codelineno-51-38" name="__codelineno-51-38" href="#__codelineno-51-38"></a><span class="sd">            np.ndarray: The gradient to propagate backward.</span>
</span><span id="__span-51-39"><a id="__codelineno-51-39" name="__codelineno-51-39" href="#__codelineno-51-39"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-51-40"><a id="__codelineno-51-40" name="__codelineno-51-40" href="#__codelineno-51-40"></a>        <span class="c1"># The derivative of e^x is e^x, so we multiply the gradient by the output value</span>
</span><span id="__span-51-41"><a id="__codelineno-51-41" name="__codelineno-51-41" href="#__codelineno-51-41"></a>        <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">output</span>
</span><span id="__span-51-42"><a id="__codelineno-51-42" name="__codelineno-51-42" href="#__codelineno-51-42"></a>
</span><span id="__span-51-43"><a id="__codelineno-51-43" name="__codelineno-51-43" href="#__codelineno-51-43"></a>    <span class="c1"># If the tensor requires gradients (i.e., it&#39;s part of the computation graph), </span>
</span><span id="__span-51-44"><a id="__codelineno-51-44" name="__codelineno-51-44" href="#__codelineno-51-44"></a>    <span class="c1"># we store the backward function in the dependencies.</span>
</span><span id="__span-51-45"><a id="__codelineno-51-45" name="__codelineno-51-45" href="#__codelineno-51-45"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-51-46"><a id="__codelineno-51-46" name="__codelineno-51-46" href="#__codelineno-51-46"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-51-47"><a id="__codelineno-51-47" name="__codelineno-51-47" href="#__codelineno-51-47"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-51-48"><a id="__codelineno-51-48" name="__codelineno-51-48" href="#__codelineno-51-48"></a>                <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
</span><span id="__span-51-49"><a id="__codelineno-51-49" name="__codelineno-51-49" href="#__codelineno-51-49"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span>
</span><span id="__span-51-50"><a id="__codelineno-51-50" name="__codelineno-51-50" href="#__codelineno-51-50"></a>            <span class="p">)</span>
</span><span id="__span-51-51"><a id="__codelineno-51-51" name="__codelineno-51-51" href="#__codelineno-51-51"></a>        <span class="p">)</span>
</span><span id="__span-51-52"><a id="__codelineno-51-52" name="__codelineno-51-52" href="#__codelineno-51-52"></a>
</span><span id="__span-51-53"><a id="__codelineno-51-53" name="__codelineno-51-53" href="#__codelineno-51-53"></a>    <span class="c1"># Return a new Tensor containing the result of the exponential operation and the necessary dependencies.</span>
</span><span id="__span-51-54"><a id="__codelineno-51-54" name="__codelineno-51-54" href="#__codelineno-51-54"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<h2 id="simple-activation-functions">Simple activation functions<a class="headerlink" href="#simple-activation-functions" title="Permanent link">&para;</a></h2>
<p>Because we prepared the derivative computations inside the <code>Tensor</code> class we don't need anything but the <code>forward</code> implemenatation for the activation functions! It would be a great example of the autogradient power.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-52-1"><a id="__codelineno-52-1" name="__codelineno-52-1" href="#__codelineno-52-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">Tanh</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-52-2"><a id="__codelineno-52-2" name="__codelineno-52-2" href="#__codelineno-52-2"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-52-3"><a id="__codelineno-52-3" name="__codelineno-52-3" href="#__codelineno-52-3"></a>        <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
</span><span id="__span-52-4"><a id="__codelineno-52-4" name="__codelineno-52-4" href="#__codelineno-52-4"></a>
</span><span id="__span-52-5"><a id="__codelineno-52-5" name="__codelineno-52-5" href="#__codelineno-52-5"></a><span class="k">class</span><span class="w"> </span><span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-52-6"><a id="__codelineno-52-6" name="__codelineno-52-6" href="#__codelineno-52-6"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-52-7"><a id="__codelineno-52-7" name="__codelineno-52-7" href="#__codelineno-52-7"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">))</span>
</span><span id="__span-52-8"><a id="__codelineno-52-8" name="__codelineno-52-8" href="#__codelineno-52-8"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span>
</span></code></pre></div>
<p>The <code>backward</code> step is fully on the <code>Tensor</code> class side. It's beautiful, isn't it?</p>
<h2 id="more-ops">More Ops<a class="headerlink" href="#more-ops" title="Permanent link">&para;</a></h2>
<p>In this section, we extend the functionality of the <code>Tensor</code> class by adding more fundamental operations. These operations are essential for building neural networks. For each operation, we provide the reasoning behind it, the derivative (for backpropagation), and the implementation of the forward and backward passes.</p>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/9EaSwTdoTag?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Building PyTorch: Enhancing MicroTorch with Squeeze, View, and Clip Operations" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<h3 id="squeeze-and-unsqueeze-operations">Squeeze and Unsqueeze Operations<a class="headerlink" href="#squeeze-and-unsqueeze-operations" title="Permanent link">&para;</a></h3>
<p>The <code>squeeze</code> and <code>unsqueeze</code> methods simplify reshaping operations, making it easier to adjust tensor shapes. These methods modify tensor dimensions by removing or adding singleton dimensions while preserving gradient tracking.</p>
<p>The <code>squeeze</code> method removes dimensions of size 1 from a specified axis, and its backward function ensures gradients are expanded back to their original shape during backpropagation. If <code>axis</code> is specified we only remove certain singleton dimensions, so during the backward pass, we must restore those specific dimensions. If <code>axis=None</code> we reshape the gradient to the original tensor's shape.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-53-1"><a id="__codelineno-53-1" name="__codelineno-53-1" href="#__codelineno-53-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-53-2"><a id="__codelineno-53-2" name="__codelineno-53-2" href="#__codelineno-53-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-53-3"><a id="__codelineno-53-3" name="__codelineno-53-3" href="#__codelineno-53-3"></a><span class="sd">    Removes dimensions of size 1 from the specified axis.</span>
</span><span id="__span-53-4"><a id="__codelineno-53-4" name="__codelineno-53-4" href="#__codelineno-53-4"></a>
</span><span id="__span-53-5"><a id="__codelineno-53-5" name="__codelineno-53-5" href="#__codelineno-53-5"></a><span class="sd">    Args:</span>
</span><span id="__span-53-6"><a id="__codelineno-53-6" name="__codelineno-53-6" href="#__codelineno-53-6"></a><span class="sd">        dim (int or Tuple[int]): The axis or axes to squeeze. Defaults to None</span>
</span><span id="__span-53-7"><a id="__codelineno-53-7" name="__codelineno-53-7" href="#__codelineno-53-7"></a>
</span><span id="__span-53-8"><a id="__codelineno-53-8" name="__codelineno-53-8" href="#__codelineno-53-8"></a><span class="sd">    Returns:</span>
</span><span id="__span-53-9"><a id="__codelineno-53-9" name="__codelineno-53-9" href="#__codelineno-53-9"></a><span class="sd">        Tensor: The squeezed tensor with tracked dependencies.</span>
</span><span id="__span-53-10"><a id="__codelineno-53-10" name="__codelineno-53-10" href="#__codelineno-53-10"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-53-11"><a id="__codelineno-53-11" name="__codelineno-53-11" href="#__codelineno-53-11"></a>
</span><span id="__span-53-12"><a id="__codelineno-53-12" name="__codelineno-53-12" href="#__codelineno-53-12"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
</span><span id="__span-53-13"><a id="__codelineno-53-13" name="__codelineno-53-13" href="#__codelineno-53-13"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-53-14"><a id="__codelineno-53-14" name="__codelineno-53-14" href="#__codelineno-53-14"></a>
</span><span id="__span-53-15"><a id="__codelineno-53-15" name="__codelineno-53-15" href="#__codelineno-53-15"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-53-16"><a id="__codelineno-53-16" name="__codelineno-53-16" href="#__codelineno-53-16"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-53-17"><a id="__codelineno-53-17" name="__codelineno-53-17" href="#__codelineno-53-17"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-53-18"><a id="__codelineno-53-18" name="__codelineno-53-18" href="#__codelineno-53-18"></a><span class="sd">            Backward function for squeeze operation.</span>
</span><span id="__span-53-19"><a id="__codelineno-53-19" name="__codelineno-53-19" href="#__codelineno-53-19"></a>
</span><span id="__span-53-20"><a id="__codelineno-53-20" name="__codelineno-53-20" href="#__codelineno-53-20"></a><span class="sd">            Args:</span>
</span><span id="__span-53-21"><a id="__codelineno-53-21" name="__codelineno-53-21" href="#__codelineno-53-21"></a><span class="sd">                grad (np.ndarray): Incoming gradient.</span>
</span><span id="__span-53-22"><a id="__codelineno-53-22" name="__codelineno-53-22" href="#__codelineno-53-22"></a>
</span><span id="__span-53-23"><a id="__codelineno-53-23" name="__codelineno-53-23" href="#__codelineno-53-23"></a><span class="sd">            Returns:</span>
</span><span id="__span-53-24"><a id="__codelineno-53-24" name="__codelineno-53-24" href="#__codelineno-53-24"></a><span class="sd">                np.ndarray: Expanded gradient to match original dimensions.</span>
</span><span id="__span-53-25"><a id="__codelineno-53-25" name="__codelineno-53-25" href="#__codelineno-53-25"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-53-26"><a id="__codelineno-53-26" name="__codelineno-53-26" href="#__codelineno-53-26"></a>
</span><span id="__span-53-27"><a id="__codelineno-53-27" name="__codelineno-53-27" href="#__codelineno-53-27"></a>            <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-53-28"><a id="__codelineno-53-28" name="__codelineno-53-28" href="#__codelineno-53-28"></a>                <span class="c1"># Reshape the gradient to the original tensor&#39;s shape</span>
</span><span id="__span-53-29"><a id="__codelineno-53-29" name="__codelineno-53-29" href="#__codelineno-53-29"></a>                <span class="k">return</span> <span class="n">grad</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="__span-53-30"><a id="__codelineno-53-30" name="__codelineno-53-30" href="#__codelineno-53-30"></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span><span id="__span-53-31"><a id="__codelineno-53-31" name="__codelineno-53-31" href="#__codelineno-53-31"></a>
</span><span id="__span-53-32"><a id="__codelineno-53-32" name="__codelineno-53-32" href="#__codelineno-53-32"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-53-33"><a id="__codelineno-53-33" name="__codelineno-53-33" href="#__codelineno-53-33"></a>            <span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span><span class="p">)</span>
</span><span id="__span-53-34"><a id="__codelineno-53-34" name="__codelineno-53-34" href="#__codelineno-53-34"></a>        <span class="p">)</span>
</span><span id="__span-53-35"><a id="__codelineno-53-35" name="__codelineno-53-35" href="#__codelineno-53-35"></a>
</span><span id="__span-53-36"><a id="__codelineno-53-36" name="__codelineno-53-36" href="#__codelineno-53-36"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>The <code>unsqueeze</code> method adds a singleton dimension at the specified axis. The backward function removes this dimension during gradient propagation.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-54-1"><a id="__codelineno-54-1" name="__codelineno-54-1" href="#__codelineno-54-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-54-2"><a id="__codelineno-54-2" name="__codelineno-54-2" href="#__codelineno-54-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-54-3"><a id="__codelineno-54-3" name="__codelineno-54-3" href="#__codelineno-54-3"></a><span class="sd">    Adds a singleton dimension at the specified axis.</span>
</span><span id="__span-54-4"><a id="__codelineno-54-4" name="__codelineno-54-4" href="#__codelineno-54-4"></a>
</span><span id="__span-54-5"><a id="__codelineno-54-5" name="__codelineno-54-5" href="#__codelineno-54-5"></a><span class="sd">    Args:</span>
</span><span id="__span-54-6"><a id="__codelineno-54-6" name="__codelineno-54-6" href="#__codelineno-54-6"></a><span class="sd">        dim (Tuple[int]): The axis at which to insert a new dimension</span>
</span><span id="__span-54-7"><a id="__codelineno-54-7" name="__codelineno-54-7" href="#__codelineno-54-7"></a>
</span><span id="__span-54-8"><a id="__codelineno-54-8" name="__codelineno-54-8" href="#__codelineno-54-8"></a><span class="sd">    Returns:</span>
</span><span id="__span-54-9"><a id="__codelineno-54-9" name="__codelineno-54-9" href="#__codelineno-54-9"></a><span class="sd">        Tensor: The unsqueezed tensor with tracked dependencies.</span>
</span><span id="__span-54-10"><a id="__codelineno-54-10" name="__codelineno-54-10" href="#__codelineno-54-10"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-54-11"><a id="__codelineno-54-11" name="__codelineno-54-11" href="#__codelineno-54-11"></a>
</span><span id="__span-54-12"><a id="__codelineno-54-12" name="__codelineno-54-12" href="#__codelineno-54-12"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
</span><span id="__span-54-13"><a id="__codelineno-54-13" name="__codelineno-54-13" href="#__codelineno-54-13"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-54-14"><a id="__codelineno-54-14" name="__codelineno-54-14" href="#__codelineno-54-14"></a>
</span><span id="__span-54-15"><a id="__codelineno-54-15" name="__codelineno-54-15" href="#__codelineno-54-15"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-54-16"><a id="__codelineno-54-16" name="__codelineno-54-16" href="#__codelineno-54-16"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-54-17"><a id="__codelineno-54-17" name="__codelineno-54-17" href="#__codelineno-54-17"></a><span class="sd">        Backward function for unsqueeze operation.</span>
</span><span id="__span-54-18"><a id="__codelineno-54-18" name="__codelineno-54-18" href="#__codelineno-54-18"></a>
</span><span id="__span-54-19"><a id="__codelineno-54-19" name="__codelineno-54-19" href="#__codelineno-54-19"></a><span class="sd">        Args:</span>
</span><span id="__span-54-20"><a id="__codelineno-54-20" name="__codelineno-54-20" href="#__codelineno-54-20"></a><span class="sd">            grad (np.ndarray): Incoming gradient.</span>
</span><span id="__span-54-21"><a id="__codelineno-54-21" name="__codelineno-54-21" href="#__codelineno-54-21"></a>
</span><span id="__span-54-22"><a id="__codelineno-54-22" name="__codelineno-54-22" href="#__codelineno-54-22"></a><span class="sd">        Returns:</span>
</span><span id="__span-54-23"><a id="__codelineno-54-23" name="__codelineno-54-23" href="#__codelineno-54-23"></a><span class="sd">            np.ndarray: Squeezed gradient to remove added dimension.</span>
</span><span id="__span-54-24"><a id="__codelineno-54-24" name="__codelineno-54-24" href="#__codelineno-54-24"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-54-25"><a id="__codelineno-54-25" name="__codelineno-54-25" href="#__codelineno-54-25"></a>
</span><span id="__span-54-26"><a id="__codelineno-54-26" name="__codelineno-54-26" href="#__codelineno-54-26"></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>  <span class="c1"># Correctly squeeze the axis</span>
</span><span id="__span-54-27"><a id="__codelineno-54-27" name="__codelineno-54-27" href="#__codelineno-54-27"></a>
</span><span id="__span-54-28"><a id="__codelineno-54-28" name="__codelineno-54-28" href="#__codelineno-54-28"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-54-29"><a id="__codelineno-54-29" name="__codelineno-54-29" href="#__codelineno-54-29"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-54-30"><a id="__codelineno-54-30" name="__codelineno-54-30" href="#__codelineno-54-30"></a>            <span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span><span class="p">)</span>
</span><span id="__span-54-31"><a id="__codelineno-54-31" name="__codelineno-54-31" href="#__codelineno-54-31"></a>        <span class="p">)</span>
</span><span id="__span-54-32"><a id="__codelineno-54-32" name="__codelineno-54-32" href="#__codelineno-54-32"></a>
</span><span id="__span-54-33"><a id="__codelineno-54-33" name="__codelineno-54-33" href="#__codelineno-54-33"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>These methods ensure proper gradient tracking while reshaping tensors, making them useful in various neural network operations.</p>
<h3 id="view"><code>view</code><a class="headerlink" href="#view" title="Permanent link">&para;</a></h3>
<p>The <code>view</code> method allows a <code>tensor</code> to be reshaped <strong>without copying the underlying data</strong>, provided the new shape is compatible with the original memory layout. This operation is efficient because it does not involve new memory allocation</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-55-1"><a id="__codelineno-55-1" name="__codelineno-55-1" href="#__codelineno-55-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">view</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-55-2"><a id="__codelineno-55-2" name="__codelineno-55-2" href="#__codelineno-55-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-55-3"><a id="__codelineno-55-3" name="__codelineno-55-3" href="#__codelineno-55-3"></a><span class="sd">    Reshape the tensor without changing its underlying data.</span>
</span><span id="__span-55-4"><a id="__codelineno-55-4" name="__codelineno-55-4" href="#__codelineno-55-4"></a>
</span><span id="__span-55-5"><a id="__codelineno-55-5" name="__codelineno-55-5" href="#__codelineno-55-5"></a><span class="sd">    This method returns a new `Tensor` object that shares the same data but is represented </span>
</span><span id="__span-55-6"><a id="__codelineno-55-6" name="__codelineno-55-6" href="#__codelineno-55-6"></a><span class="sd">    with a different shape. The operation is efficient as it does not involve memory reallocation.</span>
</span><span id="__span-55-7"><a id="__codelineno-55-7" name="__codelineno-55-7" href="#__codelineno-55-7"></a>
</span><span id="__span-55-8"><a id="__codelineno-55-8" name="__codelineno-55-8" href="#__codelineno-55-8"></a><span class="sd">    Args:</span>
</span><span id="__span-55-9"><a id="__codelineno-55-9" name="__codelineno-55-9" href="#__codelineno-55-9"></a><span class="sd">        shape (Tuple[int, ...]): The desired shape of the tensor.</span>
</span><span id="__span-55-10"><a id="__codelineno-55-10" name="__codelineno-55-10" href="#__codelineno-55-10"></a>
</span><span id="__span-55-11"><a id="__codelineno-55-11" name="__codelineno-55-11" href="#__codelineno-55-11"></a><span class="sd">    Returns:</span>
</span><span id="__span-55-12"><a id="__codelineno-55-12" name="__codelineno-55-12" href="#__codelineno-55-12"></a><span class="sd">        Tensor: A new tensor with the same data but a different shape.</span>
</span><span id="__span-55-13"><a id="__codelineno-55-13" name="__codelineno-55-13" href="#__codelineno-55-13"></a>
</span><span id="__span-55-14"><a id="__codelineno-55-14" name="__codelineno-55-14" href="#__codelineno-55-14"></a><span class="sd">    Example:</span>
</span><span id="__span-55-15"><a id="__codelineno-55-15" name="__codelineno-55-15" href="#__codelineno-55-15"></a><span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6]]), requires_grad=True)</span>
</span><span id="__span-55-16"><a id="__codelineno-55-16" name="__codelineno-55-16" href="#__codelineno-55-16"></a><span class="sd">        &gt;&gt;&gt; y = x.view((3, 2))</span>
</span><span id="__span-55-17"><a id="__codelineno-55-17" name="__codelineno-55-17" href="#__codelineno-55-17"></a><span class="sd">        &gt;&gt;&gt; print(y)</span>
</span><span id="__span-55-18"><a id="__codelineno-55-18" name="__codelineno-55-18" href="#__codelineno-55-18"></a><span class="sd">        Tensor([[1, 2],</span>
</span><span id="__span-55-19"><a id="__codelineno-55-19" name="__codelineno-55-19" href="#__codelineno-55-19"></a><span class="sd">                [3, 4],</span>
</span><span id="__span-55-20"><a id="__codelineno-55-20" name="__codelineno-55-20" href="#__codelineno-55-20"></a><span class="sd">                [5, 6]], requires_grad=True, shape=(3,2))</span>
</span><span id="__span-55-21"><a id="__codelineno-55-21" name="__codelineno-55-21" href="#__codelineno-55-21"></a>
</span><span id="__span-55-22"><a id="__codelineno-55-22" name="__codelineno-55-22" href="#__codelineno-55-22"></a><span class="sd">    Notes:</span>
</span><span id="__span-55-23"><a id="__codelineno-55-23" name="__codelineno-55-23" href="#__codelineno-55-23"></a><span class="sd">        - The returned tensor shares the same data as the original tensor.</span>
</span><span id="__span-55-24"><a id="__codelineno-55-24" name="__codelineno-55-24" href="#__codelineno-55-24"></a><span class="sd">        - If `requires_grad=True`, the backward function ensures gradients are reshaped correctly.</span>
</span><span id="__span-55-25"><a id="__codelineno-55-25" name="__codelineno-55-25" href="#__codelineno-55-25"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-55-26"><a id="__codelineno-55-26" name="__codelineno-55-26" href="#__codelineno-55-26"></a>
</span><span id="__span-55-27"><a id="__codelineno-55-27" name="__codelineno-55-27" href="#__codelineno-55-27"></a>    <span class="c1"># This will be a new view object if possible; otherwise, it will be a copy</span>
</span><span id="__span-55-28"><a id="__codelineno-55-28" name="__codelineno-55-28" href="#__codelineno-55-28"></a>    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</span><span id="__span-55-29"><a id="__codelineno-55-29" name="__codelineno-55-29" href="#__codelineno-55-29"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-55-30"><a id="__codelineno-55-30" name="__codelineno-55-30" href="#__codelineno-55-30"></a>
</span><span id="__span-55-31"><a id="__codelineno-55-31" name="__codelineno-55-31" href="#__codelineno-55-31"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-55-32"><a id="__codelineno-55-32" name="__codelineno-55-32" href="#__codelineno-55-32"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-55-33"><a id="__codelineno-55-33" name="__codelineno-55-33" href="#__codelineno-55-33"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-55-34"><a id="__codelineno-55-34" name="__codelineno-55-34" href="#__codelineno-55-34"></a><span class="sd">            Backward pass for tensor view operation.</span>
</span><span id="__span-55-35"><a id="__codelineno-55-35" name="__codelineno-55-35" href="#__codelineno-55-35"></a>
</span><span id="__span-55-36"><a id="__codelineno-55-36" name="__codelineno-55-36" href="#__codelineno-55-36"></a><span class="sd">            This function ensures that the incoming gradient is reshaped </span>
</span><span id="__span-55-37"><a id="__codelineno-55-37" name="__codelineno-55-37" href="#__codelineno-55-37"></a><span class="sd">            back to the original shape of the tensor.</span>
</span><span id="__span-55-38"><a id="__codelineno-55-38" name="__codelineno-55-38" href="#__codelineno-55-38"></a>
</span><span id="__span-55-39"><a id="__codelineno-55-39" name="__codelineno-55-39" href="#__codelineno-55-39"></a><span class="sd">            Args:</span>
</span><span id="__span-55-40"><a id="__codelineno-55-40" name="__codelineno-55-40" href="#__codelineno-55-40"></a><span class="sd">                grad (np.ndarray): The incoming gradient.</span>
</span><span id="__span-55-41"><a id="__codelineno-55-41" name="__codelineno-55-41" href="#__codelineno-55-41"></a>
</span><span id="__span-55-42"><a id="__codelineno-55-42" name="__codelineno-55-42" href="#__codelineno-55-42"></a><span class="sd">            Returns:</span>
</span><span id="__span-55-43"><a id="__codelineno-55-43" name="__codelineno-55-43" href="#__codelineno-55-43"></a><span class="sd">                np.ndarray: The gradient reshaped to match the original tensor.</span>
</span><span id="__span-55-44"><a id="__codelineno-55-44" name="__codelineno-55-44" href="#__codelineno-55-44"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-55-45"><a id="__codelineno-55-45" name="__codelineno-55-45" href="#__codelineno-55-45"></a>
</span><span id="__span-55-46"><a id="__codelineno-55-46" name="__codelineno-55-46" href="#__codelineno-55-46"></a>            <span class="k">return</span> <span class="n">grad</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="__span-55-47"><a id="__codelineno-55-47" name="__codelineno-55-47" href="#__codelineno-55-47"></a>
</span><span id="__span-55-48"><a id="__codelineno-55-48" name="__codelineno-55-48" href="#__codelineno-55-48"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span><span class="p">))</span>
</span><span id="__span-55-49"><a id="__codelineno-55-49" name="__codelineno-55-49" href="#__codelineno-55-49"></a>
</span><span id="__span-55-50"><a id="__codelineno-55-50" name="__codelineno-55-50" href="#__codelineno-55-50"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Example Usage:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-56-1"><a id="__codelineno-56-1" name="__codelineno-56-1" href="#__codelineno-56-1"></a><span class="c1"># Create a 2x3 tensor</span>
</span><span id="__span-56-2"><a id="__codelineno-56-2" name="__codelineno-56-2" href="#__codelineno-56-2"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-56-3"><a id="__codelineno-56-3" name="__codelineno-56-3" href="#__codelineno-56-3"></a>
</span><span id="__span-56-4"><a id="__codelineno-56-4" name="__codelineno-56-4" href="#__codelineno-56-4"></a><span class="c1"># Reshape into a 3x2 tensor</span>
</span><span id="__span-56-5"><a id="__codelineno-56-5" name="__codelineno-56-5" href="#__codelineno-56-5"></a><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="__span-56-6"><a id="__codelineno-56-6" name="__codelineno-56-6" href="#__codelineno-56-6"></a>
</span><span id="__span-56-7"><a id="__codelineno-56-7" name="__codelineno-56-7" href="#__codelineno-56-7"></a><span class="c1"># Print reshaped tensor</span>
</span><span id="__span-56-8"><a id="__codelineno-56-8" name="__codelineno-56-8" href="#__codelineno-56-8"></a><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span><span id="__span-56-9"><a id="__codelineno-56-9" name="__codelineno-56-9" href="#__codelineno-56-9"></a><span class="c1"># Output:</span>
</span><span id="__span-56-10"><a id="__codelineno-56-10" name="__codelineno-56-10" href="#__codelineno-56-10"></a><span class="c1"># Tensor([[1, 2],</span>
</span><span id="__span-56-11"><a id="__codelineno-56-11" name="__codelineno-56-11" href="#__codelineno-56-11"></a><span class="c1">#         [3, 4],</span>
</span><span id="__span-56-12"><a id="__codelineno-56-12" name="__codelineno-56-12" href="#__codelineno-56-12"></a><span class="c1">#         [5, 6]], requires_grad=True, shape=(3,2))</span>
</span><span id="__span-56-13"><a id="__codelineno-56-13" name="__codelineno-56-13" href="#__codelineno-56-13"></a>
</span><span id="__span-56-14"><a id="__codelineno-56-14" name="__codelineno-56-14" href="#__codelineno-56-14"></a><span class="c1"># Backward pass</span>
</span><span id="__span-56-15"><a id="__codelineno-56-15" name="__codelineno-56-15" href="#__codelineno-56-15"></a><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>  <span class="c1"># Gradient of ones</span>
</span><span id="__span-56-16"><a id="__codelineno-56-16" name="__codelineno-56-16" href="#__codelineno-56-16"></a>
</span><span id="__span-56-17"><a id="__codelineno-56-17" name="__codelineno-56-17" href="#__codelineno-56-17"></a><span class="c1"># Check gradients</span>
</span><span id="__span-56-18"><a id="__codelineno-56-18" name="__codelineno-56-18" href="#__codelineno-56-18"></a><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span><span id="__span-56-19"><a id="__codelineno-56-19" name="__codelineno-56-19" href="#__codelineno-56-19"></a><span class="c1"># Output should be reshaped correctly to match original shape:</span>
</span><span id="__span-56-20"><a id="__codelineno-56-20" name="__codelineno-56-20" href="#__codelineno-56-20"></a><span class="c1"># [[1. 1. 1.]</span>
</span><span id="__span-56-21"><a id="__codelineno-56-21" name="__codelineno-56-21" href="#__codelineno-56-21"></a><span class="c1">#  [1. 1. 1.]]</span>
</span></code></pre></div>
<p><strong>Example with backward:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-57-1"><a id="__codelineno-57-1" name="__codelineno-57-1" href="#__codelineno-57-1"></a><span class="n">a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
</span><span id="__span-57-2"><a id="__codelineno-57-2" name="__codelineno-57-2" href="#__codelineno-57-2"></a>            <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-57-3"><a id="__codelineno-57-3" name="__codelineno-57-3" href="#__codelineno-57-3"></a>
</span><span id="__span-57-4"><a id="__codelineno-57-4" name="__codelineno-57-4" href="#__codelineno-57-4"></a><span class="c1"># a.view((4, 1)).unsqueeze(0).squeeze(0).backward(np.ones_like(a.data))</span>
</span><span id="__span-57-5"><a id="__codelineno-57-5" name="__codelineno-57-5" href="#__codelineno-57-5"></a><span class="c1"># a.view((4, 1)).unsqueeze(1).squeeze(1).unsqueeze(0).squeeze(0).backward(np.ones_like(a.data))</span>
</span><span id="__span-57-6"><a id="__codelineno-57-6" name="__codelineno-57-6" href="#__codelineno-57-6"></a>
</span><span id="__span-57-7"><a id="__codelineno-57-7" name="__codelineno-57-7" href="#__codelineno-57-7"></a><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-57-8"><a id="__codelineno-57-8" name="__codelineno-57-8" href="#__codelineno-57-8"></a><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</span></code></pre></div>
<h3 id="clip"><code>clip</code><a class="headerlink" href="#clip" title="Permanent link">&para;</a></h3>
<p>The <code>clip()</code> method allows you to restrict the values of a tensor to a specific range. This is particularly useful in deep learning, where you may want to avoid exploding or vanishing gradients by limiting the range of tensor values during backpropagation. The <code>clip()</code> method ensures that all values of the tensor remain within a given minimum and maximum value.</p>
<p>This method clips (limits) the values of a tensor to a specified range, <code>[min_value, max_value]</code>. Any values below <code>min_value</code> are set to <code>min_value</code>, and any values above <code>max_value</code> are set to <code>max_value</code>. If either bound is not specified (i.e., None), no limit is applied on that side.</p>
<p>Gradient Behavior: when <code>requires_grad=True</code>, this method tracks the gradient of the clipping operation. For values that fall within the clipping range, the gradient is passed through unchanged. For values outside the clipping range, the gradient is set to zero because the clipping operation is not differentiable at the boundaries.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-58-1"><a id="__codelineno-58-1" name="__codelineno-58-1" href="#__codelineno-58-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">clip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-58-2"><a id="__codelineno-58-2" name="__codelineno-58-2" href="#__codelineno-58-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-58-3"><a id="__codelineno-58-3" name="__codelineno-58-3" href="#__codelineno-58-3"></a><span class="sd">    Clip the tensor&#39;s values to the range [min_value, max_value].</span>
</span><span id="__span-58-4"><a id="__codelineno-58-4" name="__codelineno-58-4" href="#__codelineno-58-4"></a>
</span><span id="__span-58-5"><a id="__codelineno-58-5" name="__codelineno-58-5" href="#__codelineno-58-5"></a><span class="sd">    Args:</span>
</span><span id="__span-58-6"><a id="__codelineno-58-6" name="__codelineno-58-6" href="#__codelineno-58-6"></a><span class="sd">        min_value (Optional[float]): The minimum value to clip to. If None, no lower bound is applied.</span>
</span><span id="__span-58-7"><a id="__codelineno-58-7" name="__codelineno-58-7" href="#__codelineno-58-7"></a><span class="sd">        max_value (Optional[float]): The maximum value to clip to. If None, no upper bound is applied.</span>
</span><span id="__span-58-8"><a id="__codelineno-58-8" name="__codelineno-58-8" href="#__codelineno-58-8"></a>
</span><span id="__span-58-9"><a id="__codelineno-58-9" name="__codelineno-58-9" href="#__codelineno-58-9"></a><span class="sd">    Returns:</span>
</span><span id="__span-58-10"><a id="__codelineno-58-10" name="__codelineno-58-10" href="#__codelineno-58-10"></a><span class="sd">        Tensor: A new tensor with values clipped to the specified range.</span>
</span><span id="__span-58-11"><a id="__codelineno-58-11" name="__codelineno-58-11" href="#__codelineno-58-11"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-58-12"><a id="__codelineno-58-12" name="__codelineno-58-12" href="#__codelineno-58-12"></a>
</span><span id="__span-58-13"><a id="__codelineno-58-13" name="__codelineno-58-13" href="#__codelineno-58-13"></a>    <span class="c1"># Perform clipping on the data</span>
</span><span id="__span-58-14"><a id="__codelineno-58-14" name="__codelineno-58-14" href="#__codelineno-58-14"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">min_value</span><span class="p">,</span> <span class="n">max_value</span><span class="p">)</span>
</span><span id="__span-58-15"><a id="__codelineno-58-15" name="__codelineno-58-15" href="#__codelineno-58-15"></a>
</span><span id="__span-58-16"><a id="__codelineno-58-16" name="__codelineno-58-16" href="#__codelineno-58-16"></a>    <span class="c1"># Track dependencies if requires_grad is True</span>
</span><span id="__span-58-17"><a id="__codelineno-58-17" name="__codelineno-58-17" href="#__codelineno-58-17"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-58-18"><a id="__codelineno-58-18" name="__codelineno-58-18" href="#__codelineno-58-18"></a>
</span><span id="__span-58-19"><a id="__codelineno-58-19" name="__codelineno-58-19" href="#__codelineno-58-19"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-58-20"><a id="__codelineno-58-20" name="__codelineno-58-20" href="#__codelineno-58-20"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-58-21"><a id="__codelineno-58-21" name="__codelineno-58-21" href="#__codelineno-58-21"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-58-22"><a id="__codelineno-58-22" name="__codelineno-58-22" href="#__codelineno-58-22"></a><span class="sd">            Backward function for the clip operation.</span>
</span><span id="__span-58-23"><a id="__codelineno-58-23" name="__codelineno-58-23" href="#__codelineno-58-23"></a>
</span><span id="__span-58-24"><a id="__codelineno-58-24" name="__codelineno-58-24" href="#__codelineno-58-24"></a><span class="sd">            The gradient is passed through for values within the range [min_value, max_value].</span>
</span><span id="__span-58-25"><a id="__codelineno-58-25" name="__codelineno-58-25" href="#__codelineno-58-25"></a><span class="sd">            For values outside this range, the gradient is zero because the operation is not differentiable</span>
</span><span id="__span-58-26"><a id="__codelineno-58-26" name="__codelineno-58-26" href="#__codelineno-58-26"></a><span class="sd">            at the boundaries.</span>
</span><span id="__span-58-27"><a id="__codelineno-58-27" name="__codelineno-58-27" href="#__codelineno-58-27"></a>
</span><span id="__span-58-28"><a id="__codelineno-58-28" name="__codelineno-58-28" href="#__codelineno-58-28"></a><span class="sd">            Args:</span>
</span><span id="__span-58-29"><a id="__codelineno-58-29" name="__codelineno-58-29" href="#__codelineno-58-29"></a><span class="sd">                grad (np.ndarray): The gradient passed from the downstream operation.</span>
</span><span id="__span-58-30"><a id="__codelineno-58-30" name="__codelineno-58-30" href="#__codelineno-58-30"></a>
</span><span id="__span-58-31"><a id="__codelineno-58-31" name="__codelineno-58-31" href="#__codelineno-58-31"></a><span class="sd">            Returns:</span>
</span><span id="__span-58-32"><a id="__codelineno-58-32" name="__codelineno-58-32" href="#__codelineno-58-32"></a><span class="sd">                np.ndarray: The gradient for the input tensor.</span>
</span><span id="__span-58-33"><a id="__codelineno-58-33" name="__codelineno-58-33" href="#__codelineno-58-33"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-58-34"><a id="__codelineno-58-34" name="__codelineno-58-34" href="#__codelineno-58-34"></a>
</span><span id="__span-58-35"><a id="__codelineno-58-35" name="__codelineno-58-35" href="#__codelineno-58-35"></a>            <span class="c1"># Create a mask for values within the clipping range</span>
</span><span id="__span-58-36"><a id="__codelineno-58-36" name="__codelineno-58-36" href="#__codelineno-58-36"></a>            <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-58-37"><a id="__codelineno-58-37" name="__codelineno-58-37" href="#__codelineno-58-37"></a>
</span><span id="__span-58-38"><a id="__codelineno-58-38" name="__codelineno-58-38" href="#__codelineno-58-38"></a>            <span class="c1"># Apply the mask</span>
</span><span id="__span-58-39"><a id="__codelineno-58-39" name="__codelineno-58-39" href="#__codelineno-58-39"></a>            <span class="k">if</span> <span class="n">min_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-58-40"><a id="__codelineno-58-40" name="__codelineno-58-40" href="#__codelineno-58-40"></a>                <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">&lt;=</span> <span class="n">min_value</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-58-41"><a id="__codelineno-58-41" name="__codelineno-58-41" href="#__codelineno-58-41"></a>            <span class="k">if</span> <span class="n">max_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-58-42"><a id="__codelineno-58-42" name="__codelineno-58-42" href="#__codelineno-58-42"></a>                <span class="n">mask</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;=</span> <span class="n">max_value</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="__span-58-43"><a id="__codelineno-58-43" name="__codelineno-58-43" href="#__codelineno-58-43"></a>
</span><span id="__span-58-44"><a id="__codelineno-58-44" name="__codelineno-58-44" href="#__codelineno-58-44"></a>            <span class="c1"># Multiply the gradient by the mask</span>
</span><span id="__span-58-45"><a id="__codelineno-58-45" name="__codelineno-58-45" href="#__codelineno-58-45"></a>            <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">mask</span>
</span><span id="__span-58-46"><a id="__codelineno-58-46" name="__codelineno-58-46" href="#__codelineno-58-46"></a>
</span><span id="__span-58-47"><a id="__codelineno-58-47" name="__codelineno-58-47" href="#__codelineno-58-47"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span><span class="p">))</span>
</span><span id="__span-58-48"><a id="__codelineno-58-48" name="__codelineno-58-48" href="#__codelineno-58-48"></a>
</span><span id="__span-58-49"><a id="__codelineno-58-49" name="__codelineno-58-49" href="#__codelineno-58-49"></a>    <span class="c1"># Return a new tensor with the clipped values and dependencies</span>
</span><span id="__span-58-50"><a id="__codelineno-58-50" name="__codelineno-58-50" href="#__codelineno-58-50"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Example:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-59-1"><a id="__codelineno-59-1" name="__codelineno-59-1" href="#__codelineno-59-1"></a><span class="c1"># Example 1: Clipping tensor values to a range [0, 1]</span>
</span><span id="__span-59-2"><a id="__codelineno-59-2" name="__codelineno-59-2" href="#__codelineno-59-2"></a><span class="n">a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-59-3"><a id="__codelineno-59-3" name="__codelineno-59-3" href="#__codelineno-59-3"></a><span class="n">clipped_a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</span><span id="__span-59-4"><a id="__codelineno-59-4" name="__codelineno-59-4" href="#__codelineno-59-4"></a>
</span><span id="__span-59-5"><a id="__codelineno-59-5" name="__codelineno-59-5" href="#__codelineno-59-5"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Tensor:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-59-6"><a id="__codelineno-59-6" name="__codelineno-59-6" href="#__codelineno-59-6"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Clipped Tensor:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">clipped_a</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-59-7"><a id="__codelineno-59-7" name="__codelineno-59-7" href="#__codelineno-59-7"></a>
</span><span id="__span-59-8"><a id="__codelineno-59-8" name="__codelineno-59-8" href="#__codelineno-59-8"></a><span class="c1"># Perform backward pass to test gradients</span>
</span><span id="__span-59-9"><a id="__codelineno-59-9" name="__codelineno-59-9" href="#__codelineno-59-9"></a><span class="n">grad_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-59-10"><a id="__codelineno-59-10" name="__codelineno-59-10" href="#__codelineno-59-10"></a><span class="n">clipped_a</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="__span-59-11"><a id="__codelineno-59-11" name="__codelineno-59-11" href="#__codelineno-59-11"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Gradients:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Output:</strong></p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-60-1"><a id="__codelineno-60-1" name="__codelineno-60-1" href="#__codelineno-60-1"></a>Original Tensor:
</span><span id="__span-60-2"><a id="__codelineno-60-2" name="__codelineno-60-2" href="#__codelineno-60-2"></a> [[ 0.5 -0.3]
</span><span id="__span-60-3"><a id="__codelineno-60-3" name="__codelineno-60-3" href="#__codelineno-60-3"></a> [ 1.2  2. ]]
</span><span id="__span-60-4"><a id="__codelineno-60-4" name="__codelineno-60-4" href="#__codelineno-60-4"></a>Clipped Tensor:
</span><span id="__span-60-5"><a id="__codelineno-60-5" name="__codelineno-60-5" href="#__codelineno-60-5"></a> [[0.5 0. ]
</span><span id="__span-60-6"><a id="__codelineno-60-6" name="__codelineno-60-6" href="#__codelineno-60-6"></a> [1.  1. ]]
</span><span id="__span-60-7"><a id="__codelineno-60-7" name="__codelineno-60-7" href="#__codelineno-60-7"></a>Gradients:
</span><span id="__span-60-8"><a id="__codelineno-60-8" name="__codelineno-60-8" href="#__codelineno-60-8"></a> [[1. 0.]
</span><span id="__span-60-9"><a id="__codelineno-60-9" name="__codelineno-60-9" href="#__codelineno-60-9"></a> [1. 0.]]
</span></code></pre></div>
<h2 id="where"><code>where</code><a class="headerlink" href="#where" title="Permanent link">&para;</a></h2>
<iframe width="1707" height="765" src="https://www.youtube.com/embed/4_kJy6hmv2E?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="Mastering Tensor Operations: Comparison, Where, and More in MicroTorch" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p>The <code>where</code> method is a powerful element-wise selection operation for tensors, similar to NumPy's <code>np.where</code>. It allows conditional selection of elements from two tensors (<code>a</code> and <code>b</code>) based on a boolean condition. This is particularly useful in deep learning for masking values, implementing conditional operations, and handling gradients properly in automatic differentiation.</p>
<p>This function ensures that during backpropagation, gradients are propagated only to the selected elements of <code>a</code> and <code>b</code>, preventing unnecessary computations.</p>
<p>The comparison operators allow us to create boolean tensors by comparing tensor values element-wise. These operators are essential for conditional operations like <code>where</code>.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-61-1"><a id="__codelineno-61-1" name="__codelineno-61-1" href="#__codelineno-61-1"></a><span class="c1"># Comparison Operators</span>
</span><span id="__span-61-2"><a id="__codelineno-61-2" name="__codelineno-61-2" href="#__codelineno-61-2"></a><span class="k">def</span><span class="w"> </span><span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-61-3"><a id="__codelineno-61-3" name="__codelineno-61-3" href="#__codelineno-61-3"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-61-4"><a id="__codelineno-61-4" name="__codelineno-61-4" href="#__codelineno-61-4"></a><span class="sd">    Less than operator (&lt;).</span>
</span><span id="__span-61-5"><a id="__codelineno-61-5" name="__codelineno-61-5" href="#__codelineno-61-5"></a>
</span><span id="__span-61-6"><a id="__codelineno-61-6" name="__codelineno-61-6" href="#__codelineno-61-6"></a><span class="sd">    Creates a boolean tensor where each element is True if the corresponding</span>
</span><span id="__span-61-7"><a id="__codelineno-61-7" name="__codelineno-61-7" href="#__codelineno-61-7"></a><span class="sd">    element in self is less than the corresponding element in other.</span>
</span><span id="__span-61-8"><a id="__codelineno-61-8" name="__codelineno-61-8" href="#__codelineno-61-8"></a>
</span><span id="__span-61-9"><a id="__codelineno-61-9" name="__codelineno-61-9" href="#__codelineno-61-9"></a><span class="sd">    Args:</span>
</span><span id="__span-61-10"><a id="__codelineno-61-10" name="__codelineno-61-10" href="#__codelineno-61-10"></a><span class="sd">        other (Data): Value to compare against. Can be a Tensor or a compatible data type.</span>
</span><span id="__span-61-11"><a id="__codelineno-61-11" name="__codelineno-61-11" href="#__codelineno-61-11"></a>
</span><span id="__span-61-12"><a id="__codelineno-61-12" name="__codelineno-61-12" href="#__codelineno-61-12"></a><span class="sd">    Returns:</span>
</span><span id="__span-61-13"><a id="__codelineno-61-13" name="__codelineno-61-13" href="#__codelineno-61-13"></a><span class="sd">        Tensor: A boolean tensor with the comparison results.</span>
</span><span id="__span-61-14"><a id="__codelineno-61-14" name="__codelineno-61-14" href="#__codelineno-61-14"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-61-15"><a id="__codelineno-61-15" name="__codelineno-61-15" href="#__codelineno-61-15"></a>
</span><span id="__span-61-16"><a id="__codelineno-61-16" name="__codelineno-61-16" href="#__codelineno-61-16"></a>    <span class="n">other</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-61-17"><a id="__codelineno-61-17" name="__codelineno-61-17" href="#__codelineno-61-17"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">&lt;</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-61-18"><a id="__codelineno-61-18" name="__codelineno-61-18" href="#__codelineno-61-18"></a>
</span><span id="__span-61-19"><a id="__codelineno-61-19" name="__codelineno-61-19" href="#__codelineno-61-19"></a><span class="k">def</span><span class="w"> </span><span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-61-20"><a id="__codelineno-61-20" name="__codelineno-61-20" href="#__codelineno-61-20"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-61-21"><a id="__codelineno-61-21" name="__codelineno-61-21" href="#__codelineno-61-21"></a><span class="sd">    Greater than operator (&gt;).</span>
</span><span id="__span-61-22"><a id="__codelineno-61-22" name="__codelineno-61-22" href="#__codelineno-61-22"></a>
</span><span id="__span-61-23"><a id="__codelineno-61-23" name="__codelineno-61-23" href="#__codelineno-61-23"></a><span class="sd">    Creates a boolean tensor where each element is True if the corresponding</span>
</span><span id="__span-61-24"><a id="__codelineno-61-24" name="__codelineno-61-24" href="#__codelineno-61-24"></a><span class="sd">    element in self is greater than the corresponding element in other.</span>
</span><span id="__span-61-25"><a id="__codelineno-61-25" name="__codelineno-61-25" href="#__codelineno-61-25"></a>
</span><span id="__span-61-26"><a id="__codelineno-61-26" name="__codelineno-61-26" href="#__codelineno-61-26"></a><span class="sd">    Args:</span>
</span><span id="__span-61-27"><a id="__codelineno-61-27" name="__codelineno-61-27" href="#__codelineno-61-27"></a><span class="sd">        other (Data): Value to compare against. Can be a Tensor or a compatible data type.</span>
</span><span id="__span-61-28"><a id="__codelineno-61-28" name="__codelineno-61-28" href="#__codelineno-61-28"></a>
</span><span id="__span-61-29"><a id="__codelineno-61-29" name="__codelineno-61-29" href="#__codelineno-61-29"></a><span class="sd">    Returns:</span>
</span><span id="__span-61-30"><a id="__codelineno-61-30" name="__codelineno-61-30" href="#__codelineno-61-30"></a><span class="sd">        Tensor: A boolean tensor with the comparison results.</span>
</span><span id="__span-61-31"><a id="__codelineno-61-31" name="__codelineno-61-31" href="#__codelineno-61-31"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-61-32"><a id="__codelineno-61-32" name="__codelineno-61-32" href="#__codelineno-61-32"></a>
</span><span id="__span-61-33"><a id="__codelineno-61-33" name="__codelineno-61-33" href="#__codelineno-61-33"></a>    <span class="n">other</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-61-34"><a id="__codelineno-61-34" name="__codelineno-61-34" href="#__codelineno-61-34"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-61-35"><a id="__codelineno-61-35" name="__codelineno-61-35" href="#__codelineno-61-35"></a>
</span><span id="__span-61-36"><a id="__codelineno-61-36" name="__codelineno-61-36" href="#__codelineno-61-36"></a><span class="k">def</span><span class="w"> </span><span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-61-37"><a id="__codelineno-61-37" name="__codelineno-61-37" href="#__codelineno-61-37"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-61-38"><a id="__codelineno-61-38" name="__codelineno-61-38" href="#__codelineno-61-38"></a><span class="sd">    Equal to operator (==).</span>
</span><span id="__span-61-39"><a id="__codelineno-61-39" name="__codelineno-61-39" href="#__codelineno-61-39"></a>
</span><span id="__span-61-40"><a id="__codelineno-61-40" name="__codelineno-61-40" href="#__codelineno-61-40"></a><span class="sd">    Creates a boolean tensor where each element is True if the corresponding</span>
</span><span id="__span-61-41"><a id="__codelineno-61-41" name="__codelineno-61-41" href="#__codelineno-61-41"></a><span class="sd">    element in self is equal to the corresponding element in other.</span>
</span><span id="__span-61-42"><a id="__codelineno-61-42" name="__codelineno-61-42" href="#__codelineno-61-42"></a>
</span><span id="__span-61-43"><a id="__codelineno-61-43" name="__codelineno-61-43" href="#__codelineno-61-43"></a><span class="sd">    Args:</span>
</span><span id="__span-61-44"><a id="__codelineno-61-44" name="__codelineno-61-44" href="#__codelineno-61-44"></a><span class="sd">        other (Data): Value to compare against. Can be a Tensor or a compatible data type.</span>
</span><span id="__span-61-45"><a id="__codelineno-61-45" name="__codelineno-61-45" href="#__codelineno-61-45"></a>
</span><span id="__span-61-46"><a id="__codelineno-61-46" name="__codelineno-61-46" href="#__codelineno-61-46"></a><span class="sd">    Returns:</span>
</span><span id="__span-61-47"><a id="__codelineno-61-47" name="__codelineno-61-47" href="#__codelineno-61-47"></a><span class="sd">        Tensor: A boolean tensor with the comparison results.</span>
</span><span id="__span-61-48"><a id="__codelineno-61-48" name="__codelineno-61-48" href="#__codelineno-61-48"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-61-49"><a id="__codelineno-61-49" name="__codelineno-61-49" href="#__codelineno-61-49"></a>
</span><span id="__span-61-50"><a id="__codelineno-61-50" name="__codelineno-61-50" href="#__codelineno-61-50"></a>    <span class="n">other</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-61-51"><a id="__codelineno-61-51" name="__codelineno-61-51" href="#__codelineno-61-51"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-61-52"><a id="__codelineno-61-52" name="__codelineno-61-52" href="#__codelineno-61-52"></a>
</span><span id="__span-61-53"><a id="__codelineno-61-53" name="__codelineno-61-53" href="#__codelineno-61-53"></a><span class="k">def</span><span class="w"> </span><span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-61-54"><a id="__codelineno-61-54" name="__codelineno-61-54" href="#__codelineno-61-54"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-61-55"><a id="__codelineno-61-55" name="__codelineno-61-55" href="#__codelineno-61-55"></a><span class="sd">    Less than or equal to operator (&lt;=).</span>
</span><span id="__span-61-56"><a id="__codelineno-61-56" name="__codelineno-61-56" href="#__codelineno-61-56"></a>
</span><span id="__span-61-57"><a id="__codelineno-61-57" name="__codelineno-61-57" href="#__codelineno-61-57"></a><span class="sd">    Creates a boolean tensor where each element is True if the corresponding</span>
</span><span id="__span-61-58"><a id="__codelineno-61-58" name="__codelineno-61-58" href="#__codelineno-61-58"></a><span class="sd">    element in self is less than or equal to the corresponding element in other.</span>
</span><span id="__span-61-59"><a id="__codelineno-61-59" name="__codelineno-61-59" href="#__codelineno-61-59"></a>
</span><span id="__span-61-60"><a id="__codelineno-61-60" name="__codelineno-61-60" href="#__codelineno-61-60"></a><span class="sd">    Args:</span>
</span><span id="__span-61-61"><a id="__codelineno-61-61" name="__codelineno-61-61" href="#__codelineno-61-61"></a><span class="sd">        other (Data): Value to compare against. Can be a Tensor or a compatible data type.</span>
</span><span id="__span-61-62"><a id="__codelineno-61-62" name="__codelineno-61-62" href="#__codelineno-61-62"></a>
</span><span id="__span-61-63"><a id="__codelineno-61-63" name="__codelineno-61-63" href="#__codelineno-61-63"></a><span class="sd">    Returns:</span>
</span><span id="__span-61-64"><a id="__codelineno-61-64" name="__codelineno-61-64" href="#__codelineno-61-64"></a><span class="sd">        Tensor: A boolean tensor with the comparison results.</span>
</span><span id="__span-61-65"><a id="__codelineno-61-65" name="__codelineno-61-65" href="#__codelineno-61-65"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-61-66"><a id="__codelineno-61-66" name="__codelineno-61-66" href="#__codelineno-61-66"></a>
</span><span id="__span-61-67"><a id="__codelineno-61-67" name="__codelineno-61-67" href="#__codelineno-61-67"></a>    <span class="n">other</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-61-68"><a id="__codelineno-61-68" name="__codelineno-61-68" href="#__codelineno-61-68"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">&lt;=</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-61-69"><a id="__codelineno-61-69" name="__codelineno-61-69" href="#__codelineno-61-69"></a>
</span><span id="__span-61-70"><a id="__codelineno-61-70" name="__codelineno-61-70" href="#__codelineno-61-70"></a><span class="k">def</span><span class="w"> </span><span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-61-71"><a id="__codelineno-61-71" name="__codelineno-61-71" href="#__codelineno-61-71"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-61-72"><a id="__codelineno-61-72" name="__codelineno-61-72" href="#__codelineno-61-72"></a><span class="sd">    Greater than or equal to operator (&gt;=).</span>
</span><span id="__span-61-73"><a id="__codelineno-61-73" name="__codelineno-61-73" href="#__codelineno-61-73"></a>
</span><span id="__span-61-74"><a id="__codelineno-61-74" name="__codelineno-61-74" href="#__codelineno-61-74"></a><span class="sd">    Creates a boolean tensor where each element is True if the corresponding</span>
</span><span id="__span-61-75"><a id="__codelineno-61-75" name="__codelineno-61-75" href="#__codelineno-61-75"></a><span class="sd">    element in self is greater than or equal to the corresponding element in other.</span>
</span><span id="__span-61-76"><a id="__codelineno-61-76" name="__codelineno-61-76" href="#__codelineno-61-76"></a>
</span><span id="__span-61-77"><a id="__codelineno-61-77" name="__codelineno-61-77" href="#__codelineno-61-77"></a><span class="sd">    Args:</span>
</span><span id="__span-61-78"><a id="__codelineno-61-78" name="__codelineno-61-78" href="#__codelineno-61-78"></a><span class="sd">        other (Data): Value to compare against. Can be a Tensor or a compatible data type.</span>
</span><span id="__span-61-79"><a id="__codelineno-61-79" name="__codelineno-61-79" href="#__codelineno-61-79"></a>
</span><span id="__span-61-80"><a id="__codelineno-61-80" name="__codelineno-61-80" href="#__codelineno-61-80"></a><span class="sd">    Returns:</span>
</span><span id="__span-61-81"><a id="__codelineno-61-81" name="__codelineno-61-81" href="#__codelineno-61-81"></a><span class="sd">        Tensor: A boolean tensor with the comparison results.</span>
</span><span id="__span-61-82"><a id="__codelineno-61-82" name="__codelineno-61-82" href="#__codelineno-61-82"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-61-83"><a id="__codelineno-61-83" name="__codelineno-61-83" href="#__codelineno-61-83"></a>
</span><span id="__span-61-84"><a id="__codelineno-61-84" name="__codelineno-61-84" href="#__codelineno-61-84"></a>    <span class="n">other</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-61-85"><a id="__codelineno-61-85" name="__codelineno-61-85" href="#__codelineno-61-85"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;=</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-61-86"><a id="__codelineno-61-86" name="__codelineno-61-86" href="#__codelineno-61-86"></a>
</span><span id="__span-61-87"><a id="__codelineno-61-87" name="__codelineno-61-87" href="#__codelineno-61-87"></a><span class="k">def</span><span class="w"> </span><span class="fm">__ne__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-61-88"><a id="__codelineno-61-88" name="__codelineno-61-88" href="#__codelineno-61-88"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-61-89"><a id="__codelineno-61-89" name="__codelineno-61-89" href="#__codelineno-61-89"></a><span class="sd">    Not equal to operator (!=).</span>
</span><span id="__span-61-90"><a id="__codelineno-61-90" name="__codelineno-61-90" href="#__codelineno-61-90"></a>
</span><span id="__span-61-91"><a id="__codelineno-61-91" name="__codelineno-61-91" href="#__codelineno-61-91"></a><span class="sd">    Creates a boolean tensor where each element is True if the corresponding</span>
</span><span id="__span-61-92"><a id="__codelineno-61-92" name="__codelineno-61-92" href="#__codelineno-61-92"></a><span class="sd">    element in self is not equal to the corresponding element in other.</span>
</span><span id="__span-61-93"><a id="__codelineno-61-93" name="__codelineno-61-93" href="#__codelineno-61-93"></a>
</span><span id="__span-61-94"><a id="__codelineno-61-94" name="__codelineno-61-94" href="#__codelineno-61-94"></a><span class="sd">    Args:</span>
</span><span id="__span-61-95"><a id="__codelineno-61-95" name="__codelineno-61-95" href="#__codelineno-61-95"></a><span class="sd">        other (Data): Value to compare against. Can be a Tensor or a compatible data type.</span>
</span><span id="__span-61-96"><a id="__codelineno-61-96" name="__codelineno-61-96" href="#__codelineno-61-96"></a>
</span><span id="__span-61-97"><a id="__codelineno-61-97" name="__codelineno-61-97" href="#__codelineno-61-97"></a><span class="sd">    Returns:</span>
</span><span id="__span-61-98"><a id="__codelineno-61-98" name="__codelineno-61-98" href="#__codelineno-61-98"></a><span class="sd">        Tensor: A boolean tensor with the comparison results.</span>
</span><span id="__span-61-99"><a id="__codelineno-61-99" name="__codelineno-61-99" href="#__codelineno-61-99"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-61-100"><a id="__codelineno-61-100" name="__codelineno-61-100" href="#__codelineno-61-100"></a>
</span><span id="__span-61-101"><a id="__codelineno-61-101" name="__codelineno-61-101" href="#__codelineno-61-101"></a>    <span class="n">other</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
</span><span id="__span-61-102"><a id="__codelineno-61-102" name="__codelineno-61-102" href="#__codelineno-61-102"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">!=</span> <span class="n">other</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Example:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-62-1"><a id="__codelineno-62-1" name="__codelineno-62-1" href="#__codelineno-62-1"></a><span class="c1"># Create two tensors</span>
</span><span id="__span-62-2"><a id="__codelineno-62-2" name="__codelineno-62-2" href="#__codelineno-62-2"></a><span class="n">a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
</span><span id="__span-62-3"><a id="__codelineno-62-3" name="__codelineno-62-3" href="#__codelineno-62-3"></a><span class="n">b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
</span><span id="__span-62-4"><a id="__codelineno-62-4" name="__codelineno-62-4" href="#__codelineno-62-4"></a>
</span><span id="__span-62-5"><a id="__codelineno-62-5" name="__codelineno-62-5" href="#__codelineno-62-5"></a><span class="c1"># Compare the tensors</span>
</span><span id="__span-62-6"><a id="__codelineno-62-6" name="__codelineno-62-6" href="#__codelineno-62-6"></a><span class="n">result_lt</span> <span class="o">=</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">b</span>
</span><span id="__span-62-7"><a id="__codelineno-62-7" name="__codelineno-62-7" href="#__codelineno-62-7"></a><span class="nb">print</span><span class="p">(</span><span class="n">result_lt</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-62-8"><a id="__codelineno-62-8" name="__codelineno-62-8" href="#__codelineno-62-8"></a><span class="c1"># Output: [True False False True]</span>
</span><span id="__span-62-9"><a id="__codelineno-62-9" name="__codelineno-62-9" href="#__codelineno-62-9"></a>
</span><span id="__span-62-10"><a id="__codelineno-62-10" name="__codelineno-62-10" href="#__codelineno-62-10"></a><span class="n">result_eq</span> <span class="o">=</span> <span class="n">a</span> <span class="o">==</span> <span class="n">b</span>
</span><span id="__span-62-11"><a id="__codelineno-62-11" name="__codelineno-62-11" href="#__codelineno-62-11"></a><span class="nb">print</span><span class="p">(</span><span class="n">result_eq</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-62-12"><a id="__codelineno-62-12" name="__codelineno-62-12" href="#__codelineno-62-12"></a><span class="c1"># Output: [False True False False]</span>
</span><span id="__span-62-13"><a id="__codelineno-62-13" name="__codelineno-62-13" href="#__codelineno-62-13"></a>
</span><span id="__span-62-14"><a id="__codelineno-62-14" name="__codelineno-62-14" href="#__codelineno-62-14"></a><span class="c1"># Use with where operation</span>
</span><span id="__span-62-15"><a id="__codelineno-62-15" name="__codelineno-62-15" href="#__codelineno-62-15"></a><span class="n">c</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span><span id="__span-62-16"><a id="__codelineno-62-16" name="__codelineno-62-16" href="#__codelineno-62-16"></a><span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-62-17"><a id="__codelineno-62-17" name="__codelineno-62-17" href="#__codelineno-62-17"></a><span class="c1"># Output: [2 2 3 5]</span>
</span></code></pre></div>
<p>The <code>where</code> method executes the main logic of selecting between two tensors <code>a</code> and <code>b</code> based on a condition. <strong>Gradient for <code>a</code>:</strong> If a value from tensor <code>a</code> was chosen (when <code>condition == True</code>), the gradient should flow to <code>a</code>. Otherwise, the gradient for <code>a</code> is zero.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-63-1"><a id="__codelineno-63-1" name="__codelineno-63-1" href="#__codelineno-63-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">_bkwd_a</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-63-2"><a id="__codelineno-63-2" name="__codelineno-63-2" href="#__codelineno-63-2"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span></code></pre></div>
<p><strong><code>np.where(condition.data, grad, 0.0)</code></strong> means: pass the gradient to <code>a</code> where the condition is <code>True</code> and zero out the gradient where the condition is <code>False</code>.</p>
<p><strong>Gradient for <code>b</code>:</strong> If a value from tensor <code>b</code> was chosen (when <code>condition == False</code>), the gradient should flow to <code>b</code>. Otherwise, the gradient for <code>b</code> is zero.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-64-1"><a id="__codelineno-64-1" name="__codelineno-64-1" href="#__codelineno-64-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">_bkwd_b</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-64-2"><a id="__codelineno-64-2" name="__codelineno-64-2" href="#__codelineno-64-2"></a>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Implementation:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-65-1"><a id="__codelineno-65-1" name="__codelineno-65-1" href="#__codelineno-65-1"></a><span class="nd">@staticmethod</span>
</span><span id="__span-65-2"><a id="__codelineno-65-2" name="__codelineno-65-2" href="#__codelineno-65-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">where</span><span class="p">(</span><span class="n">condition</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-65-3"><a id="__codelineno-65-3" name="__codelineno-65-3" href="#__codelineno-65-3"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-65-4"><a id="__codelineno-65-4" name="__codelineno-65-4" href="#__codelineno-65-4"></a><span class="sd">    Performs element-wise selection based on a condition.</span>
</span><span id="__span-65-5"><a id="__codelineno-65-5" name="__codelineno-65-5" href="#__codelineno-65-5"></a>
</span><span id="__span-65-6"><a id="__codelineno-65-6" name="__codelineno-65-6" href="#__codelineno-65-6"></a><span class="sd">    This function returns a tensor where each element is taken from `a` if the corresponding </span>
</span><span id="__span-65-7"><a id="__codelineno-65-7" name="__codelineno-65-7" href="#__codelineno-65-7"></a><span class="sd">    element in `condition` is True, otherwise from `b`. It supports automatic differentiation.</span>
</span><span id="__span-65-8"><a id="__codelineno-65-8" name="__codelineno-65-8" href="#__codelineno-65-8"></a>
</span><span id="__span-65-9"><a id="__codelineno-65-9" name="__codelineno-65-9" href="#__codelineno-65-9"></a><span class="sd">    Args:</span>
</span><span id="__span-65-10"><a id="__codelineno-65-10" name="__codelineno-65-10" href="#__codelineno-65-10"></a><span class="sd">        condition (Tensor): A boolean tensor where True selects from `a`, and False selects from `b`.</span>
</span><span id="__span-65-11"><a id="__codelineno-65-11" name="__codelineno-65-11" href="#__codelineno-65-11"></a><span class="sd">        a (Tensor): The tensor providing values where `condition` is True.</span>
</span><span id="__span-65-12"><a id="__codelineno-65-12" name="__codelineno-65-12" href="#__codelineno-65-12"></a><span class="sd">        b (Tensor): The tensor providing values where `condition` is False.</span>
</span><span id="__span-65-13"><a id="__codelineno-65-13" name="__codelineno-65-13" href="#__codelineno-65-13"></a>
</span><span id="__span-65-14"><a id="__codelineno-65-14" name="__codelineno-65-14" href="#__codelineno-65-14"></a><span class="sd">    Returns:</span>
</span><span id="__span-65-15"><a id="__codelineno-65-15" name="__codelineno-65-15" href="#__codelineno-65-15"></a><span class="sd">        Tensor: A tensor with elements from `a` where `condition` is True, and from `b` otherwise.</span>
</span><span id="__span-65-16"><a id="__codelineno-65-16" name="__codelineno-65-16" href="#__codelineno-65-16"></a>
</span><span id="__span-65-17"><a id="__codelineno-65-17" name="__codelineno-65-17" href="#__codelineno-65-17"></a><span class="sd">    Example:</span>
</span><span id="__span-65-18"><a id="__codelineno-65-18" name="__codelineno-65-18" href="#__codelineno-65-18"></a><span class="sd">        &gt;&gt;&gt; cond = Tensor(np.array([[True, False], [False, True]]))</span>
</span><span id="__span-65-19"><a id="__codelineno-65-19" name="__codelineno-65-19" href="#__codelineno-65-19"></a><span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), requires_grad=True)</span>
</span><span id="__span-65-20"><a id="__codelineno-65-20" name="__codelineno-65-20" href="#__codelineno-65-20"></a><span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[10, 20], [30, 40]]), requires_grad=True)</span>
</span><span id="__span-65-21"><a id="__codelineno-65-21" name="__codelineno-65-21" href="#__codelineno-65-21"></a><span class="sd">        &gt;&gt;&gt; result = Tensor.where(cond, x, y)</span>
</span><span id="__span-65-22"><a id="__codelineno-65-22" name="__codelineno-65-22" href="#__codelineno-65-22"></a><span class="sd">        &gt;&gt;&gt; print(result)</span>
</span><span id="__span-65-23"><a id="__codelineno-65-23" name="__codelineno-65-23" href="#__codelineno-65-23"></a><span class="sd">        Tensor([[ 1, 20],</span>
</span><span id="__span-65-24"><a id="__codelineno-65-24" name="__codelineno-65-24" href="#__codelineno-65-24"></a><span class="sd">                [30,  4]], requires_grad=True)</span>
</span><span id="__span-65-25"><a id="__codelineno-65-25" name="__codelineno-65-25" href="#__codelineno-65-25"></a>
</span><span id="__span-65-26"><a id="__codelineno-65-26" name="__codelineno-65-26" href="#__codelineno-65-26"></a><span class="sd">    Notes:</span>
</span><span id="__span-65-27"><a id="__codelineno-65-27" name="__codelineno-65-27" href="#__codelineno-65-27"></a><span class="sd">        - The returned tensor has `requires_grad=True` if either `a` or `b` requires gradients.</span>
</span><span id="__span-65-28"><a id="__codelineno-65-28" name="__codelineno-65-28" href="#__codelineno-65-28"></a><span class="sd">        - The backward function ensures gradients are passed only to the selected elements.</span>
</span><span id="__span-65-29"><a id="__codelineno-65-29" name="__codelineno-65-29" href="#__codelineno-65-29"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-65-30"><a id="__codelineno-65-30" name="__codelineno-65-30" href="#__codelineno-65-30"></a>
</span><span id="__span-65-31"><a id="__codelineno-65-31" name="__codelineno-65-31" href="#__codelineno-65-31"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>  <span class="c1"># Element-wise selection</span>
</span><span id="__span-65-32"><a id="__codelineno-65-32" name="__codelineno-65-32" href="#__codelineno-65-32"></a>    <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span>
</span><span id="__span-65-33"><a id="__codelineno-65-33" name="__codelineno-65-33" href="#__codelineno-65-33"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-65-34"><a id="__codelineno-65-34" name="__codelineno-65-34" href="#__codelineno-65-34"></a>
</span><span id="__span-65-35"><a id="__codelineno-65-35" name="__codelineno-65-35" href="#__codelineno-65-35"></a>    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-65-36"><a id="__codelineno-65-36" name="__codelineno-65-36" href="#__codelineno-65-36"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd_a</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-65-37"><a id="__codelineno-65-37" name="__codelineno-65-37" href="#__codelineno-65-37"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-65-38"><a id="__codelineno-65-38" name="__codelineno-65-38" href="#__codelineno-65-38"></a><span class="sd">            Backward function for `a`.</span>
</span><span id="__span-65-39"><a id="__codelineno-65-39" name="__codelineno-65-39" href="#__codelineno-65-39"></a>
</span><span id="__span-65-40"><a id="__codelineno-65-40" name="__codelineno-65-40" href="#__codelineno-65-40"></a><span class="sd">            This ensures gradients flow only to the elements selected from `a`.</span>
</span><span id="__span-65-41"><a id="__codelineno-65-41" name="__codelineno-65-41" href="#__codelineno-65-41"></a>
</span><span id="__span-65-42"><a id="__codelineno-65-42" name="__codelineno-65-42" href="#__codelineno-65-42"></a><span class="sd">            Args:</span>
</span><span id="__span-65-43"><a id="__codelineno-65-43" name="__codelineno-65-43" href="#__codelineno-65-43"></a><span class="sd">                grad (np.ndarray): Gradient of the output tensor.</span>
</span><span id="__span-65-44"><a id="__codelineno-65-44" name="__codelineno-65-44" href="#__codelineno-65-44"></a>
</span><span id="__span-65-45"><a id="__codelineno-65-45" name="__codelineno-65-45" href="#__codelineno-65-45"></a><span class="sd">            Returns:</span>
</span><span id="__span-65-46"><a id="__codelineno-65-46" name="__codelineno-65-46" href="#__codelineno-65-46"></a><span class="sd">                np.ndarray: Gradient for `a`, masked where `condition` is False.</span>
</span><span id="__span-65-47"><a id="__codelineno-65-47" name="__codelineno-65-47" href="#__codelineno-65-47"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-65-48"><a id="__codelineno-65-48" name="__codelineno-65-48" href="#__codelineno-65-48"></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</span><span id="__span-65-49"><a id="__codelineno-65-49" name="__codelineno-65-49" href="#__codelineno-65-49"></a>
</span><span id="__span-65-50"><a id="__codelineno-65-50" name="__codelineno-65-50" href="#__codelineno-65-50"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Leaf</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">_bkwd_a</span><span class="p">))</span>
</span><span id="__span-65-51"><a id="__codelineno-65-51" name="__codelineno-65-51" href="#__codelineno-65-51"></a>
</span><span id="__span-65-52"><a id="__codelineno-65-52" name="__codelineno-65-52" href="#__codelineno-65-52"></a>    <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-65-53"><a id="__codelineno-65-53" name="__codelineno-65-53" href="#__codelineno-65-53"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd_b</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-65-54"><a id="__codelineno-65-54" name="__codelineno-65-54" href="#__codelineno-65-54"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-65-55"><a id="__codelineno-65-55" name="__codelineno-65-55" href="#__codelineno-65-55"></a><span class="sd">            Backward function for `b`.</span>
</span><span id="__span-65-56"><a id="__codelineno-65-56" name="__codelineno-65-56" href="#__codelineno-65-56"></a>
</span><span id="__span-65-57"><a id="__codelineno-65-57" name="__codelineno-65-57" href="#__codelineno-65-57"></a><span class="sd">            This ensures gradients flow only to the elements selected from `b`.</span>
</span><span id="__span-65-58"><a id="__codelineno-65-58" name="__codelineno-65-58" href="#__codelineno-65-58"></a>
</span><span id="__span-65-59"><a id="__codelineno-65-59" name="__codelineno-65-59" href="#__codelineno-65-59"></a><span class="sd">            Args:</span>
</span><span id="__span-65-60"><a id="__codelineno-65-60" name="__codelineno-65-60" href="#__codelineno-65-60"></a><span class="sd">                grad (np.ndarray): Gradient of the output tensor.</span>
</span><span id="__span-65-61"><a id="__codelineno-65-61" name="__codelineno-65-61" href="#__codelineno-65-61"></a>
</span><span id="__span-65-62"><a id="__codelineno-65-62" name="__codelineno-65-62" href="#__codelineno-65-62"></a><span class="sd">            Returns:</span>
</span><span id="__span-65-63"><a id="__codelineno-65-63" name="__codelineno-65-63" href="#__codelineno-65-63"></a><span class="sd">                np.ndarray: Gradient for `b`, masked where `condition` is True.</span>
</span><span id="__span-65-64"><a id="__codelineno-65-64" name="__codelineno-65-64" href="#__codelineno-65-64"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-65-65"><a id="__codelineno-65-65" name="__codelineno-65-65" href="#__codelineno-65-65"></a>            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</span><span id="__span-65-66"><a id="__codelineno-65-66" name="__codelineno-65-66" href="#__codelineno-65-66"></a>
</span><span id="__span-65-67"><a id="__codelineno-65-67" name="__codelineno-65-67" href="#__codelineno-65-67"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Leaf</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">_bkwd_b</span><span class="p">))</span>
</span><span id="__span-65-68"><a id="__codelineno-65-68" name="__codelineno-65-68" href="#__codelineno-65-68"></a>
</span><span id="__span-65-69"><a id="__codelineno-65-69" name="__codelineno-65-69" href="#__codelineno-65-69"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Example</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-66-1"><a id="__codelineno-66-1" name="__codelineno-66-1" href="#__codelineno-66-1"></a><span class="c1"># Define a condition tensor</span>
</span><span id="__span-66-2"><a id="__codelineno-66-2" name="__codelineno-66-2" href="#__codelineno-66-2"></a><span class="n">condition</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]]))</span>
</span><span id="__span-66-3"><a id="__codelineno-66-3" name="__codelineno-66-3" href="#__codelineno-66-3"></a>
</span><span id="__span-66-4"><a id="__codelineno-66-4" name="__codelineno-66-4" href="#__codelineno-66-4"></a><span class="c1"># Define input tensors</span>
</span><span id="__span-66-5"><a id="__codelineno-66-5" name="__codelineno-66-5" href="#__codelineno-66-5"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-66-6"><a id="__codelineno-66-6" name="__codelineno-66-6" href="#__codelineno-66-6"></a><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">]]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-66-7"><a id="__codelineno-66-7" name="__codelineno-66-7" href="#__codelineno-66-7"></a>
</span><span id="__span-66-8"><a id="__codelineno-66-8" name="__codelineno-66-8" href="#__codelineno-66-8"></a><span class="c1"># Apply the where function</span>
</span><span id="__span-66-9"><a id="__codelineno-66-9" name="__codelineno-66-9" href="#__codelineno-66-9"></a><span class="n">result</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span id="__span-66-10"><a id="__codelineno-66-10" name="__codelineno-66-10" href="#__codelineno-66-10"></a>
</span><span id="__span-66-11"><a id="__codelineno-66-11" name="__codelineno-66-11" href="#__codelineno-66-11"></a><span class="c1"># Print result</span>
</span><span id="__span-66-12"><a id="__codelineno-66-12" name="__codelineno-66-12" href="#__codelineno-66-12"></a><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span><span id="__span-66-13"><a id="__codelineno-66-13" name="__codelineno-66-13" href="#__codelineno-66-13"></a><span class="c1"># Output:</span>
</span><span id="__span-66-14"><a id="__codelineno-66-14" name="__codelineno-66-14" href="#__codelineno-66-14"></a><span class="c1"># Tensor([[ 1, 20],</span>
</span><span id="__span-66-15"><a id="__codelineno-66-15" name="__codelineno-66-15" href="#__codelineno-66-15"></a><span class="c1">#         [30,  4]], requires_grad=True)</span>
</span><span id="__span-66-16"><a id="__codelineno-66-16" name="__codelineno-66-16" href="#__codelineno-66-16"></a>
</span><span id="__span-66-17"><a id="__codelineno-66-17" name="__codelineno-66-17" href="#__codelineno-66-17"></a><span class="c1"># Backward pass</span>
</span><span id="__span-66-18"><a id="__codelineno-66-18" name="__codelineno-66-18" href="#__codelineno-66-18"></a><span class="n">grad_output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]))</span>
</span><span id="__span-66-19"><a id="__codelineno-66-19" name="__codelineno-66-19" href="#__codelineno-66-19"></a><span class="n">result</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="__span-66-20"><a id="__codelineno-66-20" name="__codelineno-66-20" href="#__codelineno-66-20"></a>
</span><span id="__span-66-21"><a id="__codelineno-66-21" name="__codelineno-66-21" href="#__codelineno-66-21"></a><span class="c1"># Check gradients</span>
</span><span id="__span-66-22"><a id="__codelineno-66-22" name="__codelineno-66-22" href="#__codelineno-66-22"></a><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Should have gradients only where condition is True</span>
</span><span id="__span-66-23"><a id="__codelineno-66-23" name="__codelineno-66-23" href="#__codelineno-66-23"></a><span class="c1"># [[1. 0.]</span>
</span><span id="__span-66-24"><a id="__codelineno-66-24" name="__codelineno-66-24" href="#__codelineno-66-24"></a><span class="c1">#  [0. 1.]]</span>
</span><span id="__span-66-25"><a id="__codelineno-66-25" name="__codelineno-66-25" href="#__codelineno-66-25"></a>
</span><span id="__span-66-26"><a id="__codelineno-66-26" name="__codelineno-66-26" href="#__codelineno-66-26"></a><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Should have gradients only where condition is False</span>
</span><span id="__span-66-27"><a id="__codelineno-66-27" name="__codelineno-66-27" href="#__codelineno-66-27"></a><span class="c1"># [[0. 1.]</span>
</span><span id="__span-66-28"><a id="__codelineno-66-28" name="__codelineno-66-28" href="#__codelineno-66-28"></a><span class="c1">#  [1. 0.]]</span>
</span><span id="__span-66-29"><a id="__codelineno-66-29" name="__codelineno-66-29" href="#__codelineno-66-29"></a>
</span><span id="__span-66-30"><a id="__codelineno-66-30" name="__codelineno-66-30" href="#__codelineno-66-30"></a><span class="c1"># You can use the conditional operation</span>
</span><span id="__span-66-31"><a id="__codelineno-66-31" name="__codelineno-66-31" href="#__codelineno-66-31"></a><span class="c1"># Create two tensors</span>
</span><span id="__span-66-32"><a id="__codelineno-66-32" name="__codelineno-66-32" href="#__codelineno-66-32"></a><span class="n">a</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
</span><span id="__span-66-33"><a id="__codelineno-66-33" name="__codelineno-66-33" href="#__codelineno-66-33"></a><span class="n">b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">]))</span>
</span><span id="__span-66-34"><a id="__codelineno-66-34" name="__codelineno-66-34" href="#__codelineno-66-34"></a>
</span><span id="__span-66-35"><a id="__codelineno-66-35" name="__codelineno-66-35" href="#__codelineno-66-35"></a><span class="c1"># Compare the tensors using a conditional operation</span>
</span><span id="__span-66-36"><a id="__codelineno-66-36" name="__codelineno-66-36" href="#__codelineno-66-36"></a><span class="n">result_lt</span> <span class="o">=</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">b</span>
</span><span id="__span-66-37"><a id="__codelineno-66-37" name="__codelineno-66-37" href="#__codelineno-66-37"></a><span class="nb">print</span><span class="p">(</span><span class="n">result_lt</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-66-38"><a id="__codelineno-66-38" name="__codelineno-66-38" href="#__codelineno-66-38"></a><span class="c1"># Output: [True False False True]</span>
</span><span id="__span-66-39"><a id="__codelineno-66-39" name="__codelineno-66-39" href="#__codelineno-66-39"></a>
</span><span id="__span-66-40"><a id="__codelineno-66-40" name="__codelineno-66-40" href="#__codelineno-66-40"></a><span class="c1"># Use the `where` operation to select values based on the comparison result</span>
</span><span id="__span-66-41"><a id="__codelineno-66-41" name="__codelineno-66-41" href="#__codelineno-66-41"></a><span class="n">result</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">result_lt</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span><span id="__span-66-42"><a id="__codelineno-66-42" name="__codelineno-66-42" href="#__codelineno-66-42"></a><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-66-43"><a id="__codelineno-66-43" name="__codelineno-66-43" href="#__codelineno-66-43"></a><span class="c1"># Output: [1 2 3 5]</span>
</span></code></pre></div>
<h3 id="expanding-where-more-useful-tensor-operations">Expanding <code>where</code> - More Useful Tensor Operations<a class="headerlink" href="#expanding-where-more-useful-tensor-operations" title="Permanent link">&para;</a></h3>
<p>With the static method <code>where</code>, we can implement several useful tensor operations like <code>maximum</code> and <code>minimum</code>.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-67-1"><a id="__codelineno-67-1" name="__codelineno-67-1" href="#__codelineno-67-1"></a><span class="nd">@staticmethod</span>
</span><span id="__span-67-2"><a id="__codelineno-67-2" name="__codelineno-67-2" href="#__codelineno-67-2"></a><span class="k">def</span><span class="w"> </span><span class="nf">maximum</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">Data</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-67-3"><a id="__codelineno-67-3" name="__codelineno-67-3" href="#__codelineno-67-3"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-67-4"><a id="__codelineno-67-4" name="__codelineno-67-4" href="#__codelineno-67-4"></a><span class="sd">    Apply element-wise max operation: max(a: &quot;Tensor&quot;, b: &quot;Tensor&quot;) -&gt; &quot;Tensor&quot;</span>
</span><span id="__span-67-5"><a id="__codelineno-67-5" name="__codelineno-67-5" href="#__codelineno-67-5"></a><span class="sd">    Returns a Tensor with the result of element-wise maximum.</span>
</span><span id="__span-67-6"><a id="__codelineno-67-6" name="__codelineno-67-6" href="#__codelineno-67-6"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-67-7"><a id="__codelineno-67-7" name="__codelineno-67-7" href="#__codelineno-67-7"></a>
</span><span id="__span-67-8"><a id="__codelineno-67-8" name="__codelineno-67-8" href="#__codelineno-67-8"></a>    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span><span id="__span-67-9"><a id="__codelineno-67-9" name="__codelineno-67-9" href="#__codelineno-67-9"></a>
</span><span id="__span-67-10"><a id="__codelineno-67-10" name="__codelineno-67-10" href="#__codelineno-67-10"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span><span id="__span-67-11"><a id="__codelineno-67-11" name="__codelineno-67-11" href="#__codelineno-67-11"></a>
</span><span id="__span-67-12"><a id="__codelineno-67-12" name="__codelineno-67-12" href="#__codelineno-67-12"></a><span class="nd">@staticmethod</span>
</span><span id="__span-67-13"><a id="__codelineno-67-13" name="__codelineno-67-13" href="#__codelineno-67-13"></a><span class="k">def</span><span class="w"> </span><span class="nf">minimum</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">Data</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-67-14"><a id="__codelineno-67-14" name="__codelineno-67-14" href="#__codelineno-67-14"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-67-15"><a id="__codelineno-67-15" name="__codelineno-67-15" href="#__codelineno-67-15"></a><span class="sd">    Apply element-wise min operation: min(a: &quot;Tensor&quot;, b: &quot;Tensor&quot;) -&gt; &quot;Tensor&quot;</span>
</span><span id="__span-67-16"><a id="__codelineno-67-16" name="__codelineno-67-16" href="#__codelineno-67-16"></a><span class="sd">    Returns a Tensor with the result of element-wise minimum.</span>
</span><span id="__span-67-17"><a id="__codelineno-67-17" name="__codelineno-67-17" href="#__codelineno-67-17"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-67-18"><a id="__codelineno-67-18" name="__codelineno-67-18" href="#__codelineno-67-18"></a>
</span><span id="__span-67-19"><a id="__codelineno-67-19" name="__codelineno-67-19" href="#__codelineno-67-19"></a>    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span><span id="__span-67-20"><a id="__codelineno-67-20" name="__codelineno-67-20" href="#__codelineno-67-20"></a>
</span><span id="__span-67-21"><a id="__codelineno-67-21" name="__codelineno-67-21" href="#__codelineno-67-21"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></code></pre></div>
<p>These implementations are clean and use where for efficient, element-wise comparisons. We can push this further and implement advanced operations in a concise way. For example - the <code>threshold</code> method sets values above a given threshold while leaving others unchanged.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-68-1"><a id="__codelineno-68-1" name="__codelineno-68-1" href="#__codelineno-68-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">threshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-68-2"><a id="__codelineno-68-2" name="__codelineno-68-2" href="#__codelineno-68-2"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
</span></code></pre></div>
<p>The <code>masked_fill</code> method replaces values based on a boolean mask.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-69-1"><a id="__codelineno-69-1" name="__codelineno-69-1" href="#__codelineno-69-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-69-2"><a id="__codelineno-69-2" name="__codelineno-69-2" href="#__codelineno-69-2"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span>
</span></code></pre></div>
<p>The <code>sign</code> method returns the sign of each element in the tensor</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-70-1"><a id="__codelineno-70-1" name="__codelineno-70-1" href="#__codelineno-70-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">sign</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-70-2"><a id="__codelineno-70-2" name="__codelineno-70-2" href="#__codelineno-70-2"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
</span><span id="__span-70-3"><a id="__codelineno-70-3" name="__codelineno-70-3" href="#__codelineno-70-3"></a>        <span class="bp">self</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
</span><span id="__span-70-4"><a id="__codelineno-70-4" name="__codelineno-70-4" href="#__codelineno-70-4"></a>        <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span><span id="__span-70-5"><a id="__codelineno-70-5" name="__codelineno-70-5" href="#__codelineno-70-5"></a>    <span class="p">)</span>
</span></code></pre></div>
<p>We can also rewrite the <code>clip</code> method using where. This simplifies the code by handling both the forward and backward passes through the <code>where</code> implementation.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-71-1"><a id="__codelineno-71-1" name="__codelineno-71-1" href="#__codelineno-71-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">clip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-71-2"><a id="__codelineno-71-2" name="__codelineno-71-2" href="#__codelineno-71-2"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
</span><span id="__span-71-3"><a id="__codelineno-71-3" name="__codelineno-71-3" href="#__codelineno-71-3"></a>        <span class="bp">self</span> <span class="o">&lt;</span> <span class="n">min_value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">min_value</span><span class="p">),</span>
</span><span id="__span-71-4"><a id="__codelineno-71-4" name="__codelineno-71-4" href="#__codelineno-71-4"></a>        <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span> <span class="o">&gt;</span> <span class="n">max_value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">max_value</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span>
</span><span id="__span-71-5"><a id="__codelineno-71-5" name="__codelineno-71-5" href="#__codelineno-71-5"></a>    <span class="p">)</span>
</span></code></pre></div>
<p>By using <code>where</code>, we reduce code duplication and ensure consistency across these tensor operations.</p>
<h2 id="indexing">Indexing<a class="headerlink" href="#indexing" title="Permanent link">&para;</a></h2>
<p>Indexing in the <code>Tensor</code> class allows selecting specific elements from the tensor using another tensor or a NumPy array as an index. This operation is useful for extracting sub-tensors or performing advanced slicing operations.</p>
<iframe width="1707" height="934" src="https://www.youtube.com/embed/mk2FkpeZOAM?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p>When <code>requires_grad=True</code>, the backward pass ensures that gradients are propagated correctly by constructing a zero tensor and filling only the indexed positions with the incoming gradient.</p>
<p>Valid Python index types:</p>
<ul>
<li><code>int</code> (for scalar indexing)</li>
<li><code>slice</code> (for range selection like x[1:3])</li>
<li><code>Tuple</code> (for multi-dimensional indexing)</li>
<li><code>List</code> (for list-based indexing)</li>
</ul>
<p>Also we can include the <code>np.ndarray</code> and <code>Tensor</code> for flexibility! </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-72-1"><a id="__codelineno-72-1" name="__codelineno-72-1" href="#__codelineno-72-1"></a><span class="n">IndexType</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">slice</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">]</span>
</span><span id="__span-72-2"><a id="__codelineno-72-2" name="__codelineno-72-2" href="#__codelineno-72-2"></a>
</span><span id="__span-72-3"><a id="__codelineno-72-3" name="__codelineno-72-3" href="#__codelineno-72-3"></a><span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">IndexType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-72-4"><a id="__codelineno-72-4" name="__codelineno-72-4" href="#__codelineno-72-4"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-72-5"><a id="__codelineno-72-5" name="__codelineno-72-5" href="#__codelineno-72-5"></a><span class="sd">    Perform indexing on the tensor.</span>
</span><span id="__span-72-6"><a id="__codelineno-72-6" name="__codelineno-72-6" href="#__codelineno-72-6"></a>
</span><span id="__span-72-7"><a id="__codelineno-72-7" name="__codelineno-72-7" href="#__codelineno-72-7"></a><span class="sd">    This method allows for selecting specific elements from the tensor using </span>
</span><span id="__span-72-8"><a id="__codelineno-72-8" name="__codelineno-72-8" href="#__codelineno-72-8"></a><span class="sd">    another tensor or a NumPy array as an index.</span>
</span><span id="__span-72-9"><a id="__codelineno-72-9" name="__codelineno-72-9" href="#__codelineno-72-9"></a>
</span><span id="__span-72-10"><a id="__codelineno-72-10" name="__codelineno-72-10" href="#__codelineno-72-10"></a><span class="sd">    Args:</span>
</span><span id="__span-72-11"><a id="__codelineno-72-11" name="__codelineno-72-11" href="#__codelineno-72-11"></a><span class="sd">        index (IndexType): The indices to select from the tensor.</span>
</span><span id="__span-72-12"><a id="__codelineno-72-12" name="__codelineno-72-12" href="#__codelineno-72-12"></a>
</span><span id="__span-72-13"><a id="__codelineno-72-13" name="__codelineno-72-13" href="#__codelineno-72-13"></a><span class="sd">    Returns:</span>
</span><span id="__span-72-14"><a id="__codelineno-72-14" name="__codelineno-72-14" href="#__codelineno-72-14"></a><span class="sd">        Tensor: A new tensor containing the selected elements.</span>
</span><span id="__span-72-15"><a id="__codelineno-72-15" name="__codelineno-72-15" href="#__codelineno-72-15"></a>
</span><span id="__span-72-16"><a id="__codelineno-72-16" name="__codelineno-72-16" href="#__codelineno-72-16"></a><span class="sd">    Example:</span>
</span><span id="__span-72-17"><a id="__codelineno-72-17" name="__codelineno-72-17" href="#__codelineno-72-17"></a><span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6]], requires_grad=True)</span>
</span><span id="__span-72-18"><a id="__codelineno-72-18" name="__codelineno-72-18" href="#__codelineno-72-18"></a><span class="sd">        &gt;&gt;&gt; idx = Tensor([0, 2])</span>
</span><span id="__span-72-19"><a id="__codelineno-72-19" name="__codelineno-72-19" href="#__codelineno-72-19"></a><span class="sd">        &gt;&gt;&gt; y = x[0, idx]  # Selects elements [1, 3]</span>
</span><span id="__span-72-20"><a id="__codelineno-72-20" name="__codelineno-72-20" href="#__codelineno-72-20"></a><span class="sd">        &gt;&gt;&gt; print(y)</span>
</span><span id="__span-72-21"><a id="__codelineno-72-21" name="__codelineno-72-21" href="#__codelineno-72-21"></a><span class="sd">        Tensor([1, 3], requires_grad=True, shape=(2,))</span>
</span><span id="__span-72-22"><a id="__codelineno-72-22" name="__codelineno-72-22" href="#__codelineno-72-22"></a>
</span><span id="__span-72-23"><a id="__codelineno-72-23" name="__codelineno-72-23" href="#__codelineno-72-23"></a><span class="sd">    Notes:</span>
</span><span id="__span-72-24"><a id="__codelineno-72-24" name="__codelineno-72-24" href="#__codelineno-72-24"></a><span class="sd">        - If `requires_grad=True`, the backward function ensures that the gradient </span>
</span><span id="__span-72-25"><a id="__codelineno-72-25" name="__codelineno-72-25" href="#__codelineno-72-25"></a><span class="sd">          is only applied to the indexed positions.</span>
</span><span id="__span-72-26"><a id="__codelineno-72-26" name="__codelineno-72-26" href="#__codelineno-72-26"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-72-27"><a id="__codelineno-72-27" name="__codelineno-72-27" href="#__codelineno-72-27"></a>
</span><span id="__span-72-28"><a id="__codelineno-72-28" name="__codelineno-72-28" href="#__codelineno-72-28"></a>    <span class="c1"># Normalize tensor-based indexing to numpy arrays</span>
</span><span id="__span-72-29"><a id="__codelineno-72-29" name="__codelineno-72-29" href="#__codelineno-72-29"></a>    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
</span><span id="__span-72-30"><a id="__codelineno-72-30" name="__codelineno-72-30" href="#__codelineno-72-30"></a>        <span class="n">index</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">data_gate</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
</span><span id="__span-72-31"><a id="__codelineno-72-31" name="__codelineno-72-31" href="#__codelineno-72-31"></a>
</span><span id="__span-72-32"><a id="__codelineno-72-32" name="__codelineno-72-32" href="#__codelineno-72-32"></a>    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>  <span class="c1"># Perform indexing operation</span>
</span><span id="__span-72-33"><a id="__codelineno-72-33" name="__codelineno-72-33" href="#__codelineno-72-33"></a>    <span class="n">dependencies</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-72-34"><a id="__codelineno-72-34" name="__codelineno-72-34" href="#__codelineno-72-34"></a>
</span><span id="__span-72-35"><a id="__codelineno-72-35" name="__codelineno-72-35" href="#__codelineno-72-35"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-72-36"><a id="__codelineno-72-36" name="__codelineno-72-36" href="#__codelineno-72-36"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-72-37"><a id="__codelineno-72-37" name="__codelineno-72-37" href="#__codelineno-72-37"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-72-38"><a id="__codelineno-72-38" name="__codelineno-72-38" href="#__codelineno-72-38"></a><span class="sd">            Backward pass for tensor indexing.</span>
</span><span id="__span-72-39"><a id="__codelineno-72-39" name="__codelineno-72-39" href="#__codelineno-72-39"></a>
</span><span id="__span-72-40"><a id="__codelineno-72-40" name="__codelineno-72-40" href="#__codelineno-72-40"></a><span class="sd">            This function constructs a zero tensor of the same shape as the original </span>
</span><span id="__span-72-41"><a id="__codelineno-72-41" name="__codelineno-72-41" href="#__codelineno-72-41"></a><span class="sd">            tensor and assigns the incoming gradient only to the indexed positions.</span>
</span><span id="__span-72-42"><a id="__codelineno-72-42" name="__codelineno-72-42" href="#__codelineno-72-42"></a>
</span><span id="__span-72-43"><a id="__codelineno-72-43" name="__codelineno-72-43" href="#__codelineno-72-43"></a><span class="sd">            Args:</span>
</span><span id="__span-72-44"><a id="__codelineno-72-44" name="__codelineno-72-44" href="#__codelineno-72-44"></a><span class="sd">                grad (np.ndarray): Gradient from the next layer.</span>
</span><span id="__span-72-45"><a id="__codelineno-72-45" name="__codelineno-72-45" href="#__codelineno-72-45"></a>
</span><span id="__span-72-46"><a id="__codelineno-72-46" name="__codelineno-72-46" href="#__codelineno-72-46"></a><span class="sd">            Returns:</span>
</span><span id="__span-72-47"><a id="__codelineno-72-47" name="__codelineno-72-47" href="#__codelineno-72-47"></a><span class="sd">                np.ndarray: Gradient propagated back to the indexed positions.</span>
</span><span id="__span-72-48"><a id="__codelineno-72-48" name="__codelineno-72-48" href="#__codelineno-72-48"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-72-49"><a id="__codelineno-72-49" name="__codelineno-72-49" href="#__codelineno-72-49"></a>
</span><span id="__span-72-50"><a id="__codelineno-72-50" name="__codelineno-72-50" href="#__codelineno-72-50"></a>            <span class="n">full_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-72-51"><a id="__codelineno-72-51" name="__codelineno-72-51" href="#__codelineno-72-51"></a>            <span class="c1"># Handle multiple uses of the same index correctly</span>
</span><span id="__span-72-52"><a id="__codelineno-72-52" name="__codelineno-72-52" href="#__codelineno-72-52"></a>            <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">at</span><span class="p">(</span><span class="n">full_grad</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</span><span id="__span-72-53"><a id="__codelineno-72-53" name="__codelineno-72-53" href="#__codelineno-72-53"></a>            <span class="k">return</span> <span class="n">full_grad</span>
</span><span id="__span-72-54"><a id="__codelineno-72-54" name="__codelineno-72-54" href="#__codelineno-72-54"></a>
</span><span id="__span-72-55"><a id="__codelineno-72-55" name="__codelineno-72-55" href="#__codelineno-72-55"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span><span class="p">))</span>
</span><span id="__span-72-56"><a id="__codelineno-72-56" name="__codelineno-72-56" href="#__codelineno-72-56"></a>
</span><span id="__span-72-57"><a id="__codelineno-72-57" name="__codelineno-72-57" href="#__codelineno-72-57"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Example Usage:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-73-1"><a id="__codelineno-73-1" name="__codelineno-73-1" href="#__codelineno-73-1"></a><span class="c1"># Create a tensor</span>
</span><span id="__span-73-2"><a id="__codelineno-73-2" name="__codelineno-73-2" href="#__codelineno-73-2"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">60</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-73-3"><a id="__codelineno-73-3" name="__codelineno-73-3" href="#__codelineno-73-3"></a>
</span><span id="__span-73-4"><a id="__codelineno-73-4" name="__codelineno-73-4" href="#__codelineno-73-4"></a><span class="c1"># Indexing operation</span>
</span><span id="__span-73-5"><a id="__codelineno-73-5" name="__codelineno-73-5" href="#__codelineno-73-5"></a><span class="n">idx</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>  <span class="c1"># Selecting elements at index 0 and 2</span>
</span><span id="__span-73-6"><a id="__codelineno-73-6" name="__codelineno-73-6" href="#__codelineno-73-6"></a><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span>  <span class="c1"># Retrieves [10, 30]</span>
</span><span id="__span-73-7"><a id="__codelineno-73-7" name="__codelineno-73-7" href="#__codelineno-73-7"></a>
</span><span id="__span-73-8"><a id="__codelineno-73-8" name="__codelineno-73-8" href="#__codelineno-73-8"></a><span class="c1"># Print result</span>
</span><span id="__span-73-9"><a id="__codelineno-73-9" name="__codelineno-73-9" href="#__codelineno-73-9"></a><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># Tensor([10, 30], requires_grad=True, shape=(2,))</span>
</span><span id="__span-73-10"><a id="__codelineno-73-10" name="__codelineno-73-10" href="#__codelineno-73-10"></a>
</span><span id="__span-73-11"><a id="__codelineno-73-11" name="__codelineno-73-11" href="#__codelineno-73-11"></a><span class="c1"># Backward pass</span>
</span><span id="__span-73-12"><a id="__codelineno-73-12" name="__codelineno-73-12" href="#__codelineno-73-12"></a><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]))</span>  <span class="c1"># Set gradient to ones</span>
</span><span id="__span-73-13"><a id="__codelineno-73-13" name="__codelineno-73-13" href="#__codelineno-73-13"></a>
</span><span id="__span-73-14"><a id="__codelineno-73-14" name="__codelineno-73-14" href="#__codelineno-73-14"></a><span class="c1"># Check gradients</span>
</span><span id="__span-73-15"><a id="__codelineno-73-15" name="__codelineno-73-15" href="#__codelineno-73-15"></a><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  
</span><span id="__span-73-16"><a id="__codelineno-73-16" name="__codelineno-73-16" href="#__codelineno-73-16"></a><span class="c1"># Output should propagate gradients only to the selected positions:</span>
</span><span id="__span-73-17"><a id="__codelineno-73-17" name="__codelineno-73-17" href="#__codelineno-73-17"></a><span class="c1"># [[1.  0.  1.]</span>
</span><span id="__span-73-18"><a id="__codelineno-73-18" name="__codelineno-73-18" href="#__codelineno-73-18"></a><span class="c1">#  [0.  0.  0.]]</span>
</span></code></pre></div>
<h2 id="more-tensor-ops">More Tensor ops<a class="headerlink" href="#more-tensor-ops" title="Permanent link">&para;</a></h2>
<h3 id="abs"><code>abs</code><a class="headerlink" href="#abs" title="Permanent link">&para;</a></h3>
<p>The <strong>absolute value</strong> operation computes the magnitude of each element in a tensor, disregarding the sign. </p>
<div class="arithmatex">\[\text{abs}(x) = |x|\]</div>
<p>The derivative of <span class="arithmatex">\(\text{abs}(x)\)</span> is:</p>
<div class="arithmatex">\[\frac{d}{dx} |x| = \text{sgn}(x)\]</div>
<p>where the sign function <span class="arithmatex">\(\text{sgn}(x)\)</span> is defined as:</p>
<div class="arithmatex">\[\text{sgn}(x) =
\begin{cases}
  1, &amp; \text{if } x &gt; 0 \\
  -1, &amp; \text{if } x &lt; 0 \\
  0, &amp; \text{if } x = 0
\end{cases}\]</div>
<p>Python Code for <code>abs</code> Operation with the Derivative</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-74-1"><a id="__codelineno-74-1" name="__codelineno-74-1" href="#__codelineno-74-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-74-2"><a id="__codelineno-74-2" name="__codelineno-74-2" href="#__codelineno-74-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-74-3"><a id="__codelineno-74-3" name="__codelineno-74-3" href="#__codelineno-74-3"></a><span class="sd">    Computes the absolute value of the tensor&#39;s elements.</span>
</span><span id="__span-74-4"><a id="__codelineno-74-4" name="__codelineno-74-4" href="#__codelineno-74-4"></a>
</span><span id="__span-74-5"><a id="__codelineno-74-5" name="__codelineno-74-5" href="#__codelineno-74-5"></a><span class="sd">    The absolute value of each element is computed element-wise, and the result is returned as a new tensor.</span>
</span><span id="__span-74-6"><a id="__codelineno-74-6" name="__codelineno-74-6" href="#__codelineno-74-6"></a>
</span><span id="__span-74-7"><a id="__codelineno-74-7" name="__codelineno-74-7" href="#__codelineno-74-7"></a><span class="sd">    Returns:</span>
</span><span id="__span-74-8"><a id="__codelineno-74-8" name="__codelineno-74-8" href="#__codelineno-74-8"></a><span class="sd">        Tensor: A new tensor containing the absolute values of the input tensor&#39;s elements.</span>
</span><span id="__span-74-9"><a id="__codelineno-74-9" name="__codelineno-74-9" href="#__codelineno-74-9"></a>
</span><span id="__span-74-10"><a id="__codelineno-74-10" name="__codelineno-74-10" href="#__codelineno-74-10"></a><span class="sd">    The derivative of the absolute value function is handled in the backward pass using the sign of the input tensor.</span>
</span><span id="__span-74-11"><a id="__codelineno-74-11" name="__codelineno-74-11" href="#__codelineno-74-11"></a><span class="sd">    The gradient for positive values is 1, for negative values is -1, and the gradient is undefined at zero.</span>
</span><span id="__span-74-12"><a id="__codelineno-74-12" name="__codelineno-74-12" href="#__codelineno-74-12"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-74-13"><a id="__codelineno-74-13" name="__codelineno-74-13" href="#__codelineno-74-13"></a>
</span><span id="__span-74-14"><a id="__codelineno-74-14" name="__codelineno-74-14" href="#__codelineno-74-14"></a>    <span class="c1"># Perform absolute value operation on the data</span>
</span><span id="__span-74-15"><a id="__codelineno-74-15" name="__codelineno-74-15" href="#__codelineno-74-15"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-74-16"><a id="__codelineno-74-16" name="__codelineno-74-16" href="#__codelineno-74-16"></a>
</span><span id="__span-74-17"><a id="__codelineno-74-17" name="__codelineno-74-17" href="#__codelineno-74-17"></a>    <span class="c1"># Initialize the list of dependencies for gradient calculation</span>
</span><span id="__span-74-18"><a id="__codelineno-74-18" name="__codelineno-74-18" href="#__codelineno-74-18"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-74-19"><a id="__codelineno-74-19" name="__codelineno-74-19" href="#__codelineno-74-19"></a>
</span><span id="__span-74-20"><a id="__codelineno-74-20" name="__codelineno-74-20" href="#__codelineno-74-20"></a>    <span class="c1"># Backward function to compute the gradient for the absolute value operation</span>
</span><span id="__span-74-21"><a id="__codelineno-74-21" name="__codelineno-74-21" href="#__codelineno-74-21"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-74-22"><a id="__codelineno-74-22" name="__codelineno-74-22" href="#__codelineno-74-22"></a><span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-74-23"><a id="__codelineno-74-23" name="__codelineno-74-23" href="#__codelineno-74-23"></a><span class="sd">        Compute the gradient of the absolute value operation.</span>
</span><span id="__span-74-24"><a id="__codelineno-74-24" name="__codelineno-74-24" href="#__codelineno-74-24"></a>
</span><span id="__span-74-25"><a id="__codelineno-74-25" name="__codelineno-74-25" href="#__codelineno-74-25"></a><span class="sd">        Args:</span>
</span><span id="__span-74-26"><a id="__codelineno-74-26" name="__codelineno-74-26" href="#__codelineno-74-26"></a><span class="sd">            grad (np.ndarray): The gradient passed from the downstream operation.</span>
</span><span id="__span-74-27"><a id="__codelineno-74-27" name="__codelineno-74-27" href="#__codelineno-74-27"></a>
</span><span id="__span-74-28"><a id="__codelineno-74-28" name="__codelineno-74-28" href="#__codelineno-74-28"></a><span class="sd">        Returns:</span>
</span><span id="__span-74-29"><a id="__codelineno-74-29" name="__codelineno-74-29" href="#__codelineno-74-29"></a><span class="sd">            np.ndarray: The gradient for the input tensor.</span>
</span><span id="__span-74-30"><a id="__codelineno-74-30" name="__codelineno-74-30" href="#__codelineno-74-30"></a>
</span><span id="__span-74-31"><a id="__codelineno-74-31" name="__codelineno-74-31" href="#__codelineno-74-31"></a><span class="sd">        The gradient of abs(x) is the sign of x:</span>
</span><span id="__span-74-32"><a id="__codelineno-74-32" name="__codelineno-74-32" href="#__codelineno-74-32"></a><span class="sd">        - If x &gt; 0, the gradient is 1.</span>
</span><span id="__span-74-33"><a id="__codelineno-74-33" name="__codelineno-74-33" href="#__codelineno-74-33"></a><span class="sd">        - If x &lt; 0, the gradient is -1.</span>
</span><span id="__span-74-34"><a id="__codelineno-74-34" name="__codelineno-74-34" href="#__codelineno-74-34"></a><span class="sd">        - The gradient is undefined at x = 0.</span>
</span><span id="__span-74-35"><a id="__codelineno-74-35" name="__codelineno-74-35" href="#__codelineno-74-35"></a><span class="sd">        &quot;&quot;&quot;</span>
</span><span id="__span-74-36"><a id="__codelineno-74-36" name="__codelineno-74-36" href="#__codelineno-74-36"></a>
</span><span id="__span-74-37"><a id="__codelineno-74-37" name="__codelineno-74-37" href="#__codelineno-74-37"></a>        <span class="c1"># The derivative of abs(x) is the sign of x: 1 for positive x, -1 for negative x</span>
</span><span id="__span-74-38"><a id="__codelineno-74-38" name="__codelineno-74-38" href="#__codelineno-74-38"></a>        <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-74-39"><a id="__codelineno-74-39" name="__codelineno-74-39" href="#__codelineno-74-39"></a>
</span><span id="__span-74-40"><a id="__codelineno-74-40" name="__codelineno-74-40" href="#__codelineno-74-40"></a>    <span class="c1"># If the tensor requires gradients, add the backward function to the dependencies list</span>
</span><span id="__span-74-41"><a id="__codelineno-74-41" name="__codelineno-74-41" href="#__codelineno-74-41"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-74-42"><a id="__codelineno-74-42" name="__codelineno-74-42" href="#__codelineno-74-42"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-74-43"><a id="__codelineno-74-43" name="__codelineno-74-43" href="#__codelineno-74-43"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-74-44"><a id="__codelineno-74-44" name="__codelineno-74-44" href="#__codelineno-74-44"></a>                <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>  <span class="c1"># The input tensor</span>
</span><span id="__span-74-45"><a id="__codelineno-74-45" name="__codelineno-74-45" href="#__codelineno-74-45"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span>  <span class="c1"># The backward function to compute the gradients</span>
</span><span id="__span-74-46"><a id="__codelineno-74-46" name="__codelineno-74-46" href="#__codelineno-74-46"></a>            <span class="p">)</span>
</span><span id="__span-74-47"><a id="__codelineno-74-47" name="__codelineno-74-47" href="#__codelineno-74-47"></a>        <span class="p">)</span>
</span><span id="__span-74-48"><a id="__codelineno-74-48" name="__codelineno-74-48" href="#__codelineno-74-48"></a>
</span><span id="__span-74-49"><a id="__codelineno-74-49" name="__codelineno-74-49" href="#__codelineno-74-49"></a>    <span class="c1"># Return a new tensor containing the absolute values, with the gradient dependencies if needed</span>
</span><span id="__span-74-50"><a id="__codelineno-74-50" name="__codelineno-74-50" href="#__codelineno-74-50"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>We can also leverage <code>where</code> to implement the <code>abs</code> method. This approach removes the need for an explicit backward implementation since <code>where</code> already handles it internally. Here's a clean, one-liner implementation:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-75-1"><a id="__codelineno-75-1" name="__codelineno-75-1" href="#__codelineno-75-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-75-2"><a id="__codelineno-75-2" name="__codelineno-75-2" href="#__codelineno-75-2"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="max"><code>max</code><a class="headerlink" href="#max" title="Permanent link">&para;</a></h3>
<iframe width="1233" height="694" src="https://www.youtube.com/embed/olINYduKFJY?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p>The <strong>max operation</strong> returns the maximum value of a tensor along a specified axis. If no axis is specified, it returns the maximum value from the entire tensor.</p>
<p>For differentiation, the gradient of the maximum function is defined as:</p>
<div class="arithmatex">\[\frac{d}{dx} \max(X) =
\begin{cases}
  1, &amp; \text{if } x \text{ is the maximum value} \\
  0, &amp; \text{otherwise}
\end{cases}\]</div>
<p>During backpropagation, only the maximum value(s) receive a gradient, while all other elements receive 0. But things are more complicated and to deconvolve here are the general steps for backward calculation:</p>
<ul>
<li>
<p>Identify which values are the maximum across the specified axis (or globally).</p>
</li>
<li>
<p>Create a mask that indicates which elements of the original tensor were involved in the maximum value (i.e., the <code>1</code>'s).</p>
</li>
<li>
<p>Distribute gradients only to those maximum values, propagating the gradient from downstream only to the maximum values.</p>
</li>
<li>
<p>Handle multiple maxima (if two values are the same and are both maximum, gradients are split equally between them).</p>
</li>
</ul>
<p>For the forward pass it's pretty obvious:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-76-1"><a id="__codelineno-76-1" name="__codelineno-76-1" href="#__codelineno-76-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-76-2"><a id="__codelineno-76-2" name="__codelineno-76-2" href="#__codelineno-76-2"></a>    <span class="c1"># Calculate the maximum values in forward mode</span>
</span><span id="__span-76-3"><a id="__codelineno-76-3" name="__codelineno-76-3" href="#__codelineno-76-3"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
</span><span id="__span-76-4"><a id="__codelineno-76-4" name="__codelineno-76-4" href="#__codelineno-76-4"></a>
</span><span id="__span-76-5"><a id="__codelineno-76-5" name="__codelineno-76-5" href="#__codelineno-76-5"></a>    <span class="n">dependencies</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-76-6"><a id="__codelineno-76-6" name="__codelineno-76-6" href="#__codelineno-76-6"></a>
</span><span id="__span-76-7"><a id="__codelineno-76-7" name="__codelineno-76-7" href="#__codelineno-76-7"></a>    <span class="c1"># backward is here...</span>
</span><span id="__span-76-8"><a id="__codelineno-76-8" name="__codelineno-76-8" href="#__codelineno-76-8"></a>
</span><span id="__span-76-9"><a id="__codelineno-76-9" name="__codelineno-76-9" href="#__codelineno-76-9"></a>    <span class="c1"># Return a new tensor containing the absolute values, with the gradient dependencies if needed</span>
</span><span id="__span-76-10"><a id="__codelineno-76-10" name="__codelineno-76-10" href="#__codelineno-76-10"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>And for the backward let's take an approach where we work through each test case, identify what it requires from the <code>max</code> method, and implement the corresponding logic step by step.</p>
<p><strong>Case 1: Simple case - max with no axis (scalar output)</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-77-1"><a id="__codelineno-77-1" name="__codelineno-77-1" href="#__codelineno-77-1"></a><span class="n">a1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-77-2"><a id="__codelineno-77-2" name="__codelineno-77-2" href="#__codelineno-77-2"></a><span class="n">b1</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>  <span class="c1"># Should be 4</span>
</span><span id="__span-77-3"><a id="__codelineno-77-3" name="__codelineno-77-3" href="#__codelineno-77-3"></a>
</span><span id="__span-77-4"><a id="__codelineno-77-4" name="__codelineno-77-4" href="#__codelineno-77-4"></a><span class="n">b1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span id="__span-77-5"><a id="__codelineno-77-5" name="__codelineno-77-5" href="#__codelineno-77-5"></a><span class="nb">print</span><span class="p">(</span><span class="n">a1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Expected: [0, 0, 0, 1]</span>
</span></code></pre></div>
<p>During backpropagation, assign a gradient of 1 to the position of the maximum values by identifying the position of the max value using a mask and distributing the incoming gradient across the max values.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-78-1"><a id="__codelineno-78-1" name="__codelineno-78-1" href="#__codelineno-78-1"></a><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-78-2"><a id="__codelineno-78-2" name="__codelineno-78-2" href="#__codelineno-78-2"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-78-3"><a id="__codelineno-78-3" name="__codelineno-78-3" href="#__codelineno-78-3"></a>        <span class="c1"># Create a mask where max values are True</span>
</span><span id="__span-78-4"><a id="__codelineno-78-4" name="__codelineno-78-4" href="#__codelineno-78-4"></a>        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="n">output</span><span class="p">)</span>
</span><span id="__span-78-5"><a id="__codelineno-78-5" name="__codelineno-78-5" href="#__codelineno-78-5"></a>        <span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">grad</span>
</span><span id="__span-78-6"><a id="__codelineno-78-6" name="__codelineno-78-6" href="#__codelineno-78-6"></a>
</span><span id="__span-78-7"><a id="__codelineno-78-7" name="__codelineno-78-7" href="#__codelineno-78-7"></a>    <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span><span class="p">))</span>
</span></code></pre></div>
<p>The gradient should match the expected value: <code>[0, 0, 0, 1]</code>. </p>
<p><strong>Case 2: Max with Duplicate Maximums</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-79-1"><a id="__codelineno-79-1" name="__codelineno-79-1" href="#__codelineno-79-1"></a><span class="n">a2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-79-2"><a id="__codelineno-79-2" name="__codelineno-79-2" href="#__codelineno-79-2"></a><span class="n">b2</span> <span class="o">=</span> <span class="n">a2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</span><span id="__span-79-3"><a id="__codelineno-79-3" name="__codelineno-79-3" href="#__codelineno-79-3"></a><span class="n">b2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span id="__span-79-4"><a id="__codelineno-79-4" name="__codelineno-79-4" href="#__codelineno-79-4"></a><span class="nb">print</span><span class="p">(</span><span class="n">a2</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Expected: [0, 0.5, 0.5, 0]</span>
</span></code></pre></div>
<p>When there are <strong>multiple maximum values</strong>, we need to <strong>split the gradient equally</strong> among them. We handle the backward pass by identifying the position of the max value using a mask and distributing the incoming gradient across the max values.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-80-1"><a id="__codelineno-80-1" name="__codelineno-80-1" href="#__codelineno-80-1"></a><span class="c1"># Normalize by the number of max occurrences</span>
</span><span id="__span-80-2"><a id="__codelineno-80-2" name="__codelineno-80-2" href="#__codelineno-80-2"></a><span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</span><span id="__span-80-3"><a id="__codelineno-80-3" name="__codelineno-80-3" href="#__codelineno-80-3"></a><span class="c1"># Split the gradient equally</span>
</span><span id="__span-80-4"><a id="__codelineno-80-4" name="__codelineno-80-4" href="#__codelineno-80-4"></a><span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Case 3: Max Along a Specific Axis like <code>axis=0</code></strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-81-1"><a id="__codelineno-81-1" name="__codelineno-81-1" href="#__codelineno-81-1"></a><span class="n">a3</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
</span><span id="__span-81-2"><a id="__codelineno-81-2" name="__codelineno-81-2" href="#__codelineno-81-2"></a>             <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-81-3"><a id="__codelineno-81-3" name="__codelineno-81-3" href="#__codelineno-81-3"></a><span class="n">b3</span> <span class="o">=</span> <span class="n">a3</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Should be [5, 5]</span>
</span><span id="__span-81-4"><a id="__codelineno-81-4" name="__codelineno-81-4" href="#__codelineno-81-4"></a><span class="n">b3</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
</span><span id="__span-81-5"><a id="__codelineno-81-5" name="__codelineno-81-5" href="#__codelineno-81-5"></a><span class="nb">print</span><span class="p">(</span><span class="n">a3</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Expected: [[0, 1], [1, 0]]</span>
</span></code></pre></div>
<p>Compute the maximum along a specified axis and expand the gradient correctly along that axis during backpropagation.</p>
<p>We update the <code>_bkwd</code> function:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-82-1"><a id="__codelineno-82-1" name="__codelineno-82-1" href="#__codelineno-82-1"></a><span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-82-2"><a id="__codelineno-82-2" name="__codelineno-82-2" href="#__codelineno-82-2"></a>    <span class="c1"># For flattened tensor max, just count total max elements</span>
</span><span id="__span-82-3"><a id="__codelineno-82-3" name="__codelineno-82-3" href="#__codelineno-82-3"></a>    <span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</span><span id="__span-82-4"><a id="__codelineno-82-4" name="__codelineno-82-4" href="#__codelineno-82-4"></a>    <span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span>
</span><span id="__span-82-5"><a id="__codelineno-82-5" name="__codelineno-82-5" href="#__codelineno-82-5"></a>
</span><span id="__span-82-6"><a id="__codelineno-82-6" name="__codelineno-82-6" href="#__codelineno-82-6"></a><span class="c1"># Count max occurrences</span>
</span><span id="__span-82-7"><a id="__codelineno-82-7" name="__codelineno-82-7" href="#__codelineno-82-7"></a><span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-82-8"><a id="__codelineno-82-8" name="__codelineno-82-8" href="#__codelineno-82-8"></a><span class="n">grad_expanded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span><span id="__span-82-9"><a id="__codelineno-82-9" name="__codelineno-82-9" href="#__codelineno-82-9"></a>
</span><span id="__span-82-10"><a id="__codelineno-82-10" name="__codelineno-82-10" href="#__codelineno-82-10"></a><span class="c1"># Normalize and Apply gradient</span>
</span><span id="__span-82-11"><a id="__codelineno-82-11" name="__codelineno-82-11" href="#__codelineno-82-11"></a><span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_expanded</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Case 4: Handling <code>keepdims=True</code></strong></p>
<p>We use <code>keepdims</code> to ensure the gradient shape aligns with the input tensor during backpropagation. </p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-83-1"><a id="__codelineno-83-1" name="__codelineno-83-1" href="#__codelineno-83-1"></a><span class="n">a4</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
</span><span id="__span-83-2"><a id="__codelineno-83-2" name="__codelineno-83-2" href="#__codelineno-83-2"></a>             <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-83-3"><a id="__codelineno-83-3" name="__codelineno-83-3" href="#__codelineno-83-3"></a><span class="n">b4</span> <span class="o">=</span> <span class="n">a4</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-83-4"><a id="__codelineno-83-4" name="__codelineno-83-4" href="#__codelineno-83-4"></a><span class="n">b4</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">b4</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</span><span id="__span-83-5"><a id="__codelineno-83-5" name="__codelineno-83-5" href="#__codelineno-83-5"></a>
</span><span id="__span-83-6"><a id="__codelineno-83-6" name="__codelineno-83-6" href="#__codelineno-83-6"></a><span class="nb">print</span><span class="p">(</span><span class="n">a4</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Expected: [[0, 1], [1, 0]]</span>
</span></code></pre></div>
<p>If <code>keepdims=True</code> the output already has the correct shape, so no expansion is needed. This preserves the dimensions and ensures the gradient is applied correctly. If <code>keepdims=False</code> the output is reduced along the specified axis, so we must expand the gradient to match the original shape for proper broadcasting during multiplication.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-84-1"><a id="__codelineno-84-1" name="__codelineno-84-1" href="#__codelineno-84-1"></a><span class="n">grad_expanded</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span></code></pre></div>
<p>Or we can create a short version:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-85-1"><a id="__codelineno-85-1" name="__codelineno-85-1" href="#__codelineno-85-1"></a><span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> \
</span><span id="__span-85-2"><a id="__codelineno-85-2" name="__codelineno-85-2" href="#__codelineno-85-2"></a>    <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-85-3"><a id="__codelineno-85-3" name="__codelineno-85-3" href="#__codelineno-85-3"></a>
</span><span id="__span-85-4"><a id="__codelineno-85-4" name="__codelineno-85-4" href="#__codelineno-85-4"></a><span class="n">grad_expanded</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">or</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> \
</span><span id="__span-85-5"><a id="__codelineno-85-5" name="__codelineno-85-5" href="#__codelineno-85-5"></a>    <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span><span id="__span-85-6"><a id="__codelineno-85-6" name="__codelineno-85-6" href="#__codelineno-85-6"></a>
</span><span id="__span-85-7"><a id="__codelineno-85-7" name="__codelineno-85-7" href="#__codelineno-85-7"></a><span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_expanded</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Full implementation:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-86-1"><a id="__codelineno-86-1" name="__codelineno-86-1" href="#__codelineno-86-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-86-2"><a id="__codelineno-86-2" name="__codelineno-86-2" href="#__codelineno-86-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-86-3"><a id="__codelineno-86-3" name="__codelineno-86-3" href="#__codelineno-86-3"></a><span class="sd">    Computes the maximum value along the specified axis.</span>
</span><span id="__span-86-4"><a id="__codelineno-86-4" name="__codelineno-86-4" href="#__codelineno-86-4"></a>
</span><span id="__span-86-5"><a id="__codelineno-86-5" name="__codelineno-86-5" href="#__codelineno-86-5"></a><span class="sd">    This function returns the maximum value(s) of the tensor, either element-wise (if no axis is specified) </span>
</span><span id="__span-86-6"><a id="__codelineno-86-6" name="__codelineno-86-6" href="#__codelineno-86-6"></a><span class="sd">    or along a given axis. The backward pass ensures that only the maximum elements receive gradients.</span>
</span><span id="__span-86-7"><a id="__codelineno-86-7" name="__codelineno-86-7" href="#__codelineno-86-7"></a>
</span><span id="__span-86-8"><a id="__codelineno-86-8" name="__codelineno-86-8" href="#__codelineno-86-8"></a><span class="sd">    Args:</span>
</span><span id="__span-86-9"><a id="__codelineno-86-9" name="__codelineno-86-9" href="#__codelineno-86-9"></a><span class="sd">        axis (Optional[Union[int, Tuple[int]]]): The axis along which to compute the maximum.</span>
</span><span id="__span-86-10"><a id="__codelineno-86-10" name="__codelineno-86-10" href="#__codelineno-86-10"></a><span class="sd">            If None, the maximum of the entire tensor is returned.</span>
</span><span id="__span-86-11"><a id="__codelineno-86-11" name="__codelineno-86-11" href="#__codelineno-86-11"></a><span class="sd">        keepdims (bool): If True, retains reduced dimensions with size 1.</span>
</span><span id="__span-86-12"><a id="__codelineno-86-12" name="__codelineno-86-12" href="#__codelineno-86-12"></a>
</span><span id="__span-86-13"><a id="__codelineno-86-13" name="__codelineno-86-13" href="#__codelineno-86-13"></a><span class="sd">    Returns:</span>
</span><span id="__span-86-14"><a id="__codelineno-86-14" name="__codelineno-86-14" href="#__codelineno-86-14"></a><span class="sd">        Tensor: A new tensor containing the maximum values along the given axis.</span>
</span><span id="__span-86-15"><a id="__codelineno-86-15" name="__codelineno-86-15" href="#__codelineno-86-15"></a>
</span><span id="__span-86-16"><a id="__codelineno-86-16" name="__codelineno-86-16" href="#__codelineno-86-16"></a><span class="sd">    The gradient is computed during backpropagation by assigning a gradient of 1 </span>
</span><span id="__span-86-17"><a id="__codelineno-86-17" name="__codelineno-86-17" href="#__codelineno-86-17"></a><span class="sd">    to the maximum element(s) and 0 elsewhere.</span>
</span><span id="__span-86-18"><a id="__codelineno-86-18" name="__codelineno-86-18" href="#__codelineno-86-18"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-86-19"><a id="__codelineno-86-19" name="__codelineno-86-19" href="#__codelineno-86-19"></a>
</span><span id="__span-86-20"><a id="__codelineno-86-20" name="__codelineno-86-20" href="#__codelineno-86-20"></a>    <span class="c1"># Calculate the maximum values</span>
</span><span id="__span-86-21"><a id="__codelineno-86-21" name="__codelineno-86-21" href="#__codelineno-86-21"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
</span><span id="__span-86-22"><a id="__codelineno-86-22" name="__codelineno-86-22" href="#__codelineno-86-22"></a>
</span><span id="__span-86-23"><a id="__codelineno-86-23" name="__codelineno-86-23" href="#__codelineno-86-23"></a>    <span class="n">dependencies</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-86-24"><a id="__codelineno-86-24" name="__codelineno-86-24" href="#__codelineno-86-24"></a>
</span><span id="__span-86-25"><a id="__codelineno-86-25" name="__codelineno-86-25" href="#__codelineno-86-25"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-86-26"><a id="__codelineno-86-26" name="__codelineno-86-26" href="#__codelineno-86-26"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-86-27"><a id="__codelineno-86-27" name="__codelineno-86-27" href="#__codelineno-86-27"></a>            <span class="c1"># Handle multi-dimensional case</span>
</span><span id="__span-86-28"><a id="__codelineno-86-28" name="__codelineno-86-28" href="#__codelineno-86-28"></a>            <span class="n">output_expanded</span> <span class="o">=</span> <span class="n">output</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">or</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span><span id="__span-86-29"><a id="__codelineno-86-29" name="__codelineno-86-29" href="#__codelineno-86-29"></a>            <span class="c1"># Create a mask where only the max values are True</span>
</span><span id="__span-86-30"><a id="__codelineno-86-30" name="__codelineno-86-30" href="#__codelineno-86-30"></a>            <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="n">output_expanded</span><span class="p">)</span>
</span><span id="__span-86-31"><a id="__codelineno-86-31" name="__codelineno-86-31" href="#__codelineno-86-31"></a>
</span><span id="__span-86-32"><a id="__codelineno-86-32" name="__codelineno-86-32" href="#__codelineno-86-32"></a>            <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-86-33"><a id="__codelineno-86-33" name="__codelineno-86-33" href="#__codelineno-86-33"></a>                <span class="c1"># For flattened tensor max, just count total max elements</span>
</span><span id="__span-86-34"><a id="__codelineno-86-34" name="__codelineno-86-34" href="#__codelineno-86-34"></a>                <span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
</span><span id="__span-86-35"><a id="__codelineno-86-35" name="__codelineno-86-35" href="#__codelineno-86-35"></a>                <span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span>
</span><span id="__span-86-36"><a id="__codelineno-86-36" name="__codelineno-86-36" href="#__codelineno-86-36"></a>
</span><span id="__span-86-37"><a id="__codelineno-86-37" name="__codelineno-86-37" href="#__codelineno-86-37"></a>            <span class="c1"># Count max occurrences</span>
</span><span id="__span-86-38"><a id="__codelineno-86-38" name="__codelineno-86-38" href="#__codelineno-86-38"></a>            <span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-86-39"><a id="__codelineno-86-39" name="__codelineno-86-39" href="#__codelineno-86-39"></a>            <span class="n">grad_expanded</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span><span id="__span-86-40"><a id="__codelineno-86-40" name="__codelineno-86-40" href="#__codelineno-86-40"></a>
</span><span id="__span-86-41"><a id="__codelineno-86-41" name="__codelineno-86-41" href="#__codelineno-86-41"></a>            <span class="c1"># Normalize and Apply gradient</span>
</span><span id="__span-86-42"><a id="__codelineno-86-42" name="__codelineno-86-42" href="#__codelineno-86-42"></a>            <span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_expanded</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span>
</span><span id="__span-86-43"><a id="__codelineno-86-43" name="__codelineno-86-43" href="#__codelineno-86-43"></a>
</span><span id="__span-86-44"><a id="__codelineno-86-44" name="__codelineno-86-44" href="#__codelineno-86-44"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span><span class="p">))</span>
</span><span id="__span-86-45"><a id="__codelineno-86-45" name="__codelineno-86-45" href="#__codelineno-86-45"></a>
</span><span id="__span-86-46"><a id="__codelineno-86-46" name="__codelineno-86-46" href="#__codelineno-86-46"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Example:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-87-1"><a id="__codelineno-87-1" name="__codelineno-87-1" href="#__codelineno-87-1"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============ Testing max backward ============&quot;</span><span class="p">)</span>
</span><span id="__span-87-2"><a id="__codelineno-87-2" name="__codelineno-87-2" href="#__codelineno-87-2"></a>
</span><span id="__span-87-3"><a id="__codelineno-87-3" name="__codelineno-87-3" href="#__codelineno-87-3"></a><span class="c1"># Test 1: Simple case - max with no axis (scalar result)</span>
</span><span id="__span-87-4"><a id="__codelineno-87-4" name="__codelineno-87-4" href="#__codelineno-87-4"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test 1: max with no axis (scalar output)&quot;</span><span class="p">)</span>
</span><span id="__span-87-5"><a id="__codelineno-87-5" name="__codelineno-87-5" href="#__codelineno-87-5"></a><span class="n">a1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-87-6"><a id="__codelineno-87-6" name="__codelineno-87-6" href="#__codelineno-87-6"></a><span class="n">b1</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>  <span class="c1"># Should be 4</span>
</span><span id="__span-87-7"><a id="__codelineno-87-7" name="__codelineno-87-7" href="#__codelineno-87-7"></a><span class="n">b1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span id="__span-87-8"><a id="__codelineno-87-8" name="__codelineno-87-8" href="#__codelineno-87-8"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor: </span><span class="si">{</span><span class="n">a1</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-9"><a id="__codelineno-87-9" name="__codelineno-87-9" href="#__codelineno-87-9"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max value: </span><span class="si">{</span><span class="n">b1</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-10"><a id="__codelineno-87-10" name="__codelineno-87-10" href="#__codelineno-87-10"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient: </span><span class="si">{</span><span class="n">a1</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-11"><a id="__codelineno-87-11" name="__codelineno-87-11" href="#__codelineno-87-11"></a><span class="c1"># Expected: Only the position with max value (4) should have gradient 1, others 0</span>
</span><span id="__span-87-12"><a id="__codelineno-87-12" name="__codelineno-87-12" href="#__codelineno-87-12"></a><span class="n">expected_grad1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span><span id="__span-87-13"><a id="__codelineno-87-13" name="__codelineno-87-13" href="#__codelineno-87-13"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test passed: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a1</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">expected_grad1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-14"><a id="__codelineno-87-14" name="__codelineno-87-14" href="#__codelineno-87-14"></a>
</span><span id="__span-87-15"><a id="__codelineno-87-15" name="__codelineno-87-15" href="#__codelineno-87-15"></a><span class="c1"># Test 2: Max with duplicate maximum values</span>
</span><span id="__span-87-16"><a id="__codelineno-87-16" name="__codelineno-87-16" href="#__codelineno-87-16"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test 2: max with duplicate maximums&quot;</span><span class="p">)</span>
</span><span id="__span-87-17"><a id="__codelineno-87-17" name="__codelineno-87-17" href="#__codelineno-87-17"></a><span class="n">a2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-87-18"><a id="__codelineno-87-18" name="__codelineno-87-18" href="#__codelineno-87-18"></a><span class="n">b2</span> <span class="o">=</span> <span class="n">a2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</span><span id="__span-87-19"><a id="__codelineno-87-19" name="__codelineno-87-19" href="#__codelineno-87-19"></a><span class="n">b2</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span id="__span-87-20"><a id="__codelineno-87-20" name="__codelineno-87-20" href="#__codelineno-87-20"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor: </span><span class="si">{</span><span class="n">a2</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-21"><a id="__codelineno-87-21" name="__codelineno-87-21" href="#__codelineno-87-21"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max value: </span><span class="si">{</span><span class="n">b2</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-22"><a id="__codelineno-87-22" name="__codelineno-87-22" href="#__codelineno-87-22"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient: </span><span class="si">{</span><span class="n">a2</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-23"><a id="__codelineno-87-23" name="__codelineno-87-23" href="#__codelineno-87-23"></a><span class="c1"># Expected: Positions with max value (4) should have gradient 0.5 each (1/count)</span>
</span><span id="__span-87-24"><a id="__codelineno-87-24" name="__codelineno-87-24" href="#__codelineno-87-24"></a><span class="n">expected_grad2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</span><span id="__span-87-25"><a id="__codelineno-87-25" name="__codelineno-87-25" href="#__codelineno-87-25"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test passed: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a2</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">expected_grad2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-26"><a id="__codelineno-87-26" name="__codelineno-87-26" href="#__codelineno-87-26"></a>
</span><span id="__span-87-27"><a id="__codelineno-87-27" name="__codelineno-87-27" href="#__codelineno-87-27"></a><span class="c1"># Test 3: Max along a specific axis</span>
</span><span id="__span-87-28"><a id="__codelineno-87-28" name="__codelineno-87-28" href="#__codelineno-87-28"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test 3: max along axis=0&quot;</span><span class="p">)</span>
</span><span id="__span-87-29"><a id="__codelineno-87-29" name="__codelineno-87-29" href="#__codelineno-87-29"></a><span class="n">a3</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
</span><span id="__span-87-30"><a id="__codelineno-87-30" name="__codelineno-87-30" href="#__codelineno-87-30"></a>                <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-87-31"><a id="__codelineno-87-31" name="__codelineno-87-31" href="#__codelineno-87-31"></a><span class="n">b3</span> <span class="o">=</span> <span class="n">a3</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Should be [5, 5]</span>
</span><span id="__span-87-32"><a id="__codelineno-87-32" name="__codelineno-87-32" href="#__codelineno-87-32"></a><span class="n">b3</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
</span><span id="__span-87-33"><a id="__codelineno-87-33" name="__codelineno-87-33" href="#__codelineno-87-33"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">a3</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-34"><a id="__codelineno-87-34" name="__codelineno-87-34" href="#__codelineno-87-34"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max value: </span><span class="si">{</span><span class="n">b3</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-35"><a id="__codelineno-87-35" name="__codelineno-87-35" href="#__codelineno-87-35"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient:</span><span class="se">\n</span><span class="si">{</span><span class="n">a3</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-36"><a id="__codelineno-87-36" name="__codelineno-87-36" href="#__codelineno-87-36"></a><span class="c1"># Expected: First column: [0, 1], Second column: [1, 0]</span>
</span><span id="__span-87-37"><a id="__codelineno-87-37" name="__codelineno-87-37" href="#__codelineno-87-37"></a><span class="n">expected_grad3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</span><span id="__span-87-38"><a id="__codelineno-87-38" name="__codelineno-87-38" href="#__codelineno-87-38"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test passed: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a3</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">expected_grad3</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-39"><a id="__codelineno-87-39" name="__codelineno-87-39" href="#__codelineno-87-39"></a>
</span><span id="__span-87-40"><a id="__codelineno-87-40" name="__codelineno-87-40" href="#__codelineno-87-40"></a><span class="c1"># Test 4: Max along axis with keepdims=True</span>
</span><span id="__span-87-41"><a id="__codelineno-87-41" name="__codelineno-87-41" href="#__codelineno-87-41"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test 4: max along axis=0, keepdims=True&quot;</span><span class="p">)</span>
</span><span id="__span-87-42"><a id="__codelineno-87-42" name="__codelineno-87-42" href="#__codelineno-87-42"></a><span class="n">a4</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
</span><span id="__span-87-43"><a id="__codelineno-87-43" name="__codelineno-87-43" href="#__codelineno-87-43"></a>                <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-87-44"><a id="__codelineno-87-44" name="__codelineno-87-44" href="#__codelineno-87-44"></a><span class="n">b4</span> <span class="o">=</span> <span class="n">a4</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-87-45"><a id="__codelineno-87-45" name="__codelineno-87-45" href="#__codelineno-87-45"></a><span class="n">b4</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">b4</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</span><span id="__span-87-46"><a id="__codelineno-87-46" name="__codelineno-87-46" href="#__codelineno-87-46"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">a4</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-47"><a id="__codelineno-87-47" name="__codelineno-87-47" href="#__codelineno-87-47"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max value:</span><span class="se">\n</span><span class="si">{</span><span class="n">b4</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-48"><a id="__codelineno-87-48" name="__codelineno-87-48" href="#__codelineno-87-48"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient:</span><span class="se">\n</span><span class="si">{</span><span class="n">a4</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-49"><a id="__codelineno-87-49" name="__codelineno-87-49" href="#__codelineno-87-49"></a><span class="n">expected_grad4</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</span><span id="__span-87-50"><a id="__codelineno-87-50" name="__codelineno-87-50" href="#__codelineno-87-50"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test passed: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a4</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">expected_grad4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-51"><a id="__codelineno-87-51" name="__codelineno-87-51" href="#__codelineno-87-51"></a>
</span><span id="__span-87-52"><a id="__codelineno-87-52" name="__codelineno-87-52" href="#__codelineno-87-52"></a><span class="c1"># Test 5: Max along axis with duplicate maximums</span>
</span><span id="__span-87-53"><a id="__codelineno-87-53" name="__codelineno-87-53" href="#__codelineno-87-53"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test 5: max along axis with duplicate maximums&quot;</span><span class="p">)</span>
</span><span id="__span-87-54"><a id="__codelineno-87-54" name="__codelineno-87-54" href="#__codelineno-87-54"></a><span class="n">a5</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
</span><span id="__span-87-55"><a id="__codelineno-87-55" name="__codelineno-87-55" href="#__codelineno-87-55"></a>                <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-87-56"><a id="__codelineno-87-56" name="__codelineno-87-56" href="#__codelineno-87-56"></a><span class="n">b5</span> <span class="o">=</span> <span class="n">a5</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Should be [5, 4]</span>
</span><span id="__span-87-57"><a id="__codelineno-87-57" name="__codelineno-87-57" href="#__codelineno-87-57"></a><span class="n">b5</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
</span><span id="__span-87-58"><a id="__codelineno-87-58" name="__codelineno-87-58" href="#__codelineno-87-58"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">a5</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-59"><a id="__codelineno-87-59" name="__codelineno-87-59" href="#__codelineno-87-59"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max value: </span><span class="si">{</span><span class="n">b5</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-60"><a id="__codelineno-87-60" name="__codelineno-87-60" href="#__codelineno-87-60"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient:</span><span class="se">\n</span><span class="si">{</span><span class="n">a5</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-61"><a id="__codelineno-87-61" name="__codelineno-87-61" href="#__codelineno-87-61"></a><span class="c1"># Expected: First column has two 5s, so grad = [0.5, 0.5], Second column: [0, 1]</span>
</span><span id="__span-87-62"><a id="__codelineno-87-62" name="__codelineno-87-62" href="#__codelineno-87-62"></a><span class="n">expected_grad5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
</span><span id="__span-87-63"><a id="__codelineno-87-63" name="__codelineno-87-63" href="#__codelineno-87-63"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test passed: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a5</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">expected_grad5</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-64"><a id="__codelineno-87-64" name="__codelineno-87-64" href="#__codelineno-87-64"></a>
</span><span id="__span-87-65"><a id="__codelineno-87-65" name="__codelineno-87-65" href="#__codelineno-87-65"></a><span class="c1"># Test 6: Max over multiple dimensions</span>
</span><span id="__span-87-66"><a id="__codelineno-87-66" name="__codelineno-87-66" href="#__codelineno-87-66"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test 6: max over multiple dimensions&quot;</span><span class="p">)</span>
</span><span id="__span-87-67"><a id="__codelineno-87-67" name="__codelineno-87-67" href="#__codelineno-87-67"></a><span class="n">a6</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
</span><span id="__span-87-68"><a id="__codelineno-87-68" name="__codelineno-87-68" href="#__codelineno-87-68"></a>                <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-87-69"><a id="__codelineno-87-69" name="__codelineno-87-69" href="#__codelineno-87-69"></a><span class="n">b6</span> <span class="o">=</span> <span class="n">a6</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>  <span class="c1"># Should be 8</span>
</span><span id="__span-87-70"><a id="__codelineno-87-70" name="__codelineno-87-70" href="#__codelineno-87-70"></a><span class="n">b6</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span id="__span-87-71"><a id="__codelineno-87-71" name="__codelineno-87-71" href="#__codelineno-87-71"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor shape: </span><span class="si">{</span><span class="n">a6</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-72"><a id="__codelineno-87-72" name="__codelineno-87-72" href="#__codelineno-87-72"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max value: </span><span class="si">{</span><span class="n">b6</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-73"><a id="__codelineno-87-73" name="__codelineno-87-73" href="#__codelineno-87-73"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient at max position: </span><span class="si">{</span><span class="n">a6</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-74"><a id="__codelineno-87-74" name="__codelineno-87-74" href="#__codelineno-87-74"></a><span class="c1"># Only position of max value (8) should have gradient = 1</span>
</span><span id="__span-87-75"><a id="__codelineno-87-75" name="__codelineno-87-75" href="#__codelineno-87-75"></a><span class="n">expected_position</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># In 0-indexed, (1,1,1) is the last element</span>
</span><span id="__span-87-76"><a id="__codelineno-87-76" name="__codelineno-87-76" href="#__codelineno-87-76"></a><span class="n">expected_grad6</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span id="__span-87-77"><a id="__codelineno-87-77" name="__codelineno-87-77" href="#__codelineno-87-77"></a><span class="n">expected_grad6</span><span class="p">[</span><span class="n">expected_position</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span id="__span-87-78"><a id="__codelineno-87-78" name="__codelineno-87-78" href="#__codelineno-87-78"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test passed: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a6</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">expected_grad6</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-79"><a id="__codelineno-87-79" name="__codelineno-87-79" href="#__codelineno-87-79"></a>
</span><span id="__span-87-80"><a id="__codelineno-87-80" name="__codelineno-87-80" href="#__codelineno-87-80"></a><span class="c1"># Test 7: Another axis test with 3D tensor</span>
</span><span id="__span-87-81"><a id="__codelineno-87-81" name="__codelineno-87-81" href="#__codelineno-87-81"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test 7: max on axis in 3D tensor&quot;</span><span class="p">)</span>
</span><span id="__span-87-82"><a id="__codelineno-87-82" name="__codelineno-87-82" href="#__codelineno-87-82"></a><span class="n">a7</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
</span><span id="__span-87-83"><a id="__codelineno-87-83" name="__codelineno-87-83" href="#__codelineno-87-83"></a>                <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-87-84"><a id="__codelineno-87-84" name="__codelineno-87-84" href="#__codelineno-87-84"></a><span class="n">b7</span> <span class="o">=</span> <span class="n">a7</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Max along middle dimension</span>
</span><span id="__span-87-85"><a id="__codelineno-87-85" name="__codelineno-87-85" href="#__codelineno-87-85"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor shape: </span><span class="si">{</span><span class="n">a7</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-86"><a id="__codelineno-87-86" name="__codelineno-87-86" href="#__codelineno-87-86"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">b7</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-87"><a id="__codelineno-87-87" name="__codelineno-87-87" href="#__codelineno-87-87"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output data: </span><span class="se">\n</span><span class="si">{</span><span class="n">b7</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-88"><a id="__codelineno-87-88" name="__codelineno-87-88" href="#__codelineno-87-88"></a><span class="n">b7</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">b7</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
</span><span id="__span-87-89"><a id="__codelineno-87-89" name="__codelineno-87-89" href="#__codelineno-87-89"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient: </span><span class="se">\n</span><span class="si">{</span><span class="n">a7</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-90"><a id="__codelineno-87-90" name="__codelineno-87-90" href="#__codelineno-87-90"></a><span class="n">expected_grad7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span>
</span><span id="__span-87-91"><a id="__codelineno-87-91" name="__codelineno-87-91" href="#__codelineno-87-91"></a>                            <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]])</span>
</span><span id="__span-87-92"><a id="__codelineno-87-92" name="__codelineno-87-92" href="#__codelineno-87-92"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test passed: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">a7</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="w"> </span><span class="n">expected_grad7</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-87-93"><a id="__codelineno-87-93" name="__codelineno-87-93" href="#__codelineno-87-93"></a>
</span><span id="__span-87-94"><a id="__codelineno-87-94" name="__codelineno-87-94" href="#__codelineno-87-94"></a><span class="c1"># Return overall test result</span>
</span><span id="__span-87-95"><a id="__codelineno-87-95" name="__codelineno-87-95" href="#__codelineno-87-95"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">============ Summary ============&quot;</span><span class="p">)</span>
</span><span id="__span-87-96"><a id="__codelineno-87-96" name="__codelineno-87-96" href="#__codelineno-87-96"></a><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All tests should pass if each element of the gradient is correct.&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="min"><code>min</code><a class="headerlink" href="#min" title="Permanent link">&para;</a></h3>
<iframe width="1233" height="694" src="https://www.youtube.com/embed/kuElg7VtSII?list=PLWUV973D6J8imrTO4yJk3aI0NKJZgzFeG" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<p>The <strong>min operation</strong> returns the minimum value of a tensor along a specified axis. If no axis is specified, it returns the minimum value from the entire tensor.</p>
<p>For differentiation, the gradient of the minimum function is defined as:</p>
<div class="arithmatex">\[\frac{d}{dx} \min(X) = \begin{cases} 1, &amp; \text{if } x \text{ is the minimum value} \\ 0, &amp; \text{otherwise} \end{cases}\]</div>
<p>The gradient is the same as for the <code>max</code> operation, we can create a <code>bkwd</code> method, that can serve for both methods:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-88-1"><a id="__codelineno-88-1" name="__codelineno-88-1" href="#__codelineno-88-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">bkwd_minmax</span><span class="p">(</span>
</span><span id="__span-88-2"><a id="__codelineno-88-2" name="__codelineno-88-2" href="#__codelineno-88-2"></a>    <span class="bp">self</span><span class="p">,</span>
</span><span id="__span-88-3"><a id="__codelineno-88-3" name="__codelineno-88-3" href="#__codelineno-88-3"></a>    <span class="n">output</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
</span><span id="__span-88-4"><a id="__codelineno-88-4" name="__codelineno-88-4" href="#__codelineno-88-4"></a>    <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="__span-88-5"><a id="__codelineno-88-5" name="__codelineno-88-5" href="#__codelineno-88-5"></a>    <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
</span><span id="__span-88-6"><a id="__codelineno-88-6" name="__codelineno-88-6" href="#__codelineno-88-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-88-7"><a id="__codelineno-88-7" name="__codelineno-88-7" href="#__codelineno-88-7"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-88-8"><a id="__codelineno-88-8" name="__codelineno-88-8" href="#__codelineno-88-8"></a>        <span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> \
</span><span id="__span-88-9"><a id="__codelineno-88-9" name="__codelineno-88-9" href="#__codelineno-88-9"></a>            <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-88-10"><a id="__codelineno-88-10" name="__codelineno-88-10" href="#__codelineno-88-10"></a>
</span><span id="__span-88-11"><a id="__codelineno-88-11" name="__codelineno-88-11" href="#__codelineno-88-11"></a>        <span class="n">grad_expanded</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">or</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> \
</span><span id="__span-88-12"><a id="__codelineno-88-12" name="__codelineno-88-12" href="#__codelineno-88-12"></a>            <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span><span id="__span-88-13"><a id="__codelineno-88-13" name="__codelineno-88-13" href="#__codelineno-88-13"></a>
</span><span id="__span-88-14"><a id="__codelineno-88-14" name="__codelineno-88-14" href="#__codelineno-88-14"></a>        <span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_expanded</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span>
</span><span id="__span-88-15"><a id="__codelineno-88-15" name="__codelineno-88-15" href="#__codelineno-88-15"></a>
</span><span id="__span-88-16"><a id="__codelineno-88-16" name="__codelineno-88-16" href="#__codelineno-88-16"></a>    <span class="k">return</span> <span class="n">_bkwd</span>
</span></code></pre></div>
<p>Then we can make refactoring for the <code>max</code>:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-89-1"><a id="__codelineno-89-1" name="__codelineno-89-1" href="#__codelineno-89-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-89-2"><a id="__codelineno-89-2" name="__codelineno-89-2" href="#__codelineno-89-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-89-3"><a id="__codelineno-89-3" name="__codelineno-89-3" href="#__codelineno-89-3"></a><span class="sd">    Computes the maximum value along the specified axis.</span>
</span><span id="__span-89-4"><a id="__codelineno-89-4" name="__codelineno-89-4" href="#__codelineno-89-4"></a>
</span><span id="__span-89-5"><a id="__codelineno-89-5" name="__codelineno-89-5" href="#__codelineno-89-5"></a><span class="sd">    This function returns the maximum value(s) of the tensor, either element-wise (if no axis is specified) </span>
</span><span id="__span-89-6"><a id="__codelineno-89-6" name="__codelineno-89-6" href="#__codelineno-89-6"></a><span class="sd">    or along a given axis. The backward pass ensures that only the maximum elements receive gradients.</span>
</span><span id="__span-89-7"><a id="__codelineno-89-7" name="__codelineno-89-7" href="#__codelineno-89-7"></a>
</span><span id="__span-89-8"><a id="__codelineno-89-8" name="__codelineno-89-8" href="#__codelineno-89-8"></a><span class="sd">    Args:</span>
</span><span id="__span-89-9"><a id="__codelineno-89-9" name="__codelineno-89-9" href="#__codelineno-89-9"></a><span class="sd">        axis (Optional[Union[int, Tuple[int]]]): The axis along which to compute the maximum.</span>
</span><span id="__span-89-10"><a id="__codelineno-89-10" name="__codelineno-89-10" href="#__codelineno-89-10"></a><span class="sd">            If None, the maximum of the entire tensor is returned.</span>
</span><span id="__span-89-11"><a id="__codelineno-89-11" name="__codelineno-89-11" href="#__codelineno-89-11"></a><span class="sd">        keepdims (bool): If True, retains reduced dimensions with size 1.</span>
</span><span id="__span-89-12"><a id="__codelineno-89-12" name="__codelineno-89-12" href="#__codelineno-89-12"></a>
</span><span id="__span-89-13"><a id="__codelineno-89-13" name="__codelineno-89-13" href="#__codelineno-89-13"></a><span class="sd">    Returns:</span>
</span><span id="__span-89-14"><a id="__codelineno-89-14" name="__codelineno-89-14" href="#__codelineno-89-14"></a><span class="sd">        Tensor: A new tensor containing the maximum values along the given axis.</span>
</span><span id="__span-89-15"><a id="__codelineno-89-15" name="__codelineno-89-15" href="#__codelineno-89-15"></a>
</span><span id="__span-89-16"><a id="__codelineno-89-16" name="__codelineno-89-16" href="#__codelineno-89-16"></a><span class="sd">    The gradient is computed during backpropagation by assigning a gradient of 1 </span>
</span><span id="__span-89-17"><a id="__codelineno-89-17" name="__codelineno-89-17" href="#__codelineno-89-17"></a><span class="sd">    to the maximum element(s) and 0 elsewhere.</span>
</span><span id="__span-89-18"><a id="__codelineno-89-18" name="__codelineno-89-18" href="#__codelineno-89-18"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-89-19"><a id="__codelineno-89-19" name="__codelineno-89-19" href="#__codelineno-89-19"></a>
</span><span id="__span-89-20"><a id="__codelineno-89-20" name="__codelineno-89-20" href="#__codelineno-89-20"></a>    <span class="c1"># Calculate the maximum values</span>
</span><span id="__span-89-21"><a id="__codelineno-89-21" name="__codelineno-89-21" href="#__codelineno-89-21"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
</span><span id="__span-89-22"><a id="__codelineno-89-22" name="__codelineno-89-22" href="#__codelineno-89-22"></a>
</span><span id="__span-89-23"><a id="__codelineno-89-23" name="__codelineno-89-23" href="#__codelineno-89-23"></a>    <span class="n">dependencies</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-89-24"><a id="__codelineno-89-24" name="__codelineno-89-24" href="#__codelineno-89-24"></a>
</span><span id="__span-89-25"><a id="__codelineno-89-25" name="__codelineno-89-25" href="#__codelineno-89-25"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-89-26"><a id="__codelineno-89-26" name="__codelineno-89-26" href="#__codelineno-89-26"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-89-27"><a id="__codelineno-89-27" name="__codelineno-89-27" href="#__codelineno-89-27"></a>            <span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bkwd_minmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">))</span>
</span><span id="__span-89-28"><a id="__codelineno-89-28" name="__codelineno-89-28" href="#__codelineno-89-28"></a>        <span class="p">)</span>
</span><span id="__span-89-29"><a id="__codelineno-89-29" name="__codelineno-89-29" href="#__codelineno-89-29"></a>
</span><span id="__span-89-30"><a id="__codelineno-89-30" name="__codelineno-89-30" href="#__codelineno-89-30"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>And now we can implement <code>min</code>, the only difference lay in the forward step, calculate the minimum values for the output:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-90-1"><a id="__codelineno-90-1" name="__codelineno-90-1" href="#__codelineno-90-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-90-2"><a id="__codelineno-90-2" name="__codelineno-90-2" href="#__codelineno-90-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-90-3"><a id="__codelineno-90-3" name="__codelineno-90-3" href="#__codelineno-90-3"></a><span class="sd">    Computes the minimum value along the specified axis.</span>
</span><span id="__span-90-4"><a id="__codelineno-90-4" name="__codelineno-90-4" href="#__codelineno-90-4"></a>
</span><span id="__span-90-5"><a id="__codelineno-90-5" name="__codelineno-90-5" href="#__codelineno-90-5"></a><span class="sd">    This function returns the minimum value(s) of the tensor, either element-wise (if no axis is specified) </span>
</span><span id="__span-90-6"><a id="__codelineno-90-6" name="__codelineno-90-6" href="#__codelineno-90-6"></a><span class="sd">    or along a given axis. The backward pass ensures that only the minimum elements receive gradients.</span>
</span><span id="__span-90-7"><a id="__codelineno-90-7" name="__codelineno-90-7" href="#__codelineno-90-7"></a>
</span><span id="__span-90-8"><a id="__codelineno-90-8" name="__codelineno-90-8" href="#__codelineno-90-8"></a><span class="sd">    Args:</span>
</span><span id="__span-90-9"><a id="__codelineno-90-9" name="__codelineno-90-9" href="#__codelineno-90-9"></a><span class="sd">        axis (Optional[int]): The axis along which to compute the minimum.</span>
</span><span id="__span-90-10"><a id="__codelineno-90-10" name="__codelineno-90-10" href="#__codelineno-90-10"></a><span class="sd">            If None, the minimum of the entire tensor is returned.</span>
</span><span id="__span-90-11"><a id="__codelineno-90-11" name="__codelineno-90-11" href="#__codelineno-90-11"></a><span class="sd">        keepdims (bool): If True, retains reduced dimensions with size 1.</span>
</span><span id="__span-90-12"><a id="__codelineno-90-12" name="__codelineno-90-12" href="#__codelineno-90-12"></a>
</span><span id="__span-90-13"><a id="__codelineno-90-13" name="__codelineno-90-13" href="#__codelineno-90-13"></a><span class="sd">    Returns:</span>
</span><span id="__span-90-14"><a id="__codelineno-90-14" name="__codelineno-90-14" href="#__codelineno-90-14"></a><span class="sd">        Tensor: A new tensor containing the minimum values along the given axis.</span>
</span><span id="__span-90-15"><a id="__codelineno-90-15" name="__codelineno-90-15" href="#__codelineno-90-15"></a>
</span><span id="__span-90-16"><a id="__codelineno-90-16" name="__codelineno-90-16" href="#__codelineno-90-16"></a><span class="sd">    The gradient is computed during backpropagation by assigning a gradient of 1 </span>
</span><span id="__span-90-17"><a id="__codelineno-90-17" name="__codelineno-90-17" href="#__codelineno-90-17"></a><span class="sd">    to the minimum element(s) and 0 elsewhere.</span>
</span><span id="__span-90-18"><a id="__codelineno-90-18" name="__codelineno-90-18" href="#__codelineno-90-18"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-90-19"><a id="__codelineno-90-19" name="__codelineno-90-19" href="#__codelineno-90-19"></a>
</span><span id="__span-90-20"><a id="__codelineno-90-20" name="__codelineno-90-20" href="#__codelineno-90-20"></a>    <span class="c1"># Calculate the minimum values</span>
</span><span id="__span-90-21"><a id="__codelineno-90-21" name="__codelineno-90-21" href="#__codelineno-90-21"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
</span><span id="__span-90-22"><a id="__codelineno-90-22" name="__codelineno-90-22" href="#__codelineno-90-22"></a>
</span><span id="__span-90-23"><a id="__codelineno-90-23" name="__codelineno-90-23" href="#__codelineno-90-23"></a>    <span class="n">dependencies</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-90-24"><a id="__codelineno-90-24" name="__codelineno-90-24" href="#__codelineno-90-24"></a>
</span><span id="__span-90-25"><a id="__codelineno-90-25" name="__codelineno-90-25" href="#__codelineno-90-25"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-90-26"><a id="__codelineno-90-26" name="__codelineno-90-26" href="#__codelineno-90-26"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-90-27"><a id="__codelineno-90-27" name="__codelineno-90-27" href="#__codelineno-90-27"></a>            <span class="n">Leaf</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bkwd_minmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">))</span>
</span><span id="__span-90-28"><a id="__codelineno-90-28" name="__codelineno-90-28" href="#__codelineno-90-28"></a>        <span class="p">)</span>
</span><span id="__span-90-29"><a id="__codelineno-90-29" name="__codelineno-90-29" href="#__codelineno-90-29"></a>
</span><span id="__span-90-30"><a id="__codelineno-90-30" name="__codelineno-90-30" href="#__codelineno-90-30"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p><strong>Example:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-91-1"><a id="__codelineno-91-1" name="__codelineno-91-1" href="#__codelineno-91-1"></a><span class="c1"># Create a tensor</span>
</span><span id="__span-91-2"><a id="__codelineno-91-2" name="__codelineno-91-2" href="#__codelineno-91-2"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-91-3"><a id="__codelineno-91-3" name="__codelineno-91-3" href="#__codelineno-91-3"></a>
</span><span id="__span-91-4"><a id="__codelineno-91-4" name="__codelineno-91-4" href="#__codelineno-91-4"></a><span class="c1"># Compute the minimum along axis 1</span>
</span><span id="__span-91-5"><a id="__codelineno-91-5" name="__codelineno-91-5" href="#__codelineno-91-5"></a><span class="n">result</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-91-6"><a id="__codelineno-91-6" name="__codelineno-91-6" href="#__codelineno-91-6"></a>
</span><span id="__span-91-7"><a id="__codelineno-91-7" name="__codelineno-91-7" href="#__codelineno-91-7"></a><span class="c1"># Print the result</span>
</span><span id="__span-91-8"><a id="__codelineno-91-8" name="__codelineno-91-8" href="#__codelineno-91-8"></a><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span><span id="__span-91-9"><a id="__codelineno-91-9" name="__codelineno-91-9" href="#__codelineno-91-9"></a><span class="c1"># Output:</span>
</span><span id="__span-91-10"><a id="__codelineno-91-10" name="__codelineno-91-10" href="#__codelineno-91-10"></a><span class="c1"># Tensor([[1],</span>
</span><span id="__span-91-11"><a id="__codelineno-91-11" name="__codelineno-91-11" href="#__codelineno-91-11"></a><span class="c1">#         [2]], requires_grad=True)</span>
</span><span id="__span-91-12"><a id="__codelineno-91-12" name="__codelineno-91-12" href="#__codelineno-91-12"></a>
</span><span id="__span-91-13"><a id="__codelineno-91-13" name="__codelineno-91-13" href="#__codelineno-91-13"></a><span class="c1"># Backward pass</span>
</span><span id="__span-91-14"><a id="__codelineno-91-14" name="__codelineno-91-14" href="#__codelineno-91-14"></a><span class="n">grad_output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">]]))</span>
</span><span id="__span-91-15"><a id="__codelineno-91-15" name="__codelineno-91-15" href="#__codelineno-91-15"></a><span class="n">result</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad_output</span><span class="p">)</span>
</span><span id="__span-91-16"><a id="__codelineno-91-16" name="__codelineno-91-16" href="#__codelineno-91-16"></a>
</span><span id="__span-91-17"><a id="__codelineno-91-17" name="__codelineno-91-17" href="#__codelineno-91-17"></a><span class="c1"># Check gradients</span>
</span><span id="__span-91-18"><a id="__codelineno-91-18" name="__codelineno-91-18" href="#__codelineno-91-18"></a><span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span><span id="__span-91-19"><a id="__codelineno-91-19" name="__codelineno-91-19" href="#__codelineno-91-19"></a><span class="c1"># Expected Output:</span>
</span><span id="__span-91-20"><a id="__codelineno-91-20" name="__codelineno-91-20" href="#__codelineno-91-20"></a><span class="c1"># [[1. 0. 0.]</span>
</span><span id="__span-91-21"><a id="__codelineno-91-21" name="__codelineno-91-21" href="#__codelineno-91-21"></a><span class="c1">#  [1. 0. 0.]]</span>
</span></code></pre></div>
<h3 id="sum"><code>sum</code><a class="headerlink" href="#sum" title="Permanent link">&para;</a></h3>
<p>The <strong>sum</strong> operation computes the sum of all elements or along a specified axis. The formula for the sum is straightforward:</p>
<div class="arithmatex">\[\text{sum}(x) = \sum_{i} x_i\]</div>
<p>The derivative of the sum operation with respect to each element is simply 1:</p>
<div class="arithmatex">\[\frac{d}{dx} \sum x_i = 1\]</div>
<p>In the forward pass, we use <code>np.sum()</code> to compute the sum of tensor elements along the given axis (or across all elements if no axis is specified).</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-92-1"><a id="__codelineno-92-1" name="__codelineno-92-1" href="#__codelineno-92-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-92-2"><a id="__codelineno-92-2" name="__codelineno-92-2" href="#__codelineno-92-2"></a>    <span class="c1"># Compute the sum of the tensor along the specified axis (or over all elements)</span>
</span><span id="__span-92-3"><a id="__codelineno-92-3" name="__codelineno-92-3" href="#__codelineno-92-3"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
</span><span id="__span-92-4"><a id="__codelineno-92-4" name="__codelineno-92-4" href="#__codelineno-92-4"></a>
</span><span id="__span-92-5"><a id="__codelineno-92-5" name="__codelineno-92-5" href="#__codelineno-92-5"></a>    <span class="n">dependencies</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-92-6"><a id="__codelineno-92-6" name="__codelineno-92-6" href="#__codelineno-92-6"></a>
</span><span id="__span-92-7"><a id="__codelineno-92-7" name="__codelineno-92-7" href="#__codelineno-92-7"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-92-8"><a id="__codelineno-92-8" name="__codelineno-92-8" href="#__codelineno-92-8"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-92-9"><a id="__codelineno-92-9" name="__codelineno-92-9" href="#__codelineno-92-9"></a>            <span class="c1"># ...</span>
</span><span id="__span-92-10"><a id="__codelineno-92-10" name="__codelineno-92-10" href="#__codelineno-92-10"></a>    <span class="c1"># ...</span>
</span><span id="__span-92-11"><a id="__codelineno-92-11" name="__codelineno-92-11" href="#__codelineno-92-11"></a>
</span><span id="__span-92-12"><a id="__codelineno-92-12" name="__codelineno-92-12" href="#__codelineno-92-12"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<p>In the backward pass we need to compute the gradient with respect to the sum of tensor elements. First, we initialize <code>full_grad</code> as an array of ones with the same shape as the input tensor. This represents the gradient that will be propagated back through the tensor.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-93-1"><a id="__codelineno-93-1" name="__codelineno-93-1" href="#__codelineno-93-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-93-2"><a id="__codelineno-93-2" name="__codelineno-93-2" href="#__codelineno-93-2"></a>    <span class="c1"># Initialize a gradient array of the same shape as the input</span>
</span><span id="__span-93-3"><a id="__codelineno-93-3" name="__codelineno-93-3" href="#__codelineno-93-3"></a>    <span class="n">full_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></code></pre></div>
<p>When <code>axis is None</code>, we've summed over all elements</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-94-1"><a id="__codelineno-94-1" name="__codelineno-94-1" href="#__codelineno-94-1"></a><span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-94-2"><a id="__codelineno-94-2" name="__codelineno-94-2" href="#__codelineno-94-2"></a>    <span class="k">return</span> <span class="n">full_grad</span> <span class="o">*</span> <span class="n">grad</span>
</span></code></pre></div>
<p>If <code>axis</code> is specified and <code>keepdims=True</code>, we maintain the shape of the input tensor during the backward pass. If there are multiple elements summed over an axis, each element gets a share of the gradient proportional to its contribution.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-95-1"><a id="__codelineno-95-1" name="__codelineno-95-1" href="#__codelineno-95-1"></a><span class="n">grad_expanded</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span><span id="__span-95-2"><a id="__codelineno-95-2" name="__codelineno-95-2" href="#__codelineno-95-2"></a>
</span><span id="__span-95-3"><a id="__codelineno-95-3" name="__codelineno-95-3" href="#__codelineno-95-3"></a><span class="k">return</span> <span class="n">full_grad</span> <span class="o">*</span> <span class="n">grad_expanded</span>
</span></code></pre></div>
<p><strong>Full implementation:</strong></p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-96-1"><a id="__codelineno-96-1" name="__codelineno-96-1" href="#__codelineno-96-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-96-2"><a id="__codelineno-96-2" name="__codelineno-96-2" href="#__codelineno-96-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-96-3"><a id="__codelineno-96-3" name="__codelineno-96-3" href="#__codelineno-96-3"></a><span class="sd">    Computes the sum of all elements in the tensor along a specified axis.</span>
</span><span id="__span-96-4"><a id="__codelineno-96-4" name="__codelineno-96-4" href="#__codelineno-96-4"></a>
</span><span id="__span-96-5"><a id="__codelineno-96-5" name="__codelineno-96-5" href="#__codelineno-96-5"></a><span class="sd">    Args:</span>
</span><span id="__span-96-6"><a id="__codelineno-96-6" name="__codelineno-96-6" href="#__codelineno-96-6"></a><span class="sd">        axis (int or tuple of ints, optional): Axis or axes along which a sum is performed. </span>
</span><span id="__span-96-7"><a id="__codelineno-96-7" name="__codelineno-96-7" href="#__codelineno-96-7"></a><span class="sd">            The default is to sum over all dimensions.</span>
</span><span id="__span-96-8"><a id="__codelineno-96-8" name="__codelineno-96-8" href="#__codelineno-96-8"></a><span class="sd">        keepdims (bool, optional): If True, the reduced axes are retained with length 1, </span>
</span><span id="__span-96-9"><a id="__codelineno-96-9" name="__codelineno-96-9" href="#__codelineno-96-9"></a><span class="sd">            otherwise the result is reshaped to eliminate the reduced axes. Default is False.</span>
</span><span id="__span-96-10"><a id="__codelineno-96-10" name="__codelineno-96-10" href="#__codelineno-96-10"></a>
</span><span id="__span-96-11"><a id="__codelineno-96-11" name="__codelineno-96-11" href="#__codelineno-96-11"></a><span class="sd">    Returns:</span>
</span><span id="__span-96-12"><a id="__codelineno-96-12" name="__codelineno-96-12" href="#__codelineno-96-12"></a><span class="sd">        Tensor: A new tensor with the summed values and the gradient dependencies.</span>
</span><span id="__span-96-13"><a id="__codelineno-96-13" name="__codelineno-96-13" href="#__codelineno-96-13"></a>
</span><span id="__span-96-14"><a id="__codelineno-96-14" name="__codelineno-96-14" href="#__codelineno-96-14"></a><span class="sd">    The sum operation accumulates all elements along a given axis (or all elements if axis is None).</span>
</span><span id="__span-96-15"><a id="__codelineno-96-15" name="__codelineno-96-15" href="#__codelineno-96-15"></a><span class="sd">    The gradient for this operation is computed by broadcasting the incoming gradient across </span>
</span><span id="__span-96-16"><a id="__codelineno-96-16" name="__codelineno-96-16" href="#__codelineno-96-16"></a><span class="sd">    the axis and summing it back up for each element.</span>
</span><span id="__span-96-17"><a id="__codelineno-96-17" name="__codelineno-96-17" href="#__codelineno-96-17"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-96-18"><a id="__codelineno-96-18" name="__codelineno-96-18" href="#__codelineno-96-18"></a>
</span><span id="__span-96-19"><a id="__codelineno-96-19" name="__codelineno-96-19" href="#__codelineno-96-19"></a>    <span class="c1"># Perform summation over specified axis</span>
</span><span id="__span-96-20"><a id="__codelineno-96-20" name="__codelineno-96-20" href="#__codelineno-96-20"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
</span><span id="__span-96-21"><a id="__codelineno-96-21" name="__codelineno-96-21" href="#__codelineno-96-21"></a>
</span><span id="__span-96-22"><a id="__codelineno-96-22" name="__codelineno-96-22" href="#__codelineno-96-22"></a>    <span class="c1"># Initialize the list of dependencies for gradient calculation</span>
</span><span id="__span-96-23"><a id="__codelineno-96-23" name="__codelineno-96-23" href="#__codelineno-96-23"></a>    <span class="n">dependencies</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Leaf</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-96-24"><a id="__codelineno-96-24" name="__codelineno-96-24" href="#__codelineno-96-24"></a>
</span><span id="__span-96-25"><a id="__codelineno-96-25" name="__codelineno-96-25" href="#__codelineno-96-25"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
</span><span id="__span-96-26"><a id="__codelineno-96-26" name="__codelineno-96-26" href="#__codelineno-96-26"></a>        <span class="c1"># Backward function to calculate the gradients for the sum operation</span>
</span><span id="__span-96-27"><a id="__codelineno-96-27" name="__codelineno-96-27" href="#__codelineno-96-27"></a>        <span class="k">def</span><span class="w"> </span><span class="nf">_bkwd</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-96-28"><a id="__codelineno-96-28" name="__codelineno-96-28" href="#__codelineno-96-28"></a><span class="w">            </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-96-29"><a id="__codelineno-96-29" name="__codelineno-96-29" href="#__codelineno-96-29"></a><span class="sd">            Compute the gradient of the sum operation. The gradient is summed along the specified axis.</span>
</span><span id="__span-96-30"><a id="__codelineno-96-30" name="__codelineno-96-30" href="#__codelineno-96-30"></a>
</span><span id="__span-96-31"><a id="__codelineno-96-31" name="__codelineno-96-31" href="#__codelineno-96-31"></a><span class="sd">            Args:</span>
</span><span id="__span-96-32"><a id="__codelineno-96-32" name="__codelineno-96-32" href="#__codelineno-96-32"></a><span class="sd">                grad (np.ndarray): The gradient passed from the downstream operation.</span>
</span><span id="__span-96-33"><a id="__codelineno-96-33" name="__codelineno-96-33" href="#__codelineno-96-33"></a>
</span><span id="__span-96-34"><a id="__codelineno-96-34" name="__codelineno-96-34" href="#__codelineno-96-34"></a><span class="sd">            Returns:</span>
</span><span id="__span-96-35"><a id="__codelineno-96-35" name="__codelineno-96-35" href="#__codelineno-96-35"></a><span class="sd">                np.ndarray: The gradient for the input tensor.</span>
</span><span id="__span-96-36"><a id="__codelineno-96-36" name="__codelineno-96-36" href="#__codelineno-96-36"></a>
</span><span id="__span-96-37"><a id="__codelineno-96-37" name="__codelineno-96-37" href="#__codelineno-96-37"></a><span class="sd">            If `keepdims` is True, the gradient is broadcasted to match the original tensor&#39;s dimensions.</span>
</span><span id="__span-96-38"><a id="__codelineno-96-38" name="__codelineno-96-38" href="#__codelineno-96-38"></a><span class="sd">            &quot;&quot;&quot;</span>
</span><span id="__span-96-39"><a id="__codelineno-96-39" name="__codelineno-96-39" href="#__codelineno-96-39"></a>
</span><span id="__span-96-40"><a id="__codelineno-96-40" name="__codelineno-96-40" href="#__codelineno-96-40"></a>            <span class="c1"># Initialize a gradient array of the same shape as the input</span>
</span><span id="__span-96-41"><a id="__codelineno-96-41" name="__codelineno-96-41" href="#__codelineno-96-41"></a>            <span class="n">full_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span><span id="__span-96-42"><a id="__codelineno-96-42" name="__codelineno-96-42" href="#__codelineno-96-42"></a>
</span><span id="__span-96-43"><a id="__codelineno-96-43" name="__codelineno-96-43" href="#__codelineno-96-43"></a>            <span class="c1"># When axis is None, we&#39;ve summed over all elements</span>
</span><span id="__span-96-44"><a id="__codelineno-96-44" name="__codelineno-96-44" href="#__codelineno-96-44"></a>            <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-96-45"><a id="__codelineno-96-45" name="__codelineno-96-45" href="#__codelineno-96-45"></a>                <span class="k">return</span> <span class="n">full_grad</span> <span class="o">*</span> <span class="n">grad</span>
</span><span id="__span-96-46"><a id="__codelineno-96-46" name="__codelineno-96-46" href="#__codelineno-96-46"></a>
</span><span id="__span-96-47"><a id="__codelineno-96-47" name="__codelineno-96-47" href="#__codelineno-96-47"></a>            <span class="n">grad_expanded</span> <span class="o">=</span> <span class="n">grad</span> <span class="k">if</span> <span class="n">keepdims</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
</span><span id="__span-96-48"><a id="__codelineno-96-48" name="__codelineno-96-48" href="#__codelineno-96-48"></a>
</span><span id="__span-96-49"><a id="__codelineno-96-49" name="__codelineno-96-49" href="#__codelineno-96-49"></a>            <span class="k">return</span> <span class="n">full_grad</span> <span class="o">*</span> <span class="n">grad_expanded</span>
</span><span id="__span-96-50"><a id="__codelineno-96-50" name="__codelineno-96-50" href="#__codelineno-96-50"></a>
</span><span id="__span-96-51"><a id="__codelineno-96-51" name="__codelineno-96-51" href="#__codelineno-96-51"></a>        <span class="c1"># Add the backward function to the dependencies list</span>
</span><span id="__span-96-52"><a id="__codelineno-96-52" name="__codelineno-96-52" href="#__codelineno-96-52"></a>        <span class="n">dependencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span><span id="__span-96-53"><a id="__codelineno-96-53" name="__codelineno-96-53" href="#__codelineno-96-53"></a>            <span class="n">Leaf</span><span class="p">(</span>
</span><span id="__span-96-54"><a id="__codelineno-96-54" name="__codelineno-96-54" href="#__codelineno-96-54"></a>                <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>  <span class="c1"># The input tensor</span>
</span><span id="__span-96-55"><a id="__codelineno-96-55" name="__codelineno-96-55" href="#__codelineno-96-55"></a>                <span class="n">grad_fn</span><span class="o">=</span><span class="n">_bkwd</span>  <span class="c1"># The backward function to compute the gradients</span>
</span><span id="__span-96-56"><a id="__codelineno-96-56" name="__codelineno-96-56" href="#__codelineno-96-56"></a>            <span class="p">)</span>
</span><span id="__span-96-57"><a id="__codelineno-96-57" name="__codelineno-96-57" href="#__codelineno-96-57"></a>        <span class="p">)</span>
</span><span id="__span-96-58"><a id="__codelineno-96-58" name="__codelineno-96-58" href="#__codelineno-96-58"></a>
</span><span id="__span-96-59"><a id="__codelineno-96-59" name="__codelineno-96-59" href="#__codelineno-96-59"></a>    <span class="c1"># Return a new tensor containing the sum, with the gradient dependencies if needed</span>
</span><span id="__span-96-60"><a id="__codelineno-96-60" name="__codelineno-96-60" href="#__codelineno-96-60"></a>    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependencies</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="mean"><code>mean</code><a class="headerlink" href="#mean" title="Permanent link">&para;</a></h3>
<p>The <strong>mean</strong> operation computes the average of all elements or along a specified axis. The formula for the mean is:</p>
<div class="arithmatex">\[\text{mean}(x) = \frac{1}{n} \sum_{i} x_i\]</div>
<p>where <span class="arithmatex">\(n\)</span> is the number of elements.</p>
<p>The derivative of the mean operation with respect to each element is:</p>
<div class="arithmatex">\[\frac{d}{dx} \frac{1}{n} \sum x_i = \frac{1}{n}\]</div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-97-1"><a id="__codelineno-97-1" name="__codelineno-97-1" href="#__codelineno-97-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">:</span>
</span><span id="__span-97-2"><a id="__codelineno-97-2" name="__codelineno-97-2" href="#__codelineno-97-2"></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="__span-97-3"><a id="__codelineno-97-3" name="__codelineno-97-3" href="#__codelineno-97-3"></a><span class="sd">    Computes the mean of elements along a specified axis.</span>
</span><span id="__span-97-4"><a id="__codelineno-97-4" name="__codelineno-97-4" href="#__codelineno-97-4"></a>
</span><span id="__span-97-5"><a id="__codelineno-97-5" name="__codelineno-97-5" href="#__codelineno-97-5"></a><span class="sd">    The mean is calculated as the sum of the elements divided by the number of elements along the given axis.</span>
</span><span id="__span-97-6"><a id="__codelineno-97-6" name="__codelineno-97-6" href="#__codelineno-97-6"></a>
</span><span id="__span-97-7"><a id="__codelineno-97-7" name="__codelineno-97-7" href="#__codelineno-97-7"></a><span class="sd">    Args:</span>
</span><span id="__span-97-8"><a id="__codelineno-97-8" name="__codelineno-97-8" href="#__codelineno-97-8"></a><span class="sd">        axis (Optional[int]): Axis along which to compute the mean. If None, computes the mean of all elements.</span>
</span><span id="__span-97-9"><a id="__codelineno-97-9" name="__codelineno-97-9" href="#__codelineno-97-9"></a><span class="sd">        keepdims (bool): Whether to keep the reduced dimensions in the output.</span>
</span><span id="__span-97-10"><a id="__codelineno-97-10" name="__codelineno-97-10" href="#__codelineno-97-10"></a>
</span><span id="__span-97-11"><a id="__codelineno-97-11" name="__codelineno-97-11" href="#__codelineno-97-11"></a><span class="sd">    Returns:</span>
</span><span id="__span-97-12"><a id="__codelineno-97-12" name="__codelineno-97-12" href="#__codelineno-97-12"></a><span class="sd">        Tensor: A new tensor containing the mean of the elements along the specified axis.</span>
</span><span id="__span-97-13"><a id="__codelineno-97-13" name="__codelineno-97-13" href="#__codelineno-97-13"></a>
</span><span id="__span-97-14"><a id="__codelineno-97-14" name="__codelineno-97-14" href="#__codelineno-97-14"></a><span class="sd">    The mean is calculated using the formula:</span>
</span><span id="__span-97-15"><a id="__codelineno-97-15" name="__codelineno-97-15" href="#__codelineno-97-15"></a><span class="sd">        mean = sum(elements) / number_of_elements</span>
</span><span id="__span-97-16"><a id="__codelineno-97-16" name="__codelineno-97-16" href="#__codelineno-97-16"></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="__span-97-17"><a id="__codelineno-97-17" name="__codelineno-97-17" href="#__codelineno-97-17"></a>
</span><span id="__span-97-18"><a id="__codelineno-97-18" name="__codelineno-97-18" href="#__codelineno-97-18"></a>    <span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span>
</span><span id="__span-97-19"><a id="__codelineno-97-19" name="__codelineno-97-19" href="#__codelineno-97-19"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span> <span class="o">/</span> <span class="n">count</span>
</span></code></pre></div>
<h2 id="more-activation-functions">More Activation Functions!<a class="headerlink" href="#more-activation-functions" title="Permanent link">&para;</a></h2>
<p>With our enhanced <code>Tensor</code> class, we can now build <strong>complex functions</strong> with ease. Let's explore essential activation functions  <strong>ReLU</strong>, <strong>LeakyReLU</strong> and <strong>Softmax</strong>  which are widely used in modern <strong>neural network architectures</strong>.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-98-1"><a id="__codelineno-98-1" name="__codelineno-98-1" href="#__codelineno-98-1"></a><span class="k">class</span><span class="w"> </span><span class="nc">ReLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-98-2"><a id="__codelineno-98-2" name="__codelineno-98-2" href="#__codelineno-98-2"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-98-3"><a id="__codelineno-98-3" name="__codelineno-98-3" href="#__codelineno-98-3"></a>        <span class="c1"># Apply ReLU: max(0, x)</span>
</span><span id="__span-98-4"><a id="__codelineno-98-4" name="__codelineno-98-4" href="#__codelineno-98-4"></a>        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</span><span id="__span-98-5"><a id="__codelineno-98-5" name="__codelineno-98-5" href="#__codelineno-98-5"></a>
</span><span id="__span-98-6"><a id="__codelineno-98-6" name="__codelineno-98-6" href="#__codelineno-98-6"></a><span class="k">class</span><span class="w"> </span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-98-7"><a id="__codelineno-98-7" name="__codelineno-98-7" href="#__codelineno-98-7"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>
</span><span id="__span-98-8"><a id="__codelineno-98-8" name="__codelineno-98-8" href="#__codelineno-98-8"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-98-9"><a id="__codelineno-98-9" name="__codelineno-98-9" href="#__codelineno-98-9"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
</span><span id="__span-98-10"><a id="__codelineno-98-10" name="__codelineno-98-10" href="#__codelineno-98-10"></a>
</span><span id="__span-98-11"><a id="__codelineno-98-11" name="__codelineno-98-11" href="#__codelineno-98-11"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-98-12"><a id="__codelineno-98-12" name="__codelineno-98-12" href="#__codelineno-98-12"></a>        <span class="c1"># Apply LeakyReLU: max(0, x) + alpha * min(0, x)</span>
</span><span id="__span-98-13"><a id="__codelineno-98-13" name="__codelineno-98-13" href="#__codelineno-98-13"></a>        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</span><span id="__span-98-14"><a id="__codelineno-98-14" name="__codelineno-98-14" href="#__codelineno-98-14"></a>
</span><span id="__span-98-15"><a id="__codelineno-98-15" name="__codelineno-98-15" href="#__codelineno-98-15"></a><span class="k">class</span><span class="w"> </span><span class="nc">Softmax</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span><span id="__span-98-16"><a id="__codelineno-98-16" name="__codelineno-98-16" href="#__codelineno-98-16"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span id="__span-98-17"><a id="__codelineno-98-17" name="__codelineno-98-17" href="#__codelineno-98-17"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-98-18"><a id="__codelineno-98-18" name="__codelineno-98-18" href="#__codelineno-98-18"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
</span><span id="__span-98-19"><a id="__codelineno-98-19" name="__codelineno-98-19" href="#__codelineno-98-19"></a>
</span><span id="__span-98-20"><a id="__codelineno-98-20" name="__codelineno-98-20" href="#__codelineno-98-20"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span><span id="__span-98-21"><a id="__codelineno-98-21" name="__codelineno-98-21" href="#__codelineno-98-21"></a>        <span class="c1"># For numerical stability, subtract the maximum value along the specified dimension.</span>
</span><span id="__span-98-22"><a id="__codelineno-98-22" name="__codelineno-98-22" href="#__codelineno-98-22"></a>        <span class="n">exp_input</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
</span><span id="__span-98-23"><a id="__codelineno-98-23" name="__codelineno-98-23" href="#__codelineno-98-23"></a>        <span class="k">return</span> <span class="n">exp_input</span> <span class="o">/</span> <span class="n">exp_input</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div>
<p>These implementations <strong>highlight the power of the <code>Tensor</code> class</strong> - with minimal code, we can define functions that form the <strong>building blocks</strong> of deep learning models. Thanks to the design of our framework, we only need to implement the <strong>forward</strong> method - <strong>backpropagation</strong> is handled automatically!</p>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<p>We now have a solid foundation for building the remaining tools needed to construct and train our Deep Neural Network. These fundamental tensor operations will enable automatic differentiation, making it easier to implement anything we need.</p>







  
  



  


  


  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


  <h2 id="__comments">Comments</h2>
  <script src="https://giscus.app/client.js"
        data-repo="nickovchinnikov/datasanta"
        data-repo-id="R_kgDONS23-g"
        data-category="Show and tell"
        data-category-id="DIC_kwDONS23-s4CkjoJ"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

  <!-- Synchronize Giscus theme with palette -->
  <script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
      var theme = palette.color.scheme === "slate"
        ? "transparent_dark"
        : "light"

      // Instruct Giscus to set theme
      giscus.setAttribute("data-theme", theme) 
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate"
            ? "transparent_dark"
            : "light"

          // Instruct Giscus to change theme
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>

      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
        
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../../02/05/classification---cross-entropy--softmax/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Classification - Cross-Entropy &amp; Softmax">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Classification - Cross-Entropy & Softmax
              </div>
            </div>
          </a>
        
        
          
          <a href="../../13/empirical-risk-and-cross-entropy-in-microtorch/" class="md-footer__link md-footer__link--next" aria-label="Next: Empirical Risk and Cross-Entropy in MicroTorch">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Empirical Risk and Cross-Entropy in MicroTorch
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:contact@datasanta.net" target="_blank" rel="noopener" title="send me an email" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 112c-8.8 0-16 7.2-16 16v22.1l172.5 141.6c20.7 17 50.4 17 71.1 0L464 150.1V128c0-8.8-7.2-16-16-16zM48 212.2V384c0 8.8 7.2 16 16 16h384c8.8 0 16-7.2 16-16V212.2L322 328.8c-38.4 31.5-93.7 31.5-132 0zM0 128c0-35.3 28.7-64 64-64h384c35.3 0 64 28.7 64 64v256c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://www.youtube.com/@datasanta" target="_blank" rel="noopener" title="YouTube" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://github.com/nickovchinnikov" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
    <a href="https://x.com/datasantaa" target="_blank" rel="noopener" title="DataSanta on X" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg>
    </a>
  
    
    
    
    
    <a href="https://t.me/datasantaa" target="_blank" rel="noopener" title="DataSanta on telegram" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M248 8C111.033 8 0 119.033 0 256s111.033 248 248 248 248-111.033 248-248S384.967 8 248 8m114.952 168.66c-3.732 39.215-19.881 134.378-28.1 178.3-3.476 18.584-10.322 24.816-16.948 25.425-14.4 1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25 5.342-39.5 3.652-3.793 67.107-61.51 68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608 69.142-14.845 10.194-26.894 9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7 18.45-13.7 108.446-47.248 144.628-62.3c68.872-28.647 83.183-33.623 92.511-33.789 2.052-.034 6.639.474 9.61 2.885a10.45 10.45 0 0 1 3.53 6.716 43.8 43.8 0 0 1 .417 9.769"/></svg>
    </a>
  
    
    
    
    
    <a href="/feed_rss_created.xml" target="_blank" rel="noopener" title="RSS Feed" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 64c0-17.7 14.3-32 32-32 229.8 0 416 186.2 416 416 0 17.7-14.3 32-32 32s-32-14.3-32-32C384 253.6 226.4 96 32 96 14.3 96 0 81.7 0 64m0 352a64 64 0 1 1 128 0 64 64 0 1 1-128 0m32-256c159.1 0 288 128.9 288 288 0 17.7-14.3 32-32 32s-32-14.3-32-32c0-123.7-100.3-224-224-224-17.7 0-32-14.3-32-32s14.3-32 32-32"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.footer", "navigation.indexes"], "search": "../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>