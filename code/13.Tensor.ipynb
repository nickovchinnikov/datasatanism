{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Union, List, Callable, Optional, Tuple, Literal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Scalar = Union[int, float]\n",
    "\n",
    "Data = Union[Scalar, list, np.ndarray, \"Tensor\"]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Leaf:\n",
    "    value: \"Tensor\"\n",
    "    grad_fn: Callable[[np.ndarray], np.ndarray]\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Data,\n",
    "        requires_grad: bool = False,\n",
    "        dependencies: Optional[List[Leaf]] = None,\n",
    "        dtype=np.float32\n",
    "    ):\n",
    "        self._data = Tensor.build_ndarray(data, dtype)\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.requires_grad = requires_grad\n",
    "        self.dependencies = dependencies or []\n",
    "\n",
    "        self.grad = np.zeros_like(self._data) if requires_grad else None\n",
    "\n",
    "    @property\n",
    "    def data(self) -> np.ndarray:\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, data: Data):\n",
    "        self._data = Tensor.build_ndarray(data, self.dtype)\n",
    "        if self.requires_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return self.data.size\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self) -> int:\n",
    "        return self.data.ndim\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ndarray(data: Data, dtype=np.float32) -> np.ndarray:\n",
    "        if isinstance(data, Tensor):\n",
    "            return np.array(data.data, dtype=dtype)\n",
    "        if isinstance(data, np.ndarray):\n",
    "            return data.astype(dtype)\n",
    "        return np.array(data, dtype=dtype)\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_gate(data: Data) -> \"Tensor\":\n",
    "        if isinstance(data, Tensor):\n",
    "            return data\n",
    "        return Tensor(data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.data}, requires_grad={self.requires_grad}, shape={self.shape})\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.grad is None:\n",
    "            self.grad = np.zeros_like(self._data)\n",
    "        else:\n",
    "            self.grad.fill(0.0)\n",
    "\n",
    "    def backward(self, grad: Optional[np.ndarray] = None) -> None:\n",
    "        if not self.requires_grad:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot call backward() on a tensor that does not require gradients. \"\n",
    "                \"If you need gradients, ensure that requires_grad=True when creating the tensor.\"\n",
    "            )\n",
    "\n",
    "        if grad is None:\n",
    "            if self.shape == ():\n",
    "                grad = np.array(1.0)\n",
    "            else:\n",
    "                raise ValueError(\"Grad must be provided if tensor has shape\")\n",
    "            \n",
    "        self.grad = self.grad + grad\n",
    "\n",
    "        for dependency in self.dependencies:\n",
    "            backward_grad = dependency.grad_fn(grad)\n",
    "            dependency.value.backward(backward_grad)\n",
    "\n",
    "    def transpose(self, axes: Tuple[int, ...] = None) -> \"Tensor\":\n",
    "        # Perform the transpose operation\n",
    "        output = np.transpose(self.data, axes=axes)\n",
    "\n",
    "        # Handle dependencies for autograd\n",
    "        dependencies: List[Leaf] = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                # Compute the inverse permutation of axes for the backward function\n",
    "                if axes is None:\n",
    "                    # Implicitly reverses transpose\n",
    "                    return np.transpose(grad)  \n",
    "                else:\n",
    "                    # Compute the inverse permutation of axes\n",
    "                    inv_axes = tuple(np.argsort(axes))\n",
    "                    # Transpose the gradient back using the inverse permutation\n",
    "                    return np.transpose(grad, axes=inv_axes)\n",
    "\n",
    "            dependencies.append(\n",
    "                Leaf(value=self, grad_fn=_bkwd)\n",
    "            )\n",
    "\n",
    "        # Return the new tensor with the transposed data\n",
    "        return Tensor(\n",
    "            output,\n",
    "            requires_grad=self.requires_grad,\n",
    "            dependencies=dependencies\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.transpose()\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "        r\"\"\"\n",
    "        Static method to perform matrix multiplication of two tensors.\n",
    "\n",
    "        Args:\n",
    "            a (Tensor): First matrix.\n",
    "            b (Tensor): Second matrix.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Resulting tensor with tracked dependencies.\n",
    "        \"\"\"\n",
    "        \n",
    "        output = a.data @ b.data\n",
    "        requires_grad = a.requires_grad or b.requires_grad\n",
    "        dependencies = []\n",
    "\n",
    "        if a.requires_grad:\n",
    "            def _bkwd_a(grad: np.ndarray) -> np.ndarray:\n",
    "                if b.ndim > 1:\n",
    "                    return grad @ b.data.swapaxes(-1, -2)\n",
    "                return np.outer(grad, b.data.T).squeeze()\n",
    "            \n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=a,\n",
    "                    grad_fn=_bkwd_a\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if b.requires_grad:\n",
    "            def _bkwd_b(grad: np.ndarray) -> np.ndarray:\n",
    "                if a.ndim > 1:\n",
    "                    return a.data.swapaxes(-1, -2) @ grad\n",
    "                return np.outer(a.data.T, grad).squeeze()\n",
    "            \n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=b,\n",
    "                    grad_fn=_bkwd_b\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, requires_grad, dependencies)\n",
    "    \n",
    "    def dot(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.matmul(self, Tensor.data_gate(other))\n",
    "    \n",
    "    def __matmul__(self, other: Data) -> \"Tensor\":\n",
    "        return self.dot(other)\n",
    "\n",
    "    @staticmethod\n",
    "    def bkwd_broadcast(tensor: \"Tensor\"):\n",
    "        def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "            if tensor.ndim == 0:\n",
    "                return np.sum(grad)\n",
    "\n",
    "            if grad.ndim == 0:\n",
    "                return grad\n",
    "\n",
    "            ndim_added = max(0, grad.ndim - tensor.ndim)\n",
    "\n",
    "            if ndim_added > 0:\n",
    "                grad = grad.sum(axis=tuple(range(ndim_added)), keepdims=False)\n",
    "\n",
    "            reduce_axes = tuple(\n",
    "                dim for dim in range(tensor.ndim)\n",
    "                if tensor.shape[dim] == 1 and grad.shape[dim] > 1\n",
    "            )\n",
    "\n",
    "            if reduce_axes:\n",
    "                grad = grad.sum(axis=reduce_axes, keepdims=True)\n",
    "\n",
    "            if grad.shape != tensor.shape:\n",
    "                grad = grad.reshape(tensor.shape)\n",
    "\n",
    "            return grad\n",
    "\n",
    "        return _bkwd\n",
    "\n",
    "    @staticmethod\n",
    "    def add(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "        output = a.data + b.data\n",
    "\n",
    "        requires_grad = a.requires_grad or b.requires_grad\n",
    "\n",
    "        dependencies = []\n",
    "\n",
    "        if a.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=a,\n",
    "                    grad_fn=Tensor.bkwd_broadcast(a)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if b.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=b,\n",
    "                    grad_fn=Tensor.bkwd_broadcast(b)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, requires_grad, dependencies)\n",
    "\n",
    "    def __add__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.add(self, Tensor.data_gate(other))\n",
    "    \n",
    "    def __radd__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.add(Tensor.data_gate(other), self)\n",
    "    \n",
    "    def __iadd__(self, other: Data) -> \"Tensor\":\n",
    "        self.data = self.data + Tensor.build_ndarray(other)\n",
    "        return self\n",
    "    \n",
    "    def __neg__(self) -> \"Tensor\":\n",
    "        output = -self.data\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(value=self, grad_fn=lambda grad: -grad)\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "    \n",
    "    def __sub__(self, other: Data) -> \"Tensor\":\n",
    "        return self + (-Tensor.data_gate(other))\n",
    "\n",
    "    def __rsub__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.data_gate(other) + (-self)\n",
    "\n",
    "    def __isub__(self, other: Data) -> \"Tensor\":\n",
    "        self.data = self.data - Tensor.build_ndarray(other)\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def mul(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "        output = a.data * b.data\n",
    "\n",
    "        requires_grad = a.requires_grad or b.requires_grad\n",
    "        dependencies = []\n",
    "\n",
    "        def _backward(a: \"Tensor\", b: \"Tensor\"):\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                grad = grad * b\n",
    "                return Tensor.bkwd_broadcast(a)(grad)\n",
    "            return _bkwd\n",
    "\n",
    "        if a.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=a,\n",
    "                    grad_fn=_backward(a, b)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if b.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=b,\n",
    "                    grad_fn=_backward(b, a)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, requires_grad, dependencies)\n",
    "    \n",
    "    def __mul__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.mul(self, Tensor.data_gate(other))\n",
    "    \n",
    "    def __rmul__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.mul(Tensor.data_gate(other), self)\n",
    "    \n",
    "    def __imul__(self, other: Data) -> \"Tensor\":\n",
    "        self.data = self.data * Tensor.build_ndarray(other)\n",
    "        return self\n",
    "\n",
    "    def log(self) -> \"Tensor\":\n",
    "        output = np.log(self.data)\n",
    "\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                return grad / self.data\n",
    "            \n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=self,\n",
    "                    grad_fn=_bkwd\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "\n",
    "    def tanh(self) -> \"Tensor\":\n",
    "        output = np.tanh(self.data)\n",
    "\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                return grad * (1 - output**2)\n",
    "\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=self,\n",
    "                    grad_fn=_bkwd\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "\n",
    "    def pow(self, p: Union[int, float]) -> \"Tensor\":\n",
    "        output = self.data**p\n",
    "\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                return grad * (p * (self.data**(p - 1)))\n",
    "\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=self,\n",
    "                    grad_fn=_bkwd\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "    \n",
    "    def __pow__(self, p: Union[int, float]) -> \"Tensor\":\n",
    "        return self.pow(p)\n",
    "\n",
    "    def __truediv__(self, other: Data) -> \"Tensor\":\n",
    "        other = Tensor.data_gate(other)\n",
    "        return self * (other**-1)\n",
    "\n",
    "    def __rtruediv__(self, other: Data) -> \"Tensor\":\n",
    "        other = Tensor.data_gate(other)\n",
    "        return other * (self**-1)\n",
    "\n",
    "    def __itruediv__(self, other: Data) -> \"Tensor\":\n",
    "        self.data = self.data / Tensor.build_ndarray(other)\n",
    "        return self\n",
    "\n",
    "    def exp(self) -> \"Tensor\":\n",
    "        output = np.exp(self.data)\n",
    "\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                return grad * output\n",
    "\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=self,\n",
    "                    grad_fn=_bkwd\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "\n",
    "    def squeeze(self, axis: Optional[Union[int, Tuple[int]]] = None) -> \"Tensor\":\n",
    "        output = np.squeeze(self.data, axis=axis)\n",
    "\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                if axis is None:\n",
    "                    return grad.reshape(self.shape)\n",
    "                return np.expand_dims(grad, axis=axis)\n",
    "            \n",
    "            dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "\n",
    "    def unsqueeze(self, dim: int) -> \"Tensor\":\n",
    "        output = np.expand_dims(self.data, axis=dim)\n",
    "\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                return np.squeeze(grad, axis=dim)\n",
    "            \n",
    "            dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n",
    "        \n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "    \n",
    "    def view(self, shape: Tuple[int, ...]) -> \"Tensor\":\n",
    "        output = self.data.reshape(shape)\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                return grad.reshape(self.shape)\n",
    "            \n",
    "            dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "    \n",
    "    # Comparison Operators\n",
    "    def __lt__(self, other: Data) -> \"Tensor\":\n",
    "        other = Tensor.data_gate(other)\n",
    "        return Tensor(self.data < other.data)\n",
    "    \n",
    "    def __gt__(self, other: Data) -> \"Tensor\":\n",
    "        other = Tensor.data_gate(other)\n",
    "        return Tensor(self.data > other.data)\n",
    "    \n",
    "    def __eq__(self, other: Data) -> \"Tensor\":\n",
    "        other = Tensor.data_gate(other)\n",
    "        return Tensor(self.data == other.data)\n",
    "    \n",
    "    def __le__(self, other: Data) -> \"Tensor\":\n",
    "        other = Tensor.data_gate(other)\n",
    "        return Tensor(self.data <= other.data)\n",
    "    \n",
    "    def __ge__(self, other: Data) -> \"Tensor\":\n",
    "        other = Tensor.data_gate(other)\n",
    "        return Tensor(self.data >= other.data)\n",
    "\n",
    "    def __ne__(self, other: Data) -> \"Tensor\":\n",
    "        other = Tensor.data_gate(other)\n",
    "        return Tensor(self.data != other.data)\n",
    "\n",
    "    @staticmethod\n",
    "    def where(condition: \"Tensor\", a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "        output = np.where(condition.data, a.data, b.data)\n",
    "\n",
    "        requires_grad = a.requires_grad or b.requires_grad\n",
    "        dependencies = []\n",
    "\n",
    "        if a.requires_grad:\n",
    "            def _bkwd_a(grad: np.ndarray) -> np.ndarray:\n",
    "                return np.where(condition.data, grad, 0.0)\n",
    "\n",
    "            dependencies.append(Leaf(value=a, grad_fn=_bkwd_a))\n",
    "        \n",
    "        if b.requires_grad:\n",
    "            def _bkwd_b(grad: np.ndarray) -> np.ndarray:\n",
    "                return np.where(condition.data, 0.0, grad)\n",
    "\n",
    "            dependencies.append(Leaf(value=b, grad_fn=_bkwd_b))\n",
    "\n",
    "        return Tensor(output, requires_grad, dependencies)\n",
    "\n",
    "    @staticmethod\n",
    "    def maximum(a: Data, b: Data) -> \"Tensor\":\n",
    "        a, b = Tensor.data_gate(a), Tensor.data_gate(b)\n",
    "\n",
    "        return Tensor.where(a > b, a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def minimum(a: Data, b: Data) -> \"Tensor\":\n",
    "        a, b = Tensor.data_gate(a), Tensor.data_gate(b)\n",
    "\n",
    "        return Tensor.where(a < b, a, b)\n",
    "\n",
    "    def threshold(self, threshold: float, value: float) -> \"Tensor\":\n",
    "        return Tensor.where(self > threshold, self, Tensor(value))\n",
    "\n",
    "    def masked_fill(self, mask: \"Tensor\", value: float) -> \"Tensor\":\n",
    "        return Tensor.where(mask, Tensor(value), self)\n",
    "\n",
    "    def sign(self) -> \"Tensor\":\n",
    "        return Tensor.where(\n",
    "            self > 0, Tensor(1),\n",
    "            Tensor.where(self < 0, Tensor(-1), Tensor(0))\n",
    "        )\n",
    "\n",
    "    def clip(self, min_value: Optional[float] = None, max_value: Optional[float] = None) -> \"Tensor\":\n",
    "        return Tensor.where(\n",
    "            self < min_value, Tensor(min_value),\n",
    "            Tensor.where(self > max_value, Tensor(max_value), self)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: Union[int, slice, List[int], Tuple[int, ...], np.ndarray, \"Tensor\"]) -> \"Tensor\":\n",
    "        if isinstance(index, (Tensor, np.ndarray)):\n",
    "            index = Tensor.data_gate(index).data\n",
    "\n",
    "        output = self.data[index]\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                full_grad = np.zeros_like(self.data)\n",
    "                np.add.at(full_grad, index, grad)\n",
    "                return full_grad\n",
    "            dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "\n",
    "    def abs(self) -> \"Tensor\":\n",
    "        return Tensor.where(self >= 0, self, -self)\n",
    "\n",
    "    def bkwd_minmax(\n",
    "        self,\n",
    "        output: np.ndarray,\n",
    "        axis: Optional[Union[int, Tuple[int, ...]]] = None,\n",
    "        keepdims: bool = False\n",
    "    ) -> np.ndarray:\n",
    "        def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "            mask = (self.data == output)\n",
    "\n",
    "            count = np.sum(mask) if axis is None \\\n",
    "                else np.sum(mask, axis=axis, keepdims=True)\n",
    "            \n",
    "            grad_expanded = grad if keepdims or axis is None \\\n",
    "                else np.expand_dims(grad, axis=axis)\n",
    "            \n",
    "            return mask * (grad_expanded / count)\n",
    "\n",
    "        return _bkwd\n",
    "\n",
    "    def min(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> \"Tensor\":\n",
    "        output = np.min(self.data, axis=axis, keepdims=keepdims)\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=self,\n",
    "                    grad_fn=self.bkwd_minmax(output, axis, keepdims)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "\n",
    "    def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> \"Tensor\":\n",
    "        output = np.max(self.data, axis=axis, keepdims=keepdims)\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=self,\n",
    "                    grad_fn=self.bkwd_minmax(output, axis, keepdims)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "\n",
    "    def sum(self, axis: Optional[int] = None, keepdims: bool = False) -> \"Tensor\":\n",
    "        output = np.sum(self.data, axis=axis, keepdims=keepdims)\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                full_grad = np.ones_like(self.data)\n",
    "\n",
    "                if axis is None:\n",
    "                    return full_grad * grad\n",
    "                \n",
    "                grad_expanded = grad if keepdims else np.expand_dims(grad, axis=axis)\n",
    "\n",
    "                return full_grad * grad_expanded\n",
    "\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=self,\n",
    "                    grad_fn=_bkwd\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "\n",
    "    def mean(self, axis: Optional[int] = None, keepdims: bool = False) -> \"Tensor\":\n",
    "        count = self.data.shape[axis] if axis is not None else self.size\n",
    "        return self.sum(axis=axis, keepdims=keepdims) / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([[1, 2],\n",
    "            [3, 4]], requires_grad=True)\n",
    "b = Tensor([10], requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "d = c - a * b + c * a * b\n",
    "\n",
    "q = c / (d.log().tanh() ** 3).exp() / 2 + c\n",
    "\n",
    "q.backward(np.ones_like(d.data))\n",
    "\n",
    "a.grad, b.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "If we have a function composition:  \n",
    "\n",
    "$$\n",
    "f(x) = g(h(x))\n",
    "$$\n",
    "\n",
    "Then, by the chain rule:\n",
    "\n",
    "$$\n",
    "f'(x) = g'(h(x)) \\cdot h'(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tensor([1, 2, 3], requires_grad=True)\n",
    "t.data = [[1, 3, 5], [2, 3, 4]]\n",
    "t_T = t.T\n",
    "\n",
    "t_T.backward(np.ones_like(t_T.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitMethod = Literal[\"xavier\", \"he\", \"he_leaky\", \"normal\", \"uniform\"]\n",
    "\n",
    "\n",
    "class Parameter(Tensor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *shape: int,\n",
    "        data: Optional[np.ndarray] = None,\n",
    "        init_method: InitMethod = \"xavier\",\n",
    "        gain: float = 1.0,\n",
    "        alpha: float = 0.01,\n",
    "    ):\n",
    "        if data is None:\n",
    "            data = self._init(shape, init_method, gain, alpha)\n",
    "\n",
    "        super().__init__(data=data, requires_grad=True)\n",
    "\n",
    "    def _init(\n",
    "        self,\n",
    "        shape: Tuple[int, ...], \n",
    "        init_method: InitMethod = \"xavier\", \n",
    "        gain: float = 1.0, \n",
    "        alpha: float = 0.01\n",
    "    ):\n",
    "        weights = np.random.randn(*shape)\n",
    "\n",
    "        if init_method == \"xavier\":\n",
    "            std = gain * np.sqrt(1.0 / shape[0])\n",
    "            return std * weights\n",
    "        if init_method == \"he\":\n",
    "            std = gain * np.sqrt(2.0 / shape[0])\n",
    "            return std * weights\n",
    "        if init_method == \"he_leaky\":\n",
    "            std = gain * np.sqrt(2.0 / (1 + alpha**2) * (1 / shape[0]))\n",
    "            return std * weights\n",
    "        if init_method == \"normal\":\n",
    "            return gain * weights\n",
    "        if init_method == \"uniform\":\n",
    "            return gain * np.random.uniform(-1, 1, size=shape)\n",
    "\n",
    "        raise ValueError(f\"Unknown initialization method: {init_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Module:\n",
    "    def __call__(self, *args, **kwds) -> Tensor:\n",
    "        return self.forward(*args, **kwds)\n",
    "\n",
    "    def forward(self, *args, **kwds):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        r\"\"\"\n",
    "        Returns a list of all parameters in the module and its submodules.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for _, item in self.__dict__.items():\n",
    "            if isinstance(item, Parameter):\n",
    "                params.append(item)\n",
    "            elif isinstance(item, Module):\n",
    "                params.extend(item.parameters())\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        r\"\"\"\n",
    "        Zeroes the gradients of all parameters in the module and its submodules.\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.zero_grad()\n",
    "\n",
    "    def params_count(self) -> int:\n",
    "        return sum(param.size for param in self.parameters())\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *modules: Module):\n",
    "        self.modules = modules\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        r\"\"\"\n",
    "        Returns a list of all parameters in the sequential module and its submodules.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            params.extend(module.parameters())\n",
    "        return params\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"\n",
    "        Passes the input through all modules in sequence.\n",
    "        \"\"\"\n",
    "        for module in self.modules:\n",
    "            x = module(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyModule(Module):\n",
    "    def __init__(self, dims: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.param = Parameter(dims)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return x.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    DummyModule(10),\n",
    "    DummyModule(100),\n",
    "    DummyModule(1000),\n",
    ")\n",
    "\n",
    "model.params_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer: Matrix-Matrix Dot Product  \n",
    "\n",
    "At layer $i$, the transformation is defined as:  \n",
    "\n",
    "$$A_i(\\mathbf{X}) = \\mathbf{X} \\mathbf{W}_i^T + \\mathbf{B}_i$$\n",
    "\n",
    "For a single layer:  \n",
    "\n",
    "$$F_i(\\mathbf{X}) = \\sigma(A_i(\\mathbf{X}))$$\n",
    "\n",
    "where $A_i(\\mathbf{X})$ is the linear transformation at layer $i$.  \n",
    "\n",
    "A deep neural network applies these transformations layer by layer, leading to the final output:  \n",
    "\n",
    "$$F(\\mathbf{X}) = \\sigma(A_L(\\sigma(A_{L-1}(\\dots \\sigma(A_1(\\mathbf{X})) \\dots )))$$\n",
    "\n",
    "Using **functional composition**, this process is compactly written as:  \n",
    "\n",
    "$$F(\\mathbf{X}) = A_L \\circ \\sigma \\circ A_{L-1} \\circ \\dots \\circ \\sigma \\circ A_1 (\\mathbf{X})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        init_method: InitMethod = \"xavier\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = Parameter(out_features, in_features, init_method=init_method)\n",
    "        self.bias = Parameter(out_features, init_method=\"normal\", gain=0.01) if bias else None\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # Check dimensions of input tensors\n",
    "        if x.ndim not in (2, 3):\n",
    "            raise ValueError(f\"Input must be 2D or 3D Tensor! x.ndim={x.ndim}\")\n",
    "\n",
    "        # Check if the last dimension of input matches in_features\n",
    "        if x.shape[-1] != self.in_features:\n",
    "            raise ValueError(\n",
    "                f\"Last dimension of input: {x.shape[-1]} does not match in_features: {self.in_features}\"\n",
    "            )\n",
    "\n",
    "        # Compute matrix multiplication: x @ weight^T\n",
    "        output = x @ self.weights.T\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "\n",
    "**Backward Pass (Gradients Computation)**\n",
    "\n",
    "Compute $\\frac{\\partial L}{\\partial A}$ and $\\frac{\\partial L}{\\partial B}$ using the chain rule.\n",
    "\n",
    "**Gradient w.r.t. A**\n",
    "The gradient of the loss $L$ with respect to $A$ is given by:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial Z} \\times B^T$$\n",
    "\n",
    "```python\n",
    "if a.requires_grad:\n",
    "    def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "        if b.ndim > 1:\n",
    "            return grad @ b.data.swapaxes(-1, -2)  # grad * B^T\n",
    "        return np.outer(grad, b.data.T).squeeze()  # Handles 1D case\n",
    "```\n",
    "\n",
    "- If $B$ is 2D, we use `b.data.swapaxes(-1, -2)` to compute $B^T$.\n",
    "- If $B$ is 1D, we use `np.outer(grad, b.data.T)` to ensure correct shape.\n",
    "\n",
    "\n",
    "**Gradient w.r.t. B**\n",
    "\n",
    "The gradient of the loss $L$ with respect to $B$ is given by:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial B} = A^T \\times \\frac{\\partial L}{\\partial Z}$$\n",
    "\n",
    "Where $A^T$ is the **transpose of A**.\n",
    "\n",
    "This is implemented as:\n",
    "\n",
    "```python\n",
    "if b.requires_grad:\n",
    "    def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "        if a.ndim > 1:\n",
    "            return a.data.swapaxes(-1, -2) @ grad  # A^T * grad\n",
    "        return np.outer(a.data.T, grad).squeeze()  # Handles 1D case\n",
    "```\n",
    "\n",
    "- If $A$ is 2D, we use `a.data.swapaxes(-1, -2)` to compute $A^T$.\n",
    "- If $A$ is 1D, we use `np.outer(a.data.T, grad)`.\n",
    "\n",
    "\n",
    "**Why Do We Use `swapaxes(-1, -2)` Instead of `.T`?**\n",
    "\n",
    "`swapaxes(-1, -2)` is a **general approach** for transposing the last two dimensions. This ensures compatibility with **both 2D matrices and higher-dimensional tensors** (e.g., batches of matrices).\n",
    "\n",
    "- `.T` works **only for 2D matrices**, affecting all axes in higher dimensions.\n",
    "- `swapaxes(-1, -2)` **preserves batch and other leading dimensions**, modifying only the last two.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Shape of Tensor | `.T` Output | `swapaxes(-1, -2)` Output |\n",
    "|----------------|------------|---------------------------|\n",
    "| `(m, n)` | `(n, m)` | `(n, m)` |\n",
    "| `(batch, m, n)` | `(n, m, batch)` (incorrect) | `(batch, n, m)` (correct) |\n",
    "| `(batch, time, m, n)` | `(n, m, time, batch)` (incorrect) | `(batch, time, n, m)` (correct) |\n",
    "\n",
    "\n",
    "Matrix multiplication follows the chain rule. The backward pass computes gradients for both $A$ and $B$ using transposes. Uses `swapaxes(-1, -2)` to generalize for higher-dimensional cases.\n",
    "\n",
    "| Tensor  | Gradient Formula | Code Implementation |\n",
    "|---------|-----------------|----------------------|\n",
    "| $A$ | $\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial Z} \\times B^T$ | `grad @ b.data.swapaxes(-1, -2)` |\n",
    "| $B$ | $\\frac{\\partial L}{\\partial B} = A^T \\times \\frac{\\partial L}{\\partial Z}$ | `a.data.swapaxes(-1, -2) @ grad` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "    r\"\"\"\n",
    "    Static method to perform matrix multiplication of two tensors.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): First matrix.\n",
    "        b (Tensor): Second matrix.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Resulting tensor with tracked dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = a.data @ b.data\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "    dependencies = []\n",
    "\n",
    "    if a.requires_grad:\n",
    "        def _bkwd_a(grad: np.ndarray) -> np.ndarray:\n",
    "            if b.ndim > 1:\n",
    "                return grad @ b.data.swapaxes(-1, -2)\n",
    "            return np.outer(grad, b.data.T).squeeze()\n",
    "        \n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=a,\n",
    "                grad_fn=_bkwd_a\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if b.requires_grad:\n",
    "        def _bkwd_b(grad: np.ndarray) -> np.ndarray:\n",
    "            if a.ndim > 1:\n",
    "                return a.data.swapaxes(-1, -2) @ grad\n",
    "            return np.outer(a.data.T, grad).squeeze()\n",
    "        \n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=b,\n",
    "                grad_fn=_bkwd_b\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Tensor(output, requires_grad, dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More operations: `bkwd_broadcasting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define array A with shape (3, 1)\n",
    "A = np.array([\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "])\n",
    "print(f\"Array A shape: {A.shape}\")\n",
    "\n",
    "# Define array B with shape (1, 4)\n",
    "B = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "])\n",
    "print(f\"Array B shape: {B.shape}\")\n",
    "\n",
    "# Perform broadcasting addition\n",
    "result = A + B\n",
    "\n",
    "print(\"A + B result: \")\n",
    "print(result)\n",
    "\n",
    "print(f\"Result of A + B shape: {result.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bkwd_broadcast` method ensures gradients are correctly summed across broadcasted dimensions in `backward` mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkwd_broadcast(tensor: \"Tensor\"):\n",
    "    def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "        if tensor.ndim == 0:\n",
    "            return np.sum(grad)\n",
    "\n",
    "        if grad.ndim == 0:\n",
    "            return grad\n",
    "        \n",
    "        ndim_added = max(0, grad.ndim - tensor.ndim)\n",
    "\n",
    "        if ndim_added > 0:\n",
    "            grad = grad.sum(axis=tuple(range(ndim_added)), keepdims=False)\n",
    "\n",
    "        reduce_axes = tuple(\n",
    "            dim for dim in range(tensor.ndim)\n",
    "            if tensor.shape[dim] == 1 and grad.shape[dim] > 1\n",
    "        )\n",
    "\n",
    "        if reduce_axes:\n",
    "            grad = grad.sum(axis=reduce_axes, keepdims=True)\n",
    "\n",
    "        if grad.shape != tensor.shape:\n",
    "            grad = grad.reshape(tensor.shape)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    return _bkwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of backward broadcasting\n",
    "\n",
    "In **Scenario 1**, `b` has shape `(1,)`, meaning it was **expanded to match both dimensions** of `a`. We **sum over all extra axes `(0,1)`** (`keepdims=False`) to return to shape `(1,)`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], \n",
    "              [3, 4]])  # Shape: (2, 2)\n",
    "\n",
    "b = np.array([10])      # Shape: (1,)  (Broadcasted across both axis)\n",
    "\n",
    "c = a + b\n",
    "print(f\"c: {c}\")\n",
    "\n",
    "grad_c = np.ones_like(c)\n",
    "print(f\"grad_c: {grad_c}\")\n",
    "\n",
    "# Since `a` was not broadcasted, the gradient just passes through\n",
    "grad_a = grad_c\n",
    "print(f\"grad_a: {grad_a}\")\n",
    "\n",
    "# Since `b` was **broadcasted along both axis**, we must **sum** over \n",
    "# that axis to reduce it back to `b`'s original shape `(1,)`\n",
    "grad_b = grad_c.sum(axis=(0, 1), keepdims=False)\n",
    "print(f\"grad_b: {grad_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In **Scenario 2**, `b` has shape `(2,1)`, meaning it was broadcasted along axis `1` to match `a`'s shape `(2,2)`\n",
    "\n",
    "We **sum over axis 1** (`keepdims=True`) to restore `b`'s original shape `(2,1)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], \n",
    "              [3, 4]])  # Shape: (2, 2)\n",
    "\n",
    "b = np.array([[10],\n",
    "              [20]])    # Shape: (2, 1)  (Broadcasted across axis 1)\n",
    "\n",
    "c = a + b\n",
    "print(f\"c: {c}\")\n",
    "\n",
    "grad_c = np.ones_like(c)\n",
    "print(f\"grad_c: {grad_c}\")\n",
    "\n",
    "# Since `a` was not broadcasted, the gradient just passes through\n",
    "grad_a = grad_c\n",
    "print(f\"grad_a: {grad_a}\")\n",
    "\n",
    "# Since `b` was **broadcasted along axis 1**, we must **sum** over \n",
    "# that axis to reduce it back to `b`'s original shape `(2,1)`\n",
    "grad_b = grad_c.sum(axis=1, keepdims=True)\n",
    "print(f\"grad_b: {grad_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `add`, `sub` and their friends\n",
    "\n",
    "$$f(a, b) = a + b$$\n",
    "\n",
    "The derivative of $a + b$ with respect to $a$ and $b$ is 1:\n",
    "\n",
    "$$\\frac{d}{da} (a + b) = 1$$\n",
    "\n",
    "$$\\frac{d}{db} (a + b) = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "    output = a.data + b.data\n",
    "\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "\n",
    "    dependencies = []\n",
    "\n",
    "    if a.requires_grad:\n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=a,\n",
    "                grad_fn=Tensor.bkwd_broadcast(a)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if b.requires_grad:\n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=b,\n",
    "                grad_fn=Tensor.bkwd_broadcast(b)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Tensor(output, requires_grad, dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mul`\n",
    "\n",
    "The **multiplication** operation computes the element-wise product of two tensors.\n",
    "\n",
    "$$f(a, b) = a \\cdot b$$\n",
    "\n",
    "The derivative of $a \\cdot b$ with respect to $a$ and $b$ is:\n",
    "\n",
    "$$\\frac{d}{da} (a \\cdot b) = b$$\n",
    "\n",
    "$$\\frac{d}{db} (a \\cdot b) = a$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "    output = a.data * b.data\n",
    "\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "\n",
    "    dependencies = []\n",
    "\n",
    "    def _backward(a: \"Tensor\", b: \"Tensor\"):\n",
    "        def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "            grad = grad * b\n",
    "            return Tensor.bkwd_broadcast(a)(grad)\n",
    "        \n",
    "        return _bkwd\n",
    "\n",
    "    if a.requires_grad:\n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=a,\n",
    "                grad_fn=_backward(a, b)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if b.requires_grad:\n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=b,\n",
    "                grad_fn=_backward(b, a)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Tensor(output, requires_grad, dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `log`\n",
    "\n",
    "The **logarithmic** operation computes the natural logarithm of each element in the tensor.\n",
    "\n",
    "$$f(x) = \\log(x)$$\n",
    "\n",
    "The derivative of $\\log(x)$ is:\n",
    "\n",
    "$$\\frac{d}{dx} \\log(x) = \\frac{1}{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tanh`\n",
    "\n",
    "The **tanh** operation computes the hyperbolic tangent of each element in the tensor.\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "The derivative of $\\tanh(x)$ is:\n",
    "\n",
    "$$\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pow`\n",
    "\n",
    "The **power** operation raises each element of the tensor to the specified power.\n",
    "\n",
    "$$f(x) = x^p$$\n",
    "\n",
    "The derivative of $x^p$ is:\n",
    "\n",
    "$$\\frac{d}{dx} x^p = p \\cdot x^{p-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exp`\n",
    "\n",
    "The **exponential** operation computes the exponent (base $e$) of each element in the tensor.\n",
    "\n",
    "$$\\exp(x) = e^x$$\n",
    "\n",
    "The derivative of $e^x$ is:\n",
    "\n",
    "$$\\frac{d}{dx} e^x = e^x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "### Tanh Function\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "A smooth, S-shaped activation function that maps input to the range $(-1, 1)$.\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "A smooth activation function that squashes input to the range $(0, 1)$, often used for probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input.tanh()\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return 1 / (1 + Tensor.exp(-input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Tensor([1, 2, 3], requires_grad=True)\n",
    "\n",
    "tanh_activation = Tanh()\n",
    "tanh_activation.forward(input).backward(np.ones_like(input.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_activation = Sigmoid()\n",
    "sigmoid_activation.forward(input).backward(np.ones_like(input.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `abs`\n",
    "\n",
    "The **absolute value** operation computes the magnitude of each element in a tensor, disregarding the sign. \n",
    "\n",
    "$$\\text{abs}(x) = |x|$$\n",
    "\n",
    "The derivative of $\\text{abs}(x)$ is:\n",
    "\n",
    "$$\\frac{d}{dx} |x| = \\text{sgn}(x)$$\n",
    "\n",
    "where the sign function $\\text{sgn}(x)$ is defined as:\n",
    "\n",
    "$$\\text{sgn}(x) =\n",
    "\\begin{cases}\n",
    "  1, & \\text{if } x > 0 \\\\\n",
    "  -1, & \\text{if } x < 0 \\\\\n",
    "  0, & \\text{if } x = 0\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `max`\n",
    "\n",
    "The **max operation** returns the maximum value of a tensor along a specified axis. If no axis is specified, it returns the maximum value from the entire tensor.\n",
    "\n",
    "For differentiation, the gradient of the maximum function is defined as:\n",
    "\n",
    "$$\\frac{d}{dx} \\max(X) =\n",
    "\\begin{cases}\n",
    "  1, & \\text{if } x \\text{ is the maximum value} \\\\\n",
    "  0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Activation Functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerically stable softmax calculation:\n",
    "\n",
    "$$\\text{Softmax}(x)_i = \\frac{\\exp(x_i - \\max(x))}{\\sum \\exp(x_j - \\max(x))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Apply ReLU: max(0, x)\n",
    "        return Tensor.maximum(0, input)\n",
    "\n",
    "class LeakyReLU(Module):\n",
    "    def __init__(self, alpha: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Apply LeakyReLU: max(0, x) + alpha * min(0, x)\n",
    "        return Tensor.maximum(0, input) + self.alpha * Tensor.minimum(0, input)\n",
    "\n",
    "class Softmax(Module):\n",
    "    def __init__(self, dim: int = -1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        exp_input = (input - input.max(axis=self.dim, keepdims=True)).exp()\n",
    "        return exp_input / exp_input.sum(axis=self.dim, keepdims=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
