{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Union, List, Callable, Optional, Tuple, Literal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Scalar = Union[int, float]\n",
    "\n",
    "Data = Union[Scalar, list, np.ndarray, \"Tensor\"]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Leaf:\n",
    "    value: \"Tensor\"\n",
    "    grad_fn: Callable[[np.ndarray], np.ndarray]\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Data,\n",
    "        requires_grad: bool = False,\n",
    "        dependencies: Optional[List[Leaf]] = None,\n",
    "        dtype=np.float32\n",
    "    ):\n",
    "        self._data = Tensor.build_ndarray(data, dtype)\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.requires_grad = requires_grad\n",
    "        self.dependencies = dependencies or []\n",
    "\n",
    "        self.grad = np.zeros_like(self._data) if requires_grad else None\n",
    "\n",
    "    @property\n",
    "    def data(self) -> np.ndarray:\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, data: Data):\n",
    "        self._data = Tensor.build_ndarray(data, self.dtype)\n",
    "        if self.requires_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return self.data.size\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def ndim(self) -> int:\n",
    "        return self.data.ndim\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ndarray(data: Data, dtype=np.float32) -> np.ndarray:\n",
    "        if isinstance(data, Tensor):\n",
    "            return np.array(data.data, dtype=dtype)\n",
    "        if isinstance(data, np.ndarray):\n",
    "            return data.astype(dtype)\n",
    "        return np.array(data, dtype=dtype)\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_gate(data: Data) -> \"Tensor\":\n",
    "        if isinstance(data, Tensor):\n",
    "            return data\n",
    "        return Tensor(data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor({self.data}, requires_grad={self.requires_grad}, shape={self.shape})\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.grad is None:\n",
    "            self.grad = np.zeros_like(self._data)\n",
    "        else:\n",
    "            self.grad.fill(0.0)\n",
    "\n",
    "    def backward(self, grad: Optional[np.ndarray] = None) -> None:\n",
    "        if not self.requires_grad:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot call backward() on a tensor that does not require gradients. \"\n",
    "                \"If you need gradients, ensure that requires_grad=True when creating the tensor.\"\n",
    "            )\n",
    "\n",
    "        if grad is None:\n",
    "            if self.shape == ():\n",
    "                grad = np.array(1.0)\n",
    "            else:\n",
    "                raise ValueError(\"Grad must be provided if tensor has shape\")\n",
    "            \n",
    "        self.grad = self.grad + grad\n",
    "\n",
    "        for dependency in self.dependencies:\n",
    "            backward_grad = dependency.grad_fn(grad)\n",
    "            dependency.value.backward(backward_grad)\n",
    "\n",
    "    def transpose(self, axes: Tuple[int, ...] = None) -> \"Tensor\":\n",
    "        # Perform the transpose operation\n",
    "        output = np.transpose(self.data, axes=axes)\n",
    "\n",
    "        # Handle dependencies for autograd\n",
    "        dependencies: List[Leaf] = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                # Compute the inverse permutation of axes for the backward function\n",
    "                if axes is None:\n",
    "                    # Implicitly reverses transpose\n",
    "                    return np.transpose(grad)  \n",
    "                else:\n",
    "                    # Compute the inverse permutation of axes\n",
    "                    inv_axes = tuple(np.argsort(axes))\n",
    "                    # Transpose the gradient back using the inverse permutation\n",
    "                    return np.transpose(grad, axes=inv_axes)\n",
    "\n",
    "            dependencies.append(\n",
    "                Leaf(value=self, grad_fn=_bkwd)\n",
    "            )\n",
    "\n",
    "        # Return the new tensor with the transposed data\n",
    "        return Tensor(\n",
    "            output,\n",
    "            requires_grad=self.requires_grad,\n",
    "            dependencies=dependencies\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.transpose()\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "        r\"\"\"\n",
    "        Static method to perform matrix multiplication of two tensors.\n",
    "\n",
    "        Args:\n",
    "            a (Tensor): First matrix.\n",
    "            b (Tensor): Second matrix.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Resulting tensor with tracked dependencies.\n",
    "        \"\"\"\n",
    "        \n",
    "        output = a.data @ b.data\n",
    "        requires_grad = a.requires_grad or b.requires_grad\n",
    "        dependencies = []\n",
    "\n",
    "        if a.requires_grad:\n",
    "            def _bkwd_a(grad: np.ndarray) -> np.ndarray:\n",
    "                if b.ndim > 1:\n",
    "                    return grad @ b.data.swapaxes(-1, -2)\n",
    "                return np.outer(grad, b.data.T).squeeze()\n",
    "            \n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=a,\n",
    "                    grad_fn=_bkwd_a\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if b.requires_grad:\n",
    "            def _bkwd_b(grad: np.ndarray) -> np.ndarray:\n",
    "                if a.ndim > 1:\n",
    "                    return a.data.swapaxes(-1, -2) @ grad\n",
    "                return np.outer(a.data.T, grad).squeeze()\n",
    "            \n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=b,\n",
    "                    grad_fn=_bkwd_b\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, requires_grad, dependencies)\n",
    "    \n",
    "    def dot(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.matmul(self, Tensor.data_gate(other))\n",
    "    \n",
    "    def __matmul__(self, other: Data) -> \"Tensor\":\n",
    "        return self.dot(other)\n",
    "\n",
    "    # @staticmethod\n",
    "    def bkwd_broadcast(tensor: \"Tensor\"):\n",
    "        def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "            if tensor.ndim == 0:\n",
    "                return np.sum(grad)\n",
    "\n",
    "            if grad.ndim == 0:\n",
    "                return grad\n",
    "\n",
    "            ndim_added = max(0, grad.ndim - tensor.ndim)\n",
    "\n",
    "            if ndim_added > 0:\n",
    "                grad = grad.sum(axis=tuple(range(ndim_added)), keepdims=False)\n",
    "\n",
    "            reduce_axes = tuple(\n",
    "                dim for dim in range(tensor.ndim)\n",
    "                if tensor.shape[dim] == 1 and grad.shape[dim] > 1\n",
    "            )\n",
    "\n",
    "            if reduce_axes:\n",
    "                grad = grad.sum(axis=reduce_axes, keepdims=True)\n",
    "\n",
    "            if grad.shape != tensor.shape:\n",
    "                grad = grad.reshape(tensor.shape)\n",
    "\n",
    "            return grad\n",
    "\n",
    "        return _bkwd\n",
    "\n",
    "    @staticmethod\n",
    "    def add(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "        output = a.data + b.data\n",
    "\n",
    "        requires_grad = a.requires_grad or b.requires_grad\n",
    "\n",
    "        dependencies = []\n",
    "\n",
    "        if a.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=a,\n",
    "                    grad_fn=Tensor.bkwd_broadcast(a)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if b.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=b,\n",
    "                    grad_fn=Tensor.bkwd_broadcast(b)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, requires_grad, dependencies)\n",
    "\n",
    "    def __add__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.add(self, Tensor.data_gate(other))\n",
    "    \n",
    "    def __radd__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.add(Tensor.data_gate(other), self)\n",
    "    \n",
    "    def __iadd__(self, other: Data) -> \"Tensor\":\n",
    "        self.data = self.data + Tensor.build_ndarray(other)\n",
    "        return self\n",
    "    \n",
    "    def __neg__(self) -> \"Tensor\":\n",
    "        output = -self.data\n",
    "        dependencies = []\n",
    "\n",
    "        if self.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(value=self, grad_fn=lambda grad: -grad)\n",
    "            )\n",
    "\n",
    "        return Tensor(output, self.requires_grad, dependencies)\n",
    "    \n",
    "    def __sub__(self, other: Data) -> \"Tensor\":\n",
    "        return self + (-Tensor.data_gate(other))\n",
    "\n",
    "    def __rsub__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.data_gate(other) + (-self)\n",
    "\n",
    "    def __isub__(self, other: Data) -> \"Tensor\":\n",
    "        self.data = self.data - Tensor.build_ndarray(other)\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def mul(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "        output = a.data * b.data\n",
    "\n",
    "        requires_grad = a.requires_grad or b.requires_grad\n",
    "        dependencies = []\n",
    "\n",
    "        def _backward(a: \"Tensor\", b: \"Tensor\"):\n",
    "            def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "                grad = grad * b\n",
    "                return Tensor.bkwd_broadcast(a)(grad)\n",
    "            return _bkwd\n",
    "\n",
    "        if a.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=a,\n",
    "                    grad_fn=_backward(a, b)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if b.requires_grad:\n",
    "            dependencies.append(\n",
    "                Leaf(\n",
    "                    value=b,\n",
    "                    grad_fn=_backward(b, a)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Tensor(output, requires_grad, dependencies)\n",
    "    \n",
    "    def __mul__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.mul(self, Tensor.data_gate(other))\n",
    "    \n",
    "    def __rmul__(self, other: Data) -> \"Tensor\":\n",
    "        return Tensor.mul(Tensor.data_gate(other), self)\n",
    "    \n",
    "    def __imul__(self, other: Data) -> \"Tensor\":\n",
    "        self.data = self.data * Tensor.build_ndarray(other)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[Tensor([[111. 131.]\n",
       "          [151. 171.]], requires_grad=True, shape=(2, 2)),\n",
       "         Tensor([[111. 131.]\n",
       "          [151. 171.]], requires_grad=True, shape=(2, 2))],\n",
       "        [Tensor([[111. 131.]\n",
       "          [151. 171.]], requires_grad=True, shape=(2, 2)),\n",
       "         Tensor([[111. 131.]\n",
       "          [151. 171.]], requires_grad=True, shape=(2, 2))]], dtype=object),\n",
       " array([Tensor([[ 84. 172.]\n",
       "         [268. 372.]], requires_grad=True, shape=(2, 2))], dtype=object))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([[1, 2],\n",
    "            [3, 4]], requires_grad=True)\n",
    "b = Tensor([10], requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "d = c - a * b + c * a * b\n",
    "\n",
    "d.backward(np.ones_like(d.data))\n",
    "\n",
    "a.grad, b.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "If we have a function composition:  \n",
    "\n",
    "$$\n",
    "f(x) = g(h(x))\n",
    "$$\n",
    "\n",
    "Then, by the chain rule:\n",
    "\n",
    "$$\n",
    "f'(x) = g'(h(x)) \\cdot h'(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tensor([1, 2, 3], requires_grad=True)\n",
    "t.data = [[1, 3, 5], [2, 3, 4]]\n",
    "t_T = t.T\n",
    "\n",
    "t_T.backward(np.ones_like(t_T.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitMethod = Literal[\"xavier\", \"he\", \"he_leaky\", \"normal\", \"uniform\"]\n",
    "\n",
    "\n",
    "class Parameter(Tensor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *shape: int,\n",
    "        data: Optional[np.ndarray] = None,\n",
    "        init_method: InitMethod = \"xavier\",\n",
    "        gain: float = 1.0,\n",
    "        alpha: float = 0.01,\n",
    "    ):\n",
    "        if data is None:\n",
    "            data = self._init(shape, init_method, gain, alpha)\n",
    "\n",
    "        super().__init__(data=data, requires_grad=True)\n",
    "\n",
    "    def _init(\n",
    "        self,\n",
    "        shape: Tuple[int, ...], \n",
    "        init_method: InitMethod = \"xavier\", \n",
    "        gain: float = 1.0, \n",
    "        alpha: float = 0.01\n",
    "    ):\n",
    "        weights = np.random.randn(*shape)\n",
    "\n",
    "        if init_method == \"xavier\":\n",
    "            std = gain * np.sqrt(1.0 / shape[0])\n",
    "            return std * weights\n",
    "        if init_method == \"he\":\n",
    "            std = gain * np.sqrt(2.0 / shape[0])\n",
    "            return std * weights\n",
    "        if init_method == \"he_leaky\":\n",
    "            std = gain * np.sqrt(2.0 / (1 + alpha**2) * (1 / shape[0]))\n",
    "            return std * weights\n",
    "        if init_method == \"normal\":\n",
    "            return gain * weights\n",
    "        if init_method == \"uniform\":\n",
    "            return gain * np.random.uniform(-1, 1, size=shape)\n",
    "\n",
    "        raise ValueError(f\"Unknown initialization method: {init_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Module:\n",
    "    def __call__(self, *args, **kwds) -> Tensor:\n",
    "        return self.forward(*args, **kwds)\n",
    "\n",
    "    def forward(self, *args, **kwds):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        r\"\"\"\n",
    "        Returns a list of all parameters in the module and its submodules.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for _, item in self.__dict__.items():\n",
    "            if isinstance(item, Parameter):\n",
    "                params.append(item)\n",
    "            elif isinstance(item, Module):\n",
    "                params.extend(item.parameters())\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        r\"\"\"\n",
    "        Zeroes the gradients of all parameters in the module and its submodules.\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.zero_grad()\n",
    "\n",
    "    def params_count(self) -> int:\n",
    "        return sum(param.size for param in self.parameters())\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *modules: Module):\n",
    "        self.modules = modules\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        r\"\"\"\n",
    "        Returns a list of all parameters in the sequential module and its submodules.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            params.extend(module.parameters())\n",
    "        return params\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"\n",
    "        Passes the input through all modules in sequence.\n",
    "        \"\"\"\n",
    "        for module in self.modules:\n",
    "            input = module(*input)\n",
    "        return input\n",
    "    \n",
    "\n",
    "class DummyModule(Module):\n",
    "    def __init__(self, dims: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.param = Parameter(dims)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return x.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1110"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    DummyModule(10),\n",
    "    DummyModule(100),\n",
    "    DummyModule(1000),\n",
    ")\n",
    "\n",
    "model.params_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer: Matrix-Matrix Dot Product  \n",
    "\n",
    "At layer $i$, the transformation is defined as:  \n",
    "\n",
    "$$A_i(\\mathbf{X}) = \\mathbf{X} \\mathbf{W}_i^T + \\mathbf{B}_i$$\n",
    "\n",
    "For a single layer:  \n",
    "\n",
    "$$F_i(\\mathbf{X}) = \\sigma(A_i(\\mathbf{X}))$$\n",
    "\n",
    "where $A_i(\\mathbf{X})$ is the linear transformation at layer $i$.  \n",
    "\n",
    "A deep neural network applies these transformations layer by layer, leading to the final output:  \n",
    "\n",
    "$$F(\\mathbf{X}) = \\sigma(A_L(\\sigma(A_{L-1}(\\dots \\sigma(A_1(\\mathbf{X})) \\dots )))$$\n",
    "\n",
    "Using **functional composition**, this process is compactly written as:  \n",
    "\n",
    "$$F(\\mathbf{X}) = A_L \\circ \\sigma \\circ A_{L-1} \\circ \\dots \\circ \\sigma \\circ A_1 (\\mathbf{X})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        init_method: InitMethod = \"xavier\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = Parameter(out_features, in_features, init_method=init_method)\n",
    "        self.bias = Parameter(out_features, init_method=\"normal\", gain=0.01) if bias else None\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # Check dimensions of input tensors\n",
    "        if x.ndim not in (2, 3):\n",
    "            raise ValueError(f\"Input must be 2D or 3D Tensor! x.ndim={x.ndim}\")\n",
    "\n",
    "        # Check if the last dimension of input matches in_features\n",
    "        if x.shape[-1] != self.in_features:\n",
    "            raise ValueError(\n",
    "                f\"Last dimension of input: {x.shape[-1]} does not match in_features: {self.in_features}\"\n",
    "            )\n",
    "\n",
    "        # Compute matrix multiplication: x @ weight^T\n",
    "        output = x @ self.weights.T\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "\n",
    "**Backward Pass (Gradients Computation)**\n",
    "\n",
    "Compute $\\frac{\\partial L}{\\partial A}$ and $\\frac{\\partial L}{\\partial B}$ using the chain rule.\n",
    "\n",
    "**Gradient w.r.t. A**\n",
    "The gradient of the loss $L$ with respect to $A$ is given by:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial Z} \\times B^T$$\n",
    "\n",
    "```python\n",
    "if a.requires_grad:\n",
    "    def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "        if b.ndim > 1:\n",
    "            return grad @ b.data.swapaxes(-1, -2)  # grad * B^T\n",
    "        return np.outer(grad, b.data.T).squeeze()  # Handles 1D case\n",
    "```\n",
    "\n",
    "- If $B$ is 2D, we use `b.data.swapaxes(-1, -2)` to compute $B^T$.\n",
    "- If $B$ is 1D, we use `np.outer(grad, b.data.T)` to ensure correct shape.\n",
    "\n",
    "\n",
    "**Gradient w.r.t. B**\n",
    "\n",
    "The gradient of the loss $L$ with respect to $B$ is given by:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial B} = A^T \\times \\frac{\\partial L}{\\partial Z}$$\n",
    "\n",
    "Where $A^T$ is the **transpose of A**.\n",
    "\n",
    "This is implemented as:\n",
    "\n",
    "```python\n",
    "if b.requires_grad:\n",
    "    def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "        if a.ndim > 1:\n",
    "            return a.data.swapaxes(-1, -2) @ grad  # A^T * grad\n",
    "        return np.outer(a.data.T, grad).squeeze()  # Handles 1D case\n",
    "```\n",
    "\n",
    "- If $A$ is 2D, we use `a.data.swapaxes(-1, -2)` to compute $A^T$.\n",
    "- If $A$ is 1D, we use `np.outer(a.data.T, grad)`.\n",
    "\n",
    "\n",
    "**Why Do We Use `swapaxes(-1, -2)` Instead of `.T`?**\n",
    "\n",
    "`swapaxes(-1, -2)` is a **general approach** for transposing the last two dimensions. This ensures compatibility with **both 2D matrices and higher-dimensional tensors** (e.g., batches of matrices).\n",
    "\n",
    "- `.T` works **only for 2D matrices**, affecting all axes in higher dimensions.\n",
    "- `swapaxes(-1, -2)` **preserves batch and other leading dimensions**, modifying only the last two.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Shape of Tensor | `.T` Output | `swapaxes(-1, -2)` Output |\n",
    "|----------------|------------|---------------------------|\n",
    "| `(m, n)` | `(n, m)` | `(n, m)` |\n",
    "| `(batch, m, n)` | `(n, m, batch)` (incorrect) | `(batch, n, m)` (correct) |\n",
    "| `(batch, time, m, n)` | `(n, m, time, batch)` (incorrect) | `(batch, time, n, m)` (correct) |\n",
    "\n",
    "\n",
    "Matrix multiplication follows the chain rule. The backward pass computes gradients for both $A$ and $B$ using transposes. Uses `swapaxes(-1, -2)` to generalize for higher-dimensional cases.\n",
    "\n",
    "| Tensor  | Gradient Formula | Code Implementation |\n",
    "|---------|-----------------|----------------------|\n",
    "| $A$ | $\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial Z} \\times B^T$ | `grad @ b.data.swapaxes(-1, -2)` |\n",
    "| $B$ | $\\frac{\\partial L}{\\partial B} = A^T \\times \\frac{\\partial L}{\\partial Z}$ | `a.data.swapaxes(-1, -2) @ grad` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "    r\"\"\"\n",
    "    Static method to perform matrix multiplication of two tensors.\n",
    "\n",
    "    Args:\n",
    "        a (Tensor): First matrix.\n",
    "        b (Tensor): Second matrix.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Resulting tensor with tracked dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = a.data @ b.data\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "    dependencies = []\n",
    "\n",
    "    if a.requires_grad:\n",
    "        def _bkwd_a(grad: np.ndarray) -> np.ndarray:\n",
    "            if b.ndim > 1:\n",
    "                return grad @ b.data.swapaxes(-1, -2)\n",
    "            return np.outer(grad, b.data.T).squeeze()\n",
    "        \n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=a,\n",
    "                grad_fn=_bkwd_a\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if b.requires_grad:\n",
    "        def _bkwd_b(grad: np.ndarray) -> np.ndarray:\n",
    "            if a.ndim > 1:\n",
    "                return a.data.swapaxes(-1, -2) @ grad\n",
    "            return np.outer(a.data.T, grad).squeeze()\n",
    "        \n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=b,\n",
    "                grad_fn=_bkwd_b\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Tensor(output, requires_grad, dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More operations: `bkwd_broadcasting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array A shape: (3, 1)\n",
      "Array B shape: (1, 4)\n",
      "A + B result: \n",
      "[[2 3 4 5]\n",
      " [3 4 5 6]\n",
      " [4 5 6 7]]\n",
      "Result of A + B shape: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define array A with shape (3, 1)\n",
    "A = np.array([\n",
    "    [1],\n",
    "    [2],\n",
    "    [3],\n",
    "])\n",
    "print(f\"Array A shape: {A.shape}\")\n",
    "\n",
    "# Define array B with shape (1, 4)\n",
    "B = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "])\n",
    "print(f\"Array B shape: {B.shape}\")\n",
    "\n",
    "# Perform broadcasting addition\n",
    "result = A + B\n",
    "\n",
    "print(\"A + B result: \")\n",
    "print(result)\n",
    "\n",
    "print(f\"Result of A + B shape: {result.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bkwd_broadcast` method ensures gradients are correctly summed across broadcasted dimensions in `backward` mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkwd_broadcast(tensor: \"Tensor\"):\n",
    "    def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "        if tensor.ndim == 0:\n",
    "            return np.sum(grad)\n",
    "\n",
    "        if grad.ndim == 0:\n",
    "            return grad\n",
    "        \n",
    "        ndim_added = max(0, grad.ndim - tensor.ndim)\n",
    "\n",
    "        if ndim_added > 0:\n",
    "            grad = grad.sum(axis=tuple(range(ndim_added)), keepdims=False)\n",
    "\n",
    "        reduce_axes = tuple(\n",
    "            dim for dim in range(tensor.ndim)\n",
    "            if tensor.shape[dim] == 1 and grad.shape[dim] > 1\n",
    "        )\n",
    "\n",
    "        if reduce_axes:\n",
    "            grad = grad.sum(axis=reduce_axes, keepdims=True)\n",
    "\n",
    "        if grad.shape != tensor.shape:\n",
    "            grad = grad.reshape(tensor.shape)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    return _bkwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of backward broadcasting\n",
    "\n",
    "In **Scenario 1**, `b` has shape `(1,)`, meaning it was **expanded to match both dimensions** of `a`. We **sum over all extra axes `(0,1)`** (`keepdims=False`) to return to shape `(1,)`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: [[11 12]\n",
      " [13 14]]\n",
      "grad_c: [[1 1]\n",
      " [1 1]]\n",
      "grad_a: [[1 1]\n",
      " [1 1]]\n",
      "grad_b: 4\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2], \n",
    "              [3, 4]])  # Shape: (2, 2)\n",
    "\n",
    "b = np.array([10])      # Shape: (1,)  (Broadcasted across both axis)\n",
    "\n",
    "c = a + b\n",
    "print(f\"c: {c}\")\n",
    "\n",
    "grad_c = np.ones_like(c)\n",
    "print(f\"grad_c: {grad_c}\")\n",
    "\n",
    "# Since `a` was not broadcasted, the gradient just passes through\n",
    "grad_a = grad_c\n",
    "print(f\"grad_a: {grad_a}\")\n",
    "\n",
    "# Since `b` was **broadcasted along both axis**, we must **sum** over \n",
    "# that axis to reduce it back to `b`'s original shape `(1,)`\n",
    "grad_b = grad_c.sum(axis=(0, 1), keepdims=False)\n",
    "print(f\"grad_b: {grad_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In **Scenario 2**, `b` has shape `(2,1)`, meaning it was broadcasted along axis `1` to match `a`'s shape `(2,2)`\n",
    "\n",
    "We **sum over axis 1** (`keepdims=True`) to restore `b`'s original shape `(2,1)`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: [[11 12]\n",
      " [23 24]]\n",
      "grad_c: [[1 1]\n",
      " [1 1]]\n",
      "grad_a: [[1 1]\n",
      " [1 1]]\n",
      "grad_b: [[2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2], \n",
    "              [3, 4]])  # Shape: (2, 2)\n",
    "\n",
    "b = np.array([[10],\n",
    "              [20]])    # Shape: (2, 1)  (Broadcasted across axis 1)\n",
    "\n",
    "c = a + b\n",
    "print(f\"c: {c}\")\n",
    "\n",
    "grad_c = np.ones_like(c)\n",
    "print(f\"grad_c: {grad_c}\")\n",
    "\n",
    "# Since `a` was not broadcasted, the gradient just passes through\n",
    "grad_a = grad_c\n",
    "print(f\"grad_a: {grad_a}\")\n",
    "\n",
    "# Since `b` was **broadcasted along axis 1**, we must **sum** over \n",
    "# that axis to reduce it back to `b`'s original shape `(2,1)`\n",
    "grad_b = grad_c.sum(axis=1, keepdims=True)\n",
    "print(f\"grad_b: {grad_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `add`, `sub` and their friends\n",
    "\n",
    "$$f(a, b) = a + b$$\n",
    "\n",
    "The derivative of $a + b$ with respect to $a$ and $b$ is 1:\n",
    "\n",
    "$$\\frac{d}{da} (a + b) = 1$$\n",
    "\n",
    "$$\\frac{d}{db} (a + b) = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "    output = a.data + b.data\n",
    "\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "\n",
    "    dependencies = []\n",
    "\n",
    "    if a.requires_grad:\n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=a,\n",
    "                grad_fn=Tensor.bkwd_broadcast(a)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if b.requires_grad:\n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=b,\n",
    "                grad_fn=Tensor.bkwd_broadcast(b)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Tensor(output, requires_grad, dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mul`\n",
    "\n",
    "The **multiplication** operation computes the element-wise product of two tensors.\n",
    "\n",
    "$$f(a, b) = a \\cdot b$$\n",
    "\n",
    "The derivative of $a \\cdot b$ with respect to $a$ and $b$ is:\n",
    "\n",
    "$$\\frac{d}{da} (a \\cdot b) = b$$\n",
    "\n",
    "$$\\frac{d}{db} (a \\cdot b) = a$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul(a: \"Tensor\", b: \"Tensor\") -> \"Tensor\":\n",
    "    output = a.data * b.data\n",
    "\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "\n",
    "    dependencies = []\n",
    "\n",
    "    def _backward(a: \"Tensor\", b: \"Tensor\"):\n",
    "        def _bkwd(grad: np.ndarray) -> np.ndarray:\n",
    "            grad = grad * b\n",
    "            return Tensor.bkwd_broadcast(a)(grad)\n",
    "        \n",
    "        return _bkwd\n",
    "\n",
    "    if a.requires_grad:\n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=a,\n",
    "                grad_fn=_backward(a, b)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if b.requires_grad:\n",
    "        dependencies.append(\n",
    "            Leaf(\n",
    "                value=b,\n",
    "                grad_fn=_backward(b, a)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Tensor(output, requires_grad, dependencies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
