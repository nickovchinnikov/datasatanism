{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DataSanta","text":"<p>Open the Deep Learning Mystery: YouTube </p> <p>Welcome to DataSanta, my digital diary where numbers dance, and algorithms whisper the secrets of the universe. Read more About This Blog</p> Here you can find me on platforms! <ul> <li>YouTube </li> <li>X(Twitter) </li> <li>GitHub </li> <li>Telegram </li> <li> contact@datasanta.net</li> </ul> <p>I believe in the power of knowledge, the magic of math, and the art of programming.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#what-is-datasanta","title":"What is DataSanta?","text":"<p>The human brain might be the universe's most complex enigma. Crack its code, and you could unlock the cosmos itself. Here, we explore mimicking the brain's structure to solve problems that even seem magical.</p> <ul> <li> <p>Demystification: We're here to peel back the layers of complexity surrounding deep learning. Expect to find posts that break down intricate concepts into digestible, sometimes humorous, pieces.</p> </li> <li> <p>Journey of Discovery: Just like the ancient alchemists seeking to transmute base metals into gold, we seek to transmute raw data into insights and predictions.</p> </li> <li> <p>Community: This isn't just a one-way street. Share your thoughts, code snippets, or your own dark arts of data manipulation.</p> </li> </ul>"},{"location":"about/#what-to-expect","title":"What to Expect","text":"<ul> <li>Tutorials: Step-by-step guides for beginners to experts.</li> <li>Theory: Deep dives into how and why certain algorithms work.</li> <li>Projects: Real-world applications where you can see theory in action.</li> </ul> <p>Join the Cult of Data</p> <p>Are you ready to explore, learn, and perhaps even laugh at the absurdity and beauty of deep learning? Then you're in the right place. Let's delve into the mysteries together, where each post could be a ritual to summon a better understanding of this dark art.</p>"},{"location":"about/#feel-free-to","title":"Feel free to:","text":"<ul> <li>Subscribe for updates.</li> <li>Comment on posts with your insights or questions.</li> <li>Connect with me and others who are equally fascinated by the power of data.</li> </ul> <p>I believe in the power of knowledge, the magic of math, and the art of programming.</p> <p>Let's open the mystery together.</p>"},{"location":"about/#here-you-can-find-me-on-platforms","title":"Here you can find me on platforms:","text":"<ul> <li>YouTube </li> <li>GitHub </li> <li>X(Twitter) </li> <li>Telegram </li> <li> contact@datasanta.net</li> </ul>"},{"location":"2025/04/13/empirical-risk-and-cross-entropy-in-microtorch/","title":"Empirical Risk and Cross-Entropy in MicroTorch","text":"<p>In the previous chapter we prepared the MicroTorch - Deep Learning from Scratch. Now, it's time to dive into creating the loss functions that will guide our model during training. In this session, we're going to focus on building two fundamental loss functions: <code>Binary Cross-Entropy (BCE)</code> and <code>Cross-Entropy (CE)</code>, using the Microtorch framework. These functions are essential for training models, especially for classification tasks, and I'll walk you through how to implement them from scratch.</p> <p></p> <p>Medieval loss discovery</p>","tags":["Empirical Risk","Cross Entropy Loss","Softmax","Classification","Binary Classification","Loss Function Design","Deep Learning Fundamentals","Backpropagation","Training Loop"]},{"location":"2025/04/13/empirical-risk-and-cross-entropy-in-microtorch/#check-the-jupyter-notebook","title":"Check the Jupyter Notebook","text":"","tags":["Empirical Risk","Cross Entropy Loss","Softmax","Classification","Binary Classification","Loss Function Design","Deep Learning Fundamentals","Backpropagation","Training Loop"]},{"location":"2025/04/13/empirical-risk-and-cross-entropy-in-microtorch/#loss-functions","title":"Loss Functions","text":"<p>Loss functions, also known as cost functions or objective functions, are used to measure how well the model's predictions match the true labels. The loss function computes a scalar value that represents the error between the model's predictions and the target labels. During training, the goal is to minimize this value using optimization techniques, thereby improving the model's performance.</p> <p>In our framework, we define a base class <code>Loss</code> that serves as a template for various types of loss functions. We also provide a few commonly used loss functions like Cross-Entropy Loss and Binary Cross-Entropy Loss.</p> <p>For more details you can check my post about the cross-entropy loss</p>","tags":["Empirical Risk","Cross Entropy Loss","Softmax","Classification","Binary Classification","Loss Function Design","Deep Learning Fundamentals","Backpropagation","Training Loop"]},{"location":"2025/04/13/empirical-risk-and-cross-entropy-in-microtorch/#expected-risk-and-reduction-of-loss","title":"Expected Risk and Reduction of Loss","text":"<p>In machine learning, the expected risk measures the average loss of your model on the true data distribution \u2014 not just the training set.</p> <p>Mathematically, it's defined as:</p> \\[R(f) = \\mathbb{E}_{(x, y) \\sim P_{\\text{data}}} \\left[ \\mathcal{L}(f(x), y) \\right]\\] <p>Where:</p> <ul> <li> <p>\\(R(f)\\) is the expected risk (true error)</p> </li> <li> <p>\\(\\mathcal{L}\\) is your loss function</p> </li> <li> <p>\\(f(x)\\) is the model's prediction</p> </li> <li> <p>and \\((x, y)\\) are samples drawn from the data distribution</p> </li> </ul> <p>Let's break it down and first we focus on: </p> \\[\\mathbb{E}_{(x, y) \\sim P_{\\text{data}}}\\] <p>This is saying:</p> <p>Take the expected value (average) over all possible input-output pairs \\((x, y)\\) that are drawn from the true data distribution \\(P_{\\text{data}}\\).</p> <p>What's Going On: \\(\\mathbb{E}\\) is the expectation operator \u2014 it computes the average value of whatever comes after it. \\((x, y) \\sim P_{\\text{data}}\\) means - we are sampling input-output pairs from a probability distribution \\(P_{\\text{data}}\\). This is the true underlying distribution that your data comes from in the real world. Think of it as: \"draw all possible data points the universe can produce.\" (Not just the ones in your training set.)</p> <p>Putting it all together:</p> \\[R(f) = \\mathbb{E}_{(x, y) \\sim P_{\\text{data}}} \\left[ \\mathcal{L}(f(x), y) \\right]\\] <p>Means:</p> <p>\"On average, over the entire true data distribution, how much error does my model make?\"</p> <p>That's the expected risk \u2014 it tells us how good our model is in the real world, not just on our training samples.</p> <p>Since we don't know the full data distribution, we approximate this using the empirical risk - the average loss over a finite training set:</p> \\[\\hat{R}(f) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(f(x_i), y_i)\\] <p>In code, this corresponds to:</p> <pre><code>loss.mean()\n</code></pre> <p>If you're summing the individual losses:</p> \\[\\mathcal{L}_{\\text{sum}} = \\sum_{i=1}^N \\mathcal{L}(f(x_i), y_i)\\] <p>You're computing the total loss across the batch. This is not normalized, so it's not equivalent to empirical risk \u2014 you're missing the \\(\\frac{1}{N}\\) factor.</p> <p>In code:</p> <pre><code>loss.sum()\n</code></pre> <p>This is where loss reduction comes into play.</p> <p>After computing the loss for each element in a batch, we typically reduce it to a single scalar \u2014 this is known as loss reduction. You can choose:</p> <ul> <li> <p>Using sum makes the total loss grow with batch size, which may affect learning rate sensitivity.</p> </li> <li> <p>Mean normalizes the loss, making it more stable across batches.</p> </li> <li> <p>Keeping none gives full control, useful for debugging or custom aggregation.</p> </li> </ul> <p>The <code>reduction_loss</code> function handles this reduction process. Here's the implementation:</p> <pre><code>def reduction_loss(\n    loss: Tensor, reduction: Literal[\"mean\", \"sum\", \"none\"] = \"mean\"\n) -&gt; Tensor:\n    r\"\"\"\n    Reduction loss function.\n    Apply the specified reduction method to the loss.\n\n    Args:\n        loss (Tensor): The computed loss tensor.\n        reduction (str): The reduction method to apply, can be \"mean\", \"sum\", or \"none\".\n\n    Returns:\n        Tensor: The reduced loss value.\n    \"\"\"\n\n    if reduction == \"mean\":\n        return loss.mean()\n    if reduction == \"sum\":\n        return loss.sum()\n    return loss\n</code></pre> <p>When Would You Use <code>mean</code>?</p> <ul> <li> <p>Standard Training Setups: The most common choice, <code>mean</code> normalizes the loss across batches, providing consistent scaling regardless of batch size, ensuring training stability.</p> </li> <li> <p>Unbiased Estimator of Expected Risk: By averaging the losses, you get an unbiased estimate of the expected risk, which is the true error of the model when evaluated on the full data distribution (not just the training samples). It helps avoid the model being biased by large batches or a small batch size.</p> </li> <li> <p>Stability: Using <code>mean</code> ensures that learning rate settings remain consistent across different batch sizes, making optimization more stable.</p> </li> </ul> <p>When Would You Use <code>sum</code>?</p> <ul> <li> <p>Gradient Accumulation: If you're manually accumulating gradients over multiple small batches before an optimizer step, summing the losses ensures the gradients accumulate properly across all batches.</p> </li> <li> <p>Loss Weighting: If you're applying a global scaling factor later in the training process and want to control the weight globally, using <code>sum</code> allows you to work with the total loss rather than averaging over the batch.</p> </li> <li> <p>Scale Consistency: Be cautious when the batch size varies \u2014 using <code>sum</code> introduces scale inconsistency. The loss (and gradients!) will change if your batch size is different from one iteration to the next, which might affect training stability.</p> </li> </ul> <p>When Would You Use <code>none</code>?</p> <ul> <li> <p>Per-Sample Losses: If you need the loss per individual sample (e.g., when you're applying a custom weighting or masking strategy), <code>none</code> allows you to work with each sample's loss independently.</p> </li> <li> <p>Debugging: When debugging or analyzing your model's behavior on specific examples, having access to the per-sample loss can be extremely helpful in pinpointing issues with certain data points.</p> </li> <li> <p>Custom Aggregation Logic: If you're implementing custom loss reduction strategies, or if you want to compute a non-standard aggregate of the per-sample losses, keeping the losses separate with <code>none</code> offers the flexibility needed to apply custom logic.</p> </li> </ul> Reduction Math Approximates Use Case <code>mean</code> \\( \\frac{1}{N} \\sum \\mathcal{L}_i \\) Expected Risk \\( \\mathbb{E}[\\mathcal{L}] \\) Standard training <code>sum</code> \\( \\sum \\mathcal{L}_i \\) \\( N \\cdot \\text{Empirical Risk} \\) Gradient accumulation, custom loss scaling <code>none</code> \\( [\\mathcal{L}_1, ..., \\mathcal{L}_N] \\) Per-sample view Custom reductions, masking, debugging","tags":["Empirical Risk","Cross Entropy Loss","Softmax","Classification","Binary Classification","Loss Function Design","Deep Learning Fundamentals","Backpropagation","Training Loop"]},{"location":"2025/04/13/empirical-risk-and-cross-entropy-in-microtorch/#base-loss-class","title":"Base Loss Class","text":"<p>The <code>Loss</code> class serves as a base class for all loss functions. It defines a <code>compute_loss</code> method that subclasses must implement, and a <code>forward</code> method that applies the loss computation and reduction:</p> <pre><code>class Loss(Module):\n    r\"\"\"\n    Base class for loss functions.\n\n    This class provides a common interface for all loss functions.\n    \"\"\"\n\n    def __init__(self, reduction: Literal[\"mean\", \"sum\", \"none\"] = \"mean\"):\n        r\"\"\"\n        Initialize the loss function with the specified reduction method.\n\n        Args:\n            reduction (str): The reduction method to apply to the loss. Options are \"mean\", \"sum\", or \"none\".\n        \"\"\"\n\n        self.reduction = reduction\n\n    def compute_loss(self, pred: Tensor, target: Tensor) -&gt; Tensor:\n        r\"\"\"\n        Compute the loss function. This method must be implemented in subclasses.\n\n        Args:\n            pred (Tensor): The predicted values (output of the model).\n            target (Tensor): The true target values (ground truth).\n\n        Returns:\n            Tensor: The computed loss value.\n        \"\"\"\n\n        raise NotImplementedError\n\n    def forward(self, pred: Tensor, target: Tensor) -&gt; Tensor:\n        r\"\"\"\n        Forward pass.\n        Apply the loss computation and reduction.\n\n        Args:\n            pred (Tensor): The predicted values (output of the model).\n            target (Tensor): The true target values (ground truth).\n\n        Returns:\n            Tensor: The reduced loss value after applying the specified reduction.\n\n        Raises:\n            ValueError: If `pred` and `target` do not have the same shape.\n        \"\"\"\n\n        if pred.shape != target.shape:\n            raise ValueError(\n                f\"Input and target must have the same shape, but got {pred.shape} and {target.shape}\"\n            )\n\n        loss = self.compute_loss(pred, target)\n        return reduction_loss(loss, self.reduction)\n</code></pre>","tags":["Empirical Risk","Cross Entropy Loss","Softmax","Classification","Binary Classification","Loss Function Design","Deep Learning Fundamentals","Backpropagation","Training Loop"]},{"location":"2025/04/13/empirical-risk-and-cross-entropy-in-microtorch/#binary-cross-entropy-loss","title":"Binary Cross-Entropy Loss","text":"<p>Binary Cross-Entropy Loss is used for binary classification tasks, where each output is a probability value between 0 and 1. It measures the dissimilarity between the true labels and predicted probabilities for binary classification. The formula is:</p> \\[L(y, \\hat{y}) = -(y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}))\\] <p>Where \\(y\\) is the true label (0 or 1) and \\(\\hat{y}\\) is the predicted probability (between 0 and 1).</p> <p>The <code>BCELoss</code> class implements this loss:</p> <pre><code>class BCELoss(Loss):\n    r\"\"\"\n    Binary Cross Entropy (BCE) Loss function.\n\n    This loss function is used for binary classification tasks. It measures \n    the difference between the predicted probabilities and the actual binary \n    values (0 or 1).\n\n    Args:\n        eps (float): Small constant to avoid numerical instability when\n            taking logarithms of values close to 0 or 1.\n\n    Inherits from:\n        Loss: The base loss class.\n    \"\"\"\n\n    def __init__(self, eps: float = 1e-9):\n        \"\"\"\n        Initialize BCE Loss with mean reduction by default.\n        \"\"\"\n\n        super().__init__(reduction=\"mean\")\n        self.eps = eps\n\n    def compute_loss(self, prediction: Tensor, target: Tensor) -&gt; Tensor:\n        r\"\"\"\n        Compute Binary Cross Entropy Loss.\n\n        The Binary Cross-Entropy loss is defined as:\n\n            L = -(target * log(prediction) + (1 - target) * log(1 - prediction))\n\n        where `prediction` is the predicted probability, and `target` is the \n        true label (0 or 1).\n\n        Args:\n            prediction (Tensor): The predicted probabilities (values between 0 and 1).\n            target (Tensor): The true labels (0 or 1).\n\n        Returns:\n            Tensor: The computed Binary Cross-Entropy loss for each sample.\n        \"\"\"\n\n        # Clip predictions to avoid log(0) or log(1)\n        pred = prediction.clip(self.eps, 1 - self.eps)\n        # Compute the BCE loss using the formula: -(y*log(p) + (1-y)*log(1-p))\n        loss = -(target * pred.log() + (1 - target) * (1 - pred).log())\n        return loss\n</code></pre>","tags":["Empirical Risk","Cross Entropy Loss","Softmax","Classification","Binary Classification","Loss Function Design","Deep Learning Fundamentals","Backpropagation","Training Loop"]},{"location":"2025/04/13/empirical-risk-and-cross-entropy-in-microtorch/#cross-entropy-loss-and-softmax-together","title":"Cross-Entropy Loss and Softmax Together","text":"<p>When we are dealing with multiple classes instead of just two, we need to scale up the entropy from <code>BinaryCrossEntropyLoss</code> to <code>CrossEntropyLoss</code>. And the <code>Sigmoid</code> function is not the best choice for this task. <code>Sigmoid</code> outputs probabilities for each class independently, which is not good for multi-class classification. Instead, we need to assign probabilities across multiple classes, ensuring they sum to 1. A much better approach is to use the <code>Softmax</code> function, which converts raw model outputs (logits) into a probability distribution over all classes. This allows our model to make more accurate predictions by selecting the class with the highest probability.</p> <p>The numerically stable <code>Softmax</code> calculation:</p> \\[\\text{Softmax}(x)_i = \\frac{\\exp(x_i - \\max(x))}{\\sum \\exp(x_j - \\max(x))}\\] <p>In multiclass classification, the combination of Softmax + Cross-Entropy Loss has a unique property that simplifies the backward pass.</p> <p>The Softmax function is defined as: \\(S_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\\) and its derivative forms a Jacobian matrix:</p> \\[\\frac{\\partial S_i}{\\partial z_j} = \\begin{cases} S_i (1 - S_i) &amp; \\text{if } i = j \\\\ - S_i S_j &amp; \\text{if } i \\neq j \\end{cases}\\] <p>This Jacobian matrix is \\(N \\times N\\) (where \\(N\\) is the number of classes), which makes direct backpropagation inefficient.</p> <p>But, the Cross-Entropy Loss \\(L = -\\sum_{i} y_i \\log(S_i)\\), and its gradient after softmax is simply:</p> \\[\\frac{\\partial L}{\\partial z} = S - y\\] <p>The Softmax Jacobian cancels out with the Cross-Entropy derivative, so we avoid computing the full Jacobian. Instead, Softmax directly passes the gradient from Cross-Entropy, making backpropagation simpler and more efficient!</p> Why does the derivative of Cross-Entropy take the form \\(\\frac{\\partial L}{\\partial z_i} = S_i - y_i\\)? <p>The Cross-Entropy Loss function is \\(L = -\\sum_{i} y_i \\log(S_i)\\), where \\(y_i\\) is the one-hot encoded true label (\\(y_i = 1\\) for the correct class, 0 otherwise). \\(S_i\\) is the softmax output (predicted probability for class \\(i\\)).</p> <p>Now, let's compute the derivative of \\(L\\) with respect to \\(S_i\\):</p> \\[\\frac{\\partial L}{\\partial S_i} = -\\frac{y_i}{S_i}\\] <p>However, the goal is to compute the gradient with respect to \\(z_i\\) (the input logits), not \\(S_i\\). This is where the <code>Softmax</code> derivative comes in. Softmax is defined as:</p> \\[S_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\\] <p>The derivative of \\(S_i\\) with respect to \\(z_j\\) gives a Jacobian matrix:</p> \\[ \\frac{\\partial S_i}{\\partial z_j} = \\begin{cases} S_i (1 - S_i) &amp; \\text{if } i = j \\quad \\text{(diagonal terms)}\\\\ - S_i S_j &amp; \\text{if } i \\neq j \\quad \\text{(off-diagonal terms)} \\end{cases} \\] <p>This means that if we want to find how the loss \\(L\\) changes with respect to \\(z_i\\), we need to apply the chain rule:</p> \\[\\frac{\\partial L}{\\partial z_i} = \\sum_{j} \\frac{\\partial L}{\\partial S_j} \\frac{\\partial S_j}{\\partial z_i}\\] <p>Substituting:</p> \\[\\frac{\\partial L}{\\partial S_j} = -\\frac{y_j}{S_j}\\] <p>and</p> \\[ \\frac{\\partial S_j}{\\partial z_i} = \\begin{cases} S_j (1 - S_j) &amp; \\text{if } i = j \\\\ - S_j S_i &amp; \\text{if } i \\neq j \\end{cases} \\] <p>Let's expand:</p> \\[ \\frac{\\partial L}{\\partial z_i} = \\sum_{j} -\\frac{y_j}{S_j} \\cdot \\frac{\\partial S_j}{\\partial z_i} \\] <p>Breaking it into cases:</p> <ol> <li>Diagonal term (\\(i = j\\)):</li> </ol> \\[ -\\frac{y_i}{S_i} \\cdot S_i (1 - S_i) = - y_i (1 - S_i) \\] <ol> <li>Off-diagonal terms (\\(i \\neq j\\)):</li> </ol> \\[ -\\frac{y_j}{S_j} \\cdot (- S_j S_i) = y_j S_i \\] <p>Summing over all \\(j\\), we get:</p> \\[ \\frac{\\partial L}{\\partial z_i} = - y_i (1 - S_i) + \\sum_{j \\neq i} y_j S_i \\] <p>Since \\(y\\) is a one-hot vector, only one \\(y_j = 1\\), and all others are 0, meaning:</p> \\[ \\frac{\\partial L}{\\partial z_i} = S_i - y_i \\] <p>Intuition Behind Cancellation</p> <p>Instead of explicitly computing the full Softmax Jacobian, the multiplication of the Cross-Entropy derivative and the Softmax Jacobian simplifies directly to \\(S - y\\).</p> <ul> <li>This happens because the off-diagonal terms in the Jacobian sum cancel out in the chain rule application.</li> <li>The result is a simple gradient computation without the need for the full Jacobian matrix.</li> </ul> <p>This is why, in backpropagation, the Softmax layer doesn't need to explicitly compute its Jacobian. Instead, we can directly use:</p> \\[\\frac{\\partial L}{\\partial z} = S - y\\] <p>to efficiently update the parameters in neural network training.</p> <p>Now we are ready to make implementation of the <code>CrossEntropyLoss</code> function with the <code>Softmax</code>:</p> <pre><code>class CrossEntropyLoss(Loss):\n    def __init__(self, eps: float = 1e-9):\n        \"\"\"\n        Cross-Entropy Loss function for multi-class classification.\n\n        This loss combines softmax activation and negative log-likelihood loss\n        for multi-class classification problems.\n\n        Args:\n            eps (float): Small constant to avoid numerical instability when\n                taking logarithms of values close to 0.\n        \"\"\"\n        super().__init__(reduction=\"mean\")\n        self.eps = eps\n\n    def compute_loss(self, prediction: Tensor, target: Tensor) -&gt; Tensor:\n        \"\"\"\n        Computes the Cross-Entropy Loss.\n\n        Args:\n            prediction (Tensor): The raw model outputs (logits).\n            target (Tensor): The ground truth labels (one-hot encoded or class indices).\n\n        Returns:\n            Tensor: The computed cross-entropy loss.\n        \"\"\"\n\n        # For numerical stability, subtract max value (doesn't change softmax result)\n        shifted = prediction - prediction.max(axis=-1, keepdims=True)\n\n        # Compute softmax probabilities: exp(x_i) / sum(exp(x_j))\n        exp_values = shifted.exp()\n        probabilities = exp_values / (exp_values.sum(axis=-1, keepdims=True) + self.eps)\n\n        # Compute Cross-Entropy\n        loss = -(target * (probabilities + self.eps).log()).sum(axis=-1)\n\n        return loss\n</code></pre> <p>But there is another way. The LogSumExp (LSE) function is defined as:  </p> \\[\\operatorname{LSE}(z) = \\log \\sum_{j} e^{z_j}\\] <p>For numerical stability, it's often rewritten as:  </p> \\[\\operatorname{LSE}(z) = \\max(z) + \\log \\sum_{j} e^{z_j - \\max(z)}\\] <p>Now we need to revisit the softmax function. It's defined as:</p> \\[S_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\] <p>If we take the logarithm of both sides:</p> \\[\\log S_i = \\log \\left( \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\right)\\] <p>Applying log rules - \\(\\log e^{z_i} = z_i\\) and \\(\\log \\frac{a}{b} = \\log a - \\log b\\):</p> \\[\\log S_i = z_i - \\log \\sum_j e^{z_j}\\] <p>This is exactly the key equation used in the LogSumExp (LSE) version:</p> \\[\\log S_i = z_i - \\operatorname{LSE}(z)\\] <p>So instead of computing Softmax first, then taking the log, we directly compute log-softmax in one step.</p> <p>To implement LogSumExp in the <code>Tensor</code> class, we simply define it using existing operations like <code>sum</code>, <code>squeeze</code>, <code>max</code>, <code>exp</code>, and <code>log</code>. Since these functions already have their <code>backward</code> steps implemented, we don't need to manually define backpropagation for <code>logsumexp</code>.  </p> <p>During the backward pass, each operation computes its gradient step-by-step, propagating derivatives automatically. Mathematically, computing the gradient separately for each step or treating the entire <code>logsumexp</code> function as a single operation gives the same result.</p> <pre><code>def logsumexp(\n    self,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False\n):\n    x_max = self.max(axis, keepdims)\n    shifted_exp = (self - x_max).exp()\n    sum_exp = shifted_exp.sum(axis, keepdims)\n    logsumexp = sum_exp.log() + x_max.squeeze(axis)\n\n    return Tensor(logsumexp, requires_grad=self.requires_grad)\n</code></pre> <p>And now we are ready to implement the <code>CrossEntropyLoss</code> with <code>logsumexp</code>:</p> <pre><code>class CrossEntropyLoss(Loss):\n    r\"\"\"\n    Cross-Entropy Loss function with Log-Softmax.\n\n    This loss is used for multi-class classification. Instead of computing\n    Softmax explicitly, it applies Log-Softmax internally for numerical stability.\n\n    Args:\n        eps (float): Small constant to prevent log(0) issues.\n\n    Inherits from:\n        Loss: The base loss class.\n    \"\"\"\n\n    def __init__(self, eps: float = 1e-9):\n        super().__init__(reduction=\"mean\")\n        self.eps = eps\n\n    def compute_loss(self, logits: Tensor, target: Tensor) -&gt; Tensor:\n        r\"\"\"\n        Compute Cross-Entropy Loss using Log-Softmax.\n\n        Instead of computing softmax explicitly, we use the identity:\n\n            log(Softmax(x)) = x - logsumexp(x)\n\n        This improves numerical stability and simplifies backpropagation.\n\n        Args:\n            logits (Tensor): The raw output logits (not probabilities).\n            target (Tensor): The true labels (one-hot encoded).\n\n        Returns:\n            Tensor: The computed Cross-Entropy loss.\n        \"\"\"\n\n        # Compute log-softmax in a numerically stable way\n        log_softmax = logits - logits.logsumexp(axis=-1, keepdims=True)\n\n        # Compute cross-entropy loss (negative log likelihood)\n        loss = -(target * log_softmax).sum(axis=-1)\n\n        return loss\n</code></pre> <p>The <code>compute_loss</code> method calculates the Cross-Entropy loss for each data point and loss is summed across the last axis (<code>axis=-1</code>) to account for all classes.</p>","tags":["Empirical Risk","Cross Entropy Loss","Softmax","Classification","Binary Classification","Loss Function Design","Deep Learning Fundamentals","Backpropagation","Training Loop"]},{"location":"2025/04/13/empirical-risk-and-cross-entropy-in-microtorch/#summary","title":"Summary","text":"<p>In this post, we dive into the concept of empirical risk and its relationship to loss functions in deep learning. We implement <code>CrossEntropyLoss</code> and <code>BCELoss</code> from scratch using the MicroTorch framework and explore the math behind expected risk, reduction strategies (<code>mean</code>, <code>sum</code>, <code>none</code>), and why the combination of Softmax + CrossEntropy simplifies backpropagation.</p>","tags":["Empirical Risk","Cross Entropy Loss","Softmax","Classification","Binary Classification","Loss Function Design","Deep Learning Fundamentals","Backpropagation","Training Loop"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/","title":"MicroTorch - Deep Learning from Scratch!","text":"<p>Implementing deep learning algorithms involves managing data flow in two directions: <code>forward</code> and <code>backward</code>. While the <code>forward</code> pass is typically straightforward, handling the <code>backward</code> pass can be more challenging. As discussed in previous posts, implementing backpropagation requires a strong grasp of calculus, and even minor mistakes can lead to significant issues.</p> <p>Fortunately, modern frameworks like PyTorch simplify this process with autograd, an automatic differentiation system that dynamically computes gradients during training. This eliminates the need for manually deriving and coding gradient calculations, making development more efficient and less error-prone.</p> <p>Now, let's build the backbone of such an algorithm - <code>Tensor</code> class!</p> <p></p> <p>Build an autograd!</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#check-the-jupyter-notebook","title":"Check the Jupyter Notebook","text":"<p>In the previous chapters, I built everything from the ground up. Now, we will create a <code>Tensor</code> object that abstracts the implementation of the backward steps for our building blocks.  </p> <p>Instead of manually coding the backward pass, we'll design a class that constructs a computation graph, tracking every operation within it. Once the graph is established, we will run the backward pass to compute gradients for all operations automatically.  </p> <p>Let's start by defining the <code>Tensor</code> class and initializing its basic methods.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#imports-and-type-definitions","title":"Imports and Type Definitions","text":"<p>Imports necessary libraries and defines type aliases for better readability.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Callable, List, Optional, Tuple, Union\nimport numpy as np\n\n# Scalar is a type alias for either an int or float.\nScalar = Union[int, float]\n\n# Data is a type alias for any valid input that can be converted into a Tensor \n# (e.g., scalars, lists, NumPy arrays, or other Tensor objects).\nData = Union[Scalar, list, np.ndarray, \"Tensor\"]\n</code></pre> <p>Example:</p> <pre><code>scalar_value: Scalar = 5.0\n\ndata_list: Data = [1, 2, 3]\ndata_np: Data = np.array([1, 2, 3])\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#computational-graph","title":"Computational graph","text":"<p>A computation graph is a directed graph where nodes represent operations (like addition or multiplication), and edges represent the flow of data (tensors). In the context of your <code>Tensor</code> class, each tensor is a node, and operations between tensors create edges between them.</p> <p>When you perform an operation, such as <code>Tensor A + Tensor B</code>, a new tensor is created, which records its dependencies on <code>A</code> and <code>B</code>. These dependencies are tracked in the <code>Tensor</code> object via the <code>dependencies</code> list. During the backward pass, the graph is traversed in reverse order to compute gradients for each tensor, starting from the final result back to the input tensors.</p> <p>By storing these dependencies in <code>Leaf</code> objects, the graph allows automatic differentiation, meaning gradients are computed for all involved tensors without manually specifying the backpropagation steps.</p> <p><code>Leaf</code> is a simple class used for storing the relationship between a <code>Tensor</code> (as value) and a function (<code>grad_fn</code>), which is responsible for computing the gradient for that <code>Tensor</code>. The <code>frozen=True</code> parameter makes the instance of the class immutable, meaning once created, its attributes cannot be changed.</p> <pre><code>@dataclass(frozen=True)\nclass Leaf:\n    value: \"Tensor\"\n    grad_fn: Callable[[np.ndarray], np.ndarray]\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#tensor-class","title":"Tensor Class","text":"<p>The Tensor class is the core of this implementation. It encapsulates data and provides functionality for automatic differentiation. The code bellow defines a simple <code>Tensor</code> class backbone. </p> <pre><code>class Tensor:\n    r\"\"\"\n    A class representing a multi-dimensional array (Tensor) with automatic differentiation support.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Data,\n        requires_grad: bool = False,\n        dependencies: Optional[List[Leaf]] = None,\n        dtype=np.float32\n    ) -&gt; None:\n        r\"\"\"\n        Initializes a Tensor object.\n\n        Args:\n            data (Data): The input data, which can be a scalar, list, NumPy array, or another Tensor.\n            requires_grad (bool, optional): If True, enables gradient tracking. Defaults to False.\n            dependencies (Optional[List[Leaf]], optional): Dependencies in the computation graph. Defaults to None.\n            dtype (dtype, optional): The data type of the Tensor. Defaults to np.float32.\n        \"\"\"\n\n        # data: The input data, which can be a scalar, list, NumPy array, or another Tensor.\n        self._data = Tensor.build_ndarray(data, dtype)\n        # dtype: The data type of the Tensor (default is np.float32)\n        self.dtype = self._data.dtype\n        # requires_grad: If True, the Tensor will track operations for gradient computation.\n        self.requires_grad = requires_grad\n        # dependencies: A list of Leaf objects representing dependencies in the computation graph.\n        self.dependencies: List[Leaf] = dependencies or []\n        self.grad: np.ndarray = None\n\n        # zero_grad(): Initializes the gradient to zero if requires_grad is True.\n        if self.requires_grad:\n            self.zero_grad()\n\n        ############################\n        # Properties of the Tensor # \n        ############################\n\n        # ndim: Returns the number of dimensions of the Tensor\n        @property\n        def ndim(self) -&gt; int:\n            r\"\"\"\n            Returns the number of dimensions of the Tensor.\n\n            Returns:\n                int: Number of dimensions.\n            \"\"\"\n\n            return self._data.ndim\n\n        # shape: Returns the shape of the Tensor\n        @property\n        def shape(self) -&gt; Tuple[int, ...]:\n            r\"\"\"\n            Returns the shape of the Tensor.\n\n            Returns:\n                Tuple[int, ...]: Shape of the Tensor.\n            \"\"\"\n\n            return self._data.shape\n\n        # size: Returns the total number of elements in the Tensor\n        @property\n        def size(self) -&gt; int:\n            r\"\"\"\n            Returns the total number of elements in the Tensor.\n\n            Returns:\n                int: Total number of elements.\n            \"\"\"\n\n            return self._data.size\n\n        # data: Gets or sets the underlying NumPy array\n        @property\n        def data(self) -&gt; np.ndarray:\n            r\"\"\"\n            Gets the underlying NumPy array.\n\n            Returns:\n                np.ndarray: The data stored in the Tensor.\n            \"\"\"\n\n            return self._data\n\n        @data.setter\n        def data(self, new_data: Data) -&gt; None:\n            r\"\"\"\n            Sets new data for the Tensor and resets gradients if required.\n\n            Args:\n                new_data (Data): The new data to be assigned to the Tensor.\n            \"\"\"\n\n            self._data = Tensor.build_ndarray(new_data, self.dtype)\n            if self.requires_grad:\n                self.zero_grad()\n\n        # String Representation: Provides a string representation of the Tensor\n        def __repr__(self) -&gt; str:\n            r\"\"\"\n            Returns a string representation of the Tensor.\n\n            Returns:\n                str: A string describing the Tensor.\n            \"\"\"\n\n            return f\"Tensor({self.data}, requires_grad={self.requires_grad}, shape={self.shape})\"\n\n        # Gradient Management - resets the gradient to zero\n        def zero_grad(self) -&gt; None:\n            r\"\"\"\n            Resets the gradient of the Tensor to zero.\n            \"\"\"\n\n            if self.grad is None:\n                self.grad = np.zeros_like(self.data, dtype=self.dtype)\n            else:\n                self.grad.fill(0.0)\n\n        ##################\n        # Static Methods #\n        ##################\n\n        # build_ndarray: Converts input data into a NumPy array.\n        @staticmethod\n        def build_ndarray(data: Data, dtype=np.float32) -&gt; np.ndarray:\n            r\"\"\"\n            Converts input data into a NumPy array.\n\n            Args:\n                data (Data): The input data which could be a Tensor, NumPy array, or a list.\n                dtype (dtype, optional): The target data type. Defaults to np.float32.\n\n            Returns:\n                np.ndarray: The converted NumPy array.\n            \"\"\"\n\n            if isinstance(data, Tensor):\n                return np.array(data.data, dtype=dtype)\n            if isinstance(data, np.ndarray):\n                return data.astype(dtype)\n            return np.array(data, dtype=dtype)\n</code></pre> <p>Example:</p> <pre><code>t = Tensor([1, 2, 3], requires_grad=True)\nt.zero_grad() # Resets the gradient to zero\nprint(t)  # Output: Tensor([1 2 3], requires_grad=True, shape=(3,))\n\nt = Tensor([[1, 2], [3, 4]])\nprint(t.shape)  # Output: (2, 2)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#first-operation-transpose","title":"First operation - Transpose","text":"<p>The backbone of the <code>Tensor</code> is basically useless - it just a mechanizm above the <code>numpy.array</code>. But now we can track the dependencies in the list and compute the gradient for the whole list of the dependencies!</p> <p>I can show you the example of the foundamental tensor operation - the <code>transpose</code> method. The <code>transpose</code> operation reorders the dimensions of a tensor. If no axes are specified, automatically reverses the dimensions (<code>[::-1]</code>). For example, given a 3D tensor (shape <code>[2, 3, 4]</code>):</p> <pre><code>X = np.random.randn(2, 3, 4)\nX.T.shape  # Output: (4, 3, 2)\n</code></pre> <p>The dimensions are flipped: <code>(2, 3, 4) \u2192 (4, 3, 2)</code>.</p> <p>However, if specific axes are provided, permutes the dimensions accordingly.</p> <pre><code>X.transpose((1, 0, 2))  # Changes order of dimensions\n</code></pre> <p>In forward pass, <code>np.transpose(self.data, axes=axes)</code> swaps the tensor dimensions. In backward pass, we must apply the inverse permutation to propagate gradients correctly. If <code>axes=None</code>, the gradient reverses dimensions back with the transpose operation (default case). If <code>axes</code> is provided (i.e., custom permutation), we must invert that permutation. <code>np.argsort(axes)</code> finds the inverse order to revert the transpose. For example, if we permute <code>(0,1,2) \u2192 (1,2,0)</code>, we need the inverse mapping to undo it:</p> <pre><code>axes = (1, 2, 0)   # Forward permutation\ninv_axes = np.argsort(axes)  # Output: (2, 0, 1)  \u2192 This restores original order\n</code></pre> <p>Implementation:</p> <pre><code>def transpose(self, axes: Tuple[int, ...] = None) -&gt; \"Tensor\":\n    # Perform the transpose operation\n    output = np.transpose(self.data, axes=axes)\n\n    # Handle dependencies for autograd\n    dependencies: List[Leaf] = []\n\n    if self.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            # Compute the inverse permutation of axes for the backward function\n            if axes is None:\n                # Implicitly reverses dimensions\n                return np.transpose(grad)  \n            else:\n                # Compute the inverse permutation of axes\n                inv_axes = tuple(np.argsort(axes))\n                # Transpose the gradient back using the inverse permutation\n                return np.transpose(grad, axes=inv_axes)\n\n        dependencies.append(\n            Leaf(value=self, grad_fn=_bkwd)\n        )\n\n    # Return the new tensor with the transposed data\n    return Tensor(\n        output,\n        requires_grad=self.requires_grad,\n        dependencies=dependencies\n    )\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#the-backward-method","title":"The <code>backward</code> Method","text":"<p>The <code>transpose</code> method is the first operation that the <code>Tensor</code> class supports. To fully showcase the power of our simple implementation, let's implement the <code>backward</code> method.</p> <p>The <code>backward</code> method implements reverse-mode automatic differentiation using the chain rule of calculus.</p> <p>Chain Rule:</p> \\[ \\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} \\] <p>If we have a function composition:  </p> \\[ f(x) = g(h(x)) \\] <p>Then, by the chain rule:</p> \\[ f'(x) = g'(h(x)) \\cdot h'(x) \\] <p>In the context of our <code>Tensor</code> class, this method is responsible for propagating gradients backward through a computation graph, ensuring that each node in the graph correctly accumulates its contribution to the final gradient.</p> <pre><code># Backward Propagation: Computes gradients using backpropagation\ndef backward(self, grad: Optional[np.ndarray] = None) -&gt; None:\n    # Step 1: Checking If Gradient Tracking is Enabled\n    if not self.requires_grad:\n        raise RuntimeError(\n            \"Cannot call backward() on a tensor that does not require gradients. \"\n            \"If you need gradients, ensure that requires_grad=True when creating the tensor.\"\n        )\n\n    # Step 2: Initializing the Gradient If Not Provided\n    if grad is None:\n        if self.shape == ():\n            # The gradient of a scalar itself is 1\n            grad = np.array(1.0)\n        else:\n            # If the tensor is not a scalar, `grad` must be provided explicitly.\n            raise ValueError(\"Grad must be provided if tensor has shape\")\n\n    # Step 3: Accumulating the Gradient\n    self.grad = self.grad + grad\n\n    # The Chain Rule in Action\n    for dependency in self.dependencies:\n        # Step 4: Applying the Chain Rule\n        # Propagates the gradient through the computation graph using `grad_fn`\n        backward_grad = dependency.grad_fn(grad)\n        # Step 5: Recursively Propagating Gradients\n        dependency.value.backward(backward_grad)\n</code></pre> <p>Step 3: Accumulating the Gradient</p> <pre><code>self.grad = self.grad + grad\n</code></pre> <p>Mathematically, this represents: \\(\\text{self.grad} \\gets \\text{self.grad} + \\text{grad}\\)</p> <p>The Chain Rule in Action. This part implements the chain rule:</p> <pre><code>for dependency in self.dependencies:\n    backward_grad = dependency.grad_fn(grad)\n    dependency.value.backward(backward_grad)\n</code></pre> <p>Step 4: Applying the Chain Rule</p> <p>For each dependency (i.e., an operation that contributed to this tensor), we compute the gradient contribution using the chain rule:</p> \\[\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\\] <p>where \\(dz/dy\\) is <code>grad</code> (gradient of the current tensor with respect to its output), \\(dy/dx\\) is <code>dependency.grad_fn</code> (gradient of the dependency with respect to its input).</p> <pre><code>backward_grad = dependency.grad_fn(grad)\n</code></pre> <ul> <li>This calls the stored gradient function (<code>grad_fn</code>) for this dependency.</li> <li>It effectively computes: \\(\\text{backward_grad} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\\)</li> </ul> <p>Step 5: Recursively Propagating Gradients</p> <pre><code>dependency.value.backward(backward_grad)\n</code></pre> <ul> <li>This recursively calls <code>backward</code> on the dependency.</li> <li>It ensures that gradients are propagated through the entire computation graph.</li> </ul> <p>Why Does This Work?</p> <p>When we compute gradients backward, we need to apply the chain rule in reverse order from the output back to the inputs.</p> <ul> <li>Forward Pass: Builds a directed acyclic graph (DAG) where each tensor stores dependencies (operations that produced it).</li> <li>Backward Pass: Uses recursive calls, which implicitly use a stack, ensuring the last dependency is processed first.</li> </ul> <p>This is crucial because the last computed tensor (final output, e.g., loss) is at the top of the graph. Gradients flow backward through dependencies (from output to input). Recursive calls unwind the computation graph in the correct order (LIFO - Last In, First Out).</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#parameter-class-foundation-for-neural-network-parameters","title":"Parameter Class: Foundation for Neural Network Parameters","text":"<p>The <code>Parameter</code> class handles the initialization and management of model parameters like weights and biases in neural networks. This class simplifies defining and managing weights and biases, ensuring efficient model optimization. It supports multiple initialization methods like \"xavier\", \"he\", \"normal\", \"uniform\" to set the right starting values, preventing issues like vanishing or exploding gradients. The class also ensures parameters are ready for optimization by setting <code>requires_grad=True</code> for backpropagation and includes a <code>gain</code> parameter to fine-tune initialization. </p> <pre><code>from typing import Any, Literal, Optional, Tuple\n\nimport numpy as np\n\nfrom au2grad.tensor import Tensor\n\ntype InitMethod = Literal[\"xavier\", \"he\", \"normal\", \"uniform\"]\n\n\nclass Parameter(Tensor):\n    r\"\"\"\n    Foundation for models parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        *shape: int,\n        data: Optional[np.ndarray] = None,\n        init_method: InitMethod = \"normal\",\n        gain: float = 1.0,\n        alpha: float = 0.01,\n    ) -&gt; None:\n        r\"\"\"\n        Initialize the parameter.\n\n        Args:\n            shape (tuple of int): The shape of the parameter.\n            data (np.ndarray, optional): The data of the parameter. If not \\\n                provided, the parameter is initialized using the initialization \\\n                method.\n            init_method (str): The initialization method. Defaults to 'normal'. \\\n                Possible values are 'xavier', 'he', 'normal', 'uniform'.\n            gain (float): The gain for the initialization method. Defaults to 1.0.\n            alpha (float): Slope for Leaky ReLU in \"he_leaky\" initialization.\n        \"\"\"\n\n        if data is None:\n            data = self._initialize(shape, init_method, gain, alpha)\n\n        super().__init__(data=data, requires_grad=True)\n\n    def _initialize(\n        self, shape: Tuple[int, ...], method: InitMethod | Any, gain: float, alpha: float\n    ) -&gt; np.ndarray:\n        r\"\"\"\n        Initialize the parameter data.\n        \"\"\"\n\n        weights = np.random.randn(*shape)\n\n        if init_method == \"xavier\":\n            std = gain * np.sqrt(1.0 / shape[0])\n            return std * weights\n        if init_method == \"he\":\n            std = gain * np.sqrt(2.0 / shape[0])\n            return std * weights\n        if init_method == \"he_leaky\":\n            std = gain * np.sqrt(2.0 / (1 + alpha**2) * (1 / shape[0]))\n            return std * weights\n        if init_method == \"normal\":\n            return gain * weights\n        if init_method == \"uniform\":\n            return gain * np.random.uniform(-1, 1, size=shape)\n\n        raise ValueError(f\"Unknown initialization method: {method}\")\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#module-class-base-for-all-neural-network-modules","title":"Module Class: Base for All Neural Network Modules","text":"<p>The <code>Module</code> class serves as the foundation for building neural network components, like layers and models. It defines essential methods like <code>forward</code>, which must be implemented in subclasses to process inputs and generate outputs. The class also provides functionality to switch between training (<code>train</code>) and evaluation (<code>eval</code>) modes, ensuring that all submodules are properly updated. </p> <p>The <code>parameters</code> method recursively collects all parameters from the module and its submodules, while <code>zero_grad</code> resets gradients for all parameters. The <code>params_count</code> method returns the total number of parameters in the module. </p> <p>In neural network development, the <code>Module</code> class simplifies handling the structure, state, and parameters of layers and models, making it easier to implement and train complex architectures.</p> <pre><code>from typing import Any\n\nclass Module:\n    r\"\"\"\n    Base class for all modules.\n    \"\"\"\n\n    def __call__(self, *args: Any) -&gt; Tensor:\n        return self.forward(*args)\n\n    def forward(self, *input: Any) -&gt; Tensor:\n        r\"\"\"\n        Forward method to be implemented in children class\n\n        Args:\n            input (Tensor or different object): Inputs\n\n        Returns:\n            Tensor: Outputs\n        \"\"\"\n        raise NotImplementedError()\n\n    def parameters(self) -&gt; List[Parameter]:\n        r\"\"\"\n        Returns:\n            List[Parameter]: Iterator of parameters\n        \"\"\"\n\n        params = []\n        for _, item in self.__dict__.items():\n            if isinstance(item, Parameter):\n                params.append(item)\n            elif isinstance(item, Module):\n                params.extend(item.parameters())\n        return params\n\n    def zero_grad(self) -&gt; None:\n        r\"\"\"\n        Zero the gradients of all parameters\n        \"\"\"\n\n        for param in self.parameters():\n            param.zero_grad()\n\n    def params_count(self) -&gt; int:\n        r\"\"\"\n        Returns:\n            int: Number of parameters\n        \"\"\"\n\n        num_parameters = sum(p.data.size for p in self.parameters())\n        return num_parameters\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#sequential-class-chaining-modules-in-order","title":"Sequential Class: Chaining Modules in Order","text":"<p>The <code>Sequential</code> class provides a simple way to stack multiple <code>Module</code> instances in a defined order. It automates the forward pass by passing the input tensor through each module sequentially, making it useful for building feedforward networks.  </p> <p>The <code>parameters</code> method collects all parameters from the contained modules, ensuring easy access for optimization. The <code>forward</code> method iterates through the sequence, applying each module to the input.  </p> <p>By structuring models in a linear fashion, <code>Sequential</code> simplifies neural network construction, reducing boilerplate and improving code clarity.</p> <pre><code>class Sequential(Module):\n    def __init__(self, *modules: Module):\n        self.modules = modules\n\n    def parameters(self) -&gt; List[Parameter]:\n        r\"\"\"\n        Returns a list of all parameters in the sequential module and its submodules.\n        \"\"\"\n        params = []\n        for module in self.modules:\n            params.extend(module.parameters())\n        return params\n\n    def forward(self, x):\n        r\"\"\"\n        Passes the input through all modules in sequence.\n        \"\"\"\n        for module in self.modules:\n            x = module(x)\n        return x\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#linear-layer-matrix-matrix-dot-product","title":"Linear Layer: Matrix-Matrix Dot Product","text":"<p>The mathematics behind the linear layer rely on matrix-matrix multiplication instead of vector operations. This allows efficient computation when processing multiple input samples simultaneously.</p> <p>At layer \\(i\\), the transformation is defined as:  </p> \\[\\tag{linear step} \\label{eq:linear_step} A_i(\\mathbf{X}) = \\mathbf{X} \\mathbf{W}_i + \\mathbf{B}_i\\] <p>Where \\(\\mathbf{X}\\) is the input matrix (batch of samples), \\(\\mathbf{W}_i\\) represents the weight matrix, and \\(\\mathbf{B}_i\\) is the bias matrix, typically broadcasted across the batch. The activation function \\(\\sigma\\) introduces non-linearity after this transformation.</p> <p>For a single layer:  </p> \\[F_i(\\mathbf{X}) = \\sigma(A_i(\\mathbf{X}))\\] <p>where \\(A_i(\\mathbf{X})\\) is the linear transformation at layer \\(i\\).  </p> <p>A deep neural network applies these transformations layer by layer, leading to the final output:  </p> \\[F(\\mathbf{X}) = \\sigma(A_L(\\sigma(A_{L-1}(\\dots \\sigma(A_1(\\mathbf{X})) \\dots )))\\] <p>Using functional composition, this process is compactly written as:  </p> \\[\\tag{deep neural net} \\label{eq:deep_nn} F(\\mathbf{X}) = A_L \\circ \\sigma \\circ A_{L-1} \\circ \\dots \\circ \\sigma \\circ A_1 (\\mathbf{X})\\] <p>The forward pass computes these transformations, storing intermediate values for the backward pass. We can implement the <code>Linear</code> layer's <code>forward</code> method directly based on these equations.</p> <p>The <code>tensor</code> implementation must support all necessary operations since it tracks dependencies within the gradient graph and accumulates gradients for the <code>backward</code> pass. This ensures automatic differentiation works seamlessly.</p> <p>For the <code>Linear</code> layer, we only need to implement the <code>forward</code> step, as all gradient computations are handled within the <code>tensor</code> itself. However, inside <code>tensor</code>, we must implement <code>backward</code> for every operation used in the <code>Linear</code> layer\u2019s <code>forward</code> step to enable proper gradient propagation during backpropagation.</p> <p>As the step number one, let's implement the matrix dot product inside the tensor class.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#dot-product","title":"Dot product","text":"<p>Matrix multiplication follows the chain rule during backpropagation. Let's break it down step by step.</p> <p>1. Forward Pass (MatMul Operation)</p> <p>Given two tensors A and B, matrix multiplication is:</p> \\[Z = A \\times B\\] <p>Where \\(A\\) has shape \\((m, n)\\), \\(B\\) has shape \\((n, p)\\) and the result \\(Z\\) has shape \\((m, p)\\). You can find more details here: Matrix Multiplication in Detail</p> <p>This is implemented in the forward pass:</p> <pre><code># Matrix multiplication\noutput = a.data @ b.data\n</code></pre> <p>2. Backward Pass (Gradients Computation)</p> <p>For backpropagation, we need to compute \\(\\frac{\\partial L}{\\partial A}\\) and \\(\\frac{\\partial L}{\\partial B}\\) using the chain rule.</p> <p>Gradient w.r.t. A The gradient of the loss \\(L\\) with respect to \\(A\\) is given by:</p> \\[\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial Z} \\times B^T\\] <p>Where: \\(\\frac{\\partial L}{\\partial Z}\\) is the incoming gradient (represented as <code>grad</code>) and \\(B^T\\) is the transpose of B.</p> <p>This is implemented as:</p> <pre><code>if a.requires_grad:\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        if b.ndim &gt; 1:\n            return grad @ b.data.swapaxes(-1, -2)  # grad * B^T\n        return np.outer(grad, b.data.T).squeeze()  # Handles 1D case\n</code></pre> <ul> <li>If \\(B\\) is 2D, we use <code>b.data.swapaxes(-1, -2)</code> to compute \\(B^T\\).</li> <li>If \\(B\\) is 1D, we use <code>np.outer(grad, b.data.T)</code> to ensure correct shape.</li> </ul> <p>Gradient w.r.t. B</p> <p>The gradient of the loss \\(L\\) with respect to \\(B\\) is given by:</p> \\[\\frac{\\partial L}{\\partial B} = A^T \\times \\frac{\\partial L}{\\partial Z}\\] <p>Where \\(A^T\\) is the transpose of A.</p> <p>This is implemented as:</p> <pre><code>if b.requires_grad:\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        if a.ndim &gt; 1:\n            return a.data.swapaxes(-1, -2) @ grad  # A^T * grad\n        return np.outer(a.data.T, grad).squeeze()  # Handles 1D case\n</code></pre> <ul> <li>If \\(A\\) is 2D, we use <code>a.data.swapaxes(-1, -2)</code> to compute \\(A^T\\).</li> <li>If \\(A\\) is 1D, we use <code>np.outer(a.data.T, grad)</code>.</li> </ul> <p>3. Why Do We Use <code>swapaxes(-1, -2)</code> Instead of <code>.T</code>?</p> <p><code>swapaxes(-1, -2)</code> is a general approach for transposing the last two dimensions. This ensures compatibility with both 2D matrices and higher-dimensional tensors (e.g., batches of matrices).</p> <ul> <li><code>.T</code> works only for 2D matrices, affecting all axes in higher dimensions.</li> <li><code>swapaxes(-1, -2)</code> preserves batch and other leading dimensions, modifying only the last two.</li> </ul> <p>Example:</p> Shape of Tensor <code>.T</code> Output <code>swapaxes(-1, -2)</code> Output <code>(m, n)</code> <code>(n, m)</code> <code>(n, m)</code> <code>(batch, m, n)</code> <code>(n, m, batch)</code> (incorrect) <code>(batch, n, m)</code> (correct) <code>(batch, time, m, n)</code> <code>(n, m, time, batch)</code> (incorrect) <code>(batch, time, n, m)</code> (correct) <p>4. How Does This Work in Backpropagation?</p> <ul> <li>During backpropagation, when a gradient flows back through the <code>matmul</code> operation, it needs to be properly propagated to both <code>A</code> and <code>B</code>.</li> <li>The gradient computation follows the chain rule and ensures that the gradients for both matrices are computed correctly.</li> </ul> <p>5. Summary</p> <p>Matrix multiplication follows the chain rule. The backward pass computes gradients for both \\(A\\) and \\(B\\) using transposes. Uses <code>swapaxes(-1, -2)</code> to generalize for higher-dimensional cases.</p> Tensor Gradient Formula Code Implementation \\(A\\) \\(\\frac{\\partial L}{\\partial A} = \\frac{\\partial L}{\\partial Z} \\times B^T\\) <code>grad @ b.data.swapaxes(-1, -2)</code> \\(B\\) \\(\\frac{\\partial L}{\\partial B} = A^T \\times \\frac{\\partial L}{\\partial Z}\\) <code>a.data.swapaxes(-1, -2) @ grad</code> <p>Implementation</p> <p>To perform matrix-matrix multiplication, we first implement the static method <code>matmul</code> in the <code>Tensor</code> class. This method computes the dot product of two matrices \\(A\\) and \\(B\\), tracks dependencies, and sets up gradient functions for backpropagation.</p> <pre><code>@staticmethod\ndef matmul(a: \"Tensor\", b: \"Tensor\") -&gt; \"Tensor\":\n    r\"\"\"\n    Static method to perform matrix multiplication of two tensors.\n\n    Args:\n        a (Tensor): First matrix.\n        b (Tensor): Second matrix.\n\n    Returns:\n        Tensor: Resulting tensor with tracked dependencies.\n    \"\"\"\n\n    output = a.data @ b.data\n    requires_grad = a.requires_grad or b.requires_grad\n    dependencies: List[Leaf] = []\n\n    if a.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Backward gradient function for matmul with respect to a.\n            \"\"\"\n\n            if b.ndim &gt; 1:\n                return grad @ b.data.swapaxes(-1, -2)\n            return np.outer(grad, b.data.T).squeeze()\n\n        dependencies.append(\n            Leaf(\n                value=a,\n                grad_fn=_bkwd\n            )\n        )\n\n    if b.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Backward gradient function for matmul with respect to b.\n            \"\"\"\n\n            if a.ndim &gt; 1:\n                return a.data.swapaxes(-1, -2) @ grad\n            return np.outer(a.data.T, grad).squeeze()\n\n        dependencies.append(\n            Leaf(\n                value=b,\n                grad_fn=_bkwd\n            )\n        )\n\n    return Tensor(output, requires_grad, dependencies)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#ensuring-data-consistency-with-data_gate","title":"Ensuring Data Consistency with <code>data_gate</code>","text":"<p>When performing matrix multiplication or other tensor operations, we must ensure that the data types are compatible. For example, attempting to multiply a <code>Tensor</code> with a <code>numpy.ndarray</code> directly may lead to unexpected behavior. To prevent such issues, we can create a <code>data_gate</code> method that automatically converts inputs to the <code>Tensor</code> type if they are not already.  </p> <pre><code>@staticmethod\ndef data_gate(data_object: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Ensures the input is a Tensor.\n\n    This method checks if the provided object is already a Tensor. \n    If not, it converts it into a Tensor before proceeding with operations.\n\n    Args:\n        data_object (Data): The input data, which can be a Tensor or a compatible type.\n\n    Returns:\n        Tensor: The input converted to a Tensor if necessary.\n    \"\"\"\n    if isinstance(data_object, Tensor):\n        return data_object  # Return as-is if already a Tensor\n    return Tensor(data_object)  # Convert to Tensor if not\n</code></pre> <p>This function acts as a safeguard, ensuring that all operations are performed with the correct data type. Simple but effective, preventing potential errors when working with mixed data types.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#matmul-operator","title":"Matmul operator <code>@</code>","text":"<p>Next, we define the dot method as the standard interface for matrix dot products. To enable the <code>@</code> operator for matrix multiplication, we overload the <code>__matmul__</code> method.</p> <pre><code>def dot(self, other: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Perform matrix dot product with another tensor or data.\n\n    Args:\n        other (Data): The other operand.\n\n    Returns:\n        Tensor: Result of the dot product.\n    \"\"\"\n\n    return Tensor.matmul(self, Tensor.data_gate(other))\n\n\ndef __matmul__(self, other: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Overload the `@` operator for matrix multiplication.\n\n    Args:\n        other (Data): The other operand.\n\n    Returns:\n        Tensor: Result of the matrix multiplication.\n    \"\"\"\n\n    return self.dot(other)\n</code></pre> <p>This implementation ensures that the <code>Tensor</code> class handles both forward and backward computations for matrix multiplication, integrating smoothly into the automatic differentiation framework.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#linear-layer-implementation","title":"Linear Layer Implementation","text":"<p>The <code>Linear</code> layer applies a linear transformation to the input tensor, mapping it from <code>in_features</code> to <code>out_features</code>. It consists of learnable weight parameters and an optional bias.  </p> <p>During initialization, the weights are assigned based on the specified <code>init_method</code>, and the bias is included if enabled. The <code>forward</code> method performs matrix multiplication between the input tensor and the weight matrix. If the bias is present, it is reshaped accordingly and added to the output.  </p> <p>To support both 2D and 3D inputs, the implementation ensures that matrix dimensions align properly before performing operations. The result is a transformed tensor, ready for further processing in the neural network.</p> <pre><code>class Linear(Module):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        init_method: InitMethod = \"xavier\",\n    ) -&gt; None:\n        super().__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n\n        self.weight = Parameter(out_features, in_features, init_method=init_method)\n        self.bias = Parameter(out_features, init_method=init_method) if bias else None\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        # Check dimensions of input tensors\n        assert x.ndim in (2, 3), f\"Input must be 2D or 3D Tensor! x.ndim={x.ndim}\"\n\n        # Check if the last dimension of input matches in_features\n        if x.shape[-1] != self.in_features:\n            raise ValueError(\n                f\"Last dimension of input: {x.shape[-1]} does not match in_features: {self.in_features}\"\n            )\n\n        # Compute matrix multiplication: x @ weight^T\n        output = x @ self.weight.T\n\n        # Add the bias directly. Broadcasting will handle it!\n        if self.bias is not None:\n            output = output + self.bias\n\n        return output\n</code></pre> <p>But here, we have the <code>+</code> operation: <code>output = output + self.bias</code>, which is not implemented inside the <code>Tensor</code> class. To make the <code>Linear</code> implementation work, we need to handling this operation correctly.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#broadcasting-in-backward-mode","title":"Broadcasting in backward mode","text":"<p>Gradients must be correctly propagated across dimensions that may differ between tensors. Broadcasting allows tensors of different shapes to interact, but when computing gradients, we need to handle these differences in dimensions.</p> <p>Operations like <code>+</code>, <code>-</code>, or <code>*</code> are straightforward to implement in the <code>forward</code> pass. However, during the <code>backward</code> pass, we must account for broadcasting rules when computing gradients. To handle this, we need to introduce an additional method in the <code>Tensor</code> class.</p> <p>Broadcasting is a method used by most scientific computing libraries like PyTorch or NumPy to handle operations between arrays of different shapes. Broadcasting Rules - when performing an operation, compare the dimensions from right to left side. If the dimensions do not match, the shape with a size of 1 is stretched to match the other shape.</p> <p>In our example, if we were to broadcast:</p> <ul> <li>Matrix <code>A</code> with shape <code>(3, 1)</code> (our <code>X * W</code> result)</li> <li>Matrix <code>B</code> with shape <code>(1, 4)</code> (our bias <code>B</code> expanded to match <code>X * W</code> for broadcasting)</li> </ul> <p>For the addition, we:</p> <ol> <li>Stretch <code>A</code> to match <code>B</code> by duplicating the column four times.</li> <li>Stretch <code>B</code> to match <code>A</code> by duplicating the row three times.</li> </ol> <p>Thus, both matrices would be aligned to have dimensions of <code>3x4</code>, allowing for element-wise addition.</p> <p>Let's see this in code using NumPy:</p> <pre><code>import numpy as np\n\n# Define array A with shape (3, 1)\nA = np.array([\n    [1],\n    [2],\n    [3],\n])\nprint(f\"Array A shape: {A.shape}\")\n\n# Define array B with shape (1, 4)\nB = np.array([\n    [1, 2, 3, 4],\n])\nprint(f\"Array B shape: {B.shape}\")\n\n# Perform broadcasting addition\nresult = A + B\n\nprint(\"A + B result: \")\nprint(result)\n\nprint(f\"Result of A + B shape: {result.shape}\")\n</code></pre> <p>Output:</p> <pre><code>Array A shape: (3, 1)\nArray B shape: (1, 4)\nA + B result: \n[[2 3 4 5]\n [3 4 5 6]\n [4 5 6 7]]\nResult of A + B shape: (3, 4)\n</code></pre> <p>Matrix multiplication example:</p> <pre><code># Broadcasting the same for the matrix multiplication\nmatmul = A @ B\nprint(f\"Matmul A @ B shape: {matmul.shape}\")\n\nprint(\"Matmul result: \")\nprint(matmul)\n</code></pre> <p>Output:</p> <pre><code>Matmul A @ B shape: (3, 4)\nMatmul result: \n[[ 1  2  3  4]\n [ 2  4  6  8]\n [ 3  6  9 12]]\n</code></pre> <p>The <code>bkwd_broadcast</code> method ensures gradients are correctly summed across broadcasted dimensions in <code>backward</code> mode. When tensors of different shapes interact, broadcasting aligns them by repeating elements. The method handles gradient propagation by summing over the extra dimensions created by broadcasting, ensuring consistency and preventing errors in gradient calculations. This is crucial for element-wise operations with mismatched tensor shapes, maintaining correct backpropagation.</p> <p>In Scenario 1, <code>b</code> has shape <code>(1,)</code>, meaning it was expanded to match both dimensions of <code>a</code>. The backward pass gives <code>grad_c</code> shape <code>(2,2)</code>, but <code>b</code> originally had no explicit dimensions. We sum over all extra axes <code>(0,1)</code> (<code>keepdims=False</code>) to return to shape <code>(1,)</code>.</p> <pre><code>a = np.array([[1, 2], \n              [3, 4]])  # Shape: (2, 2)\n\nb = np.array([10])      # Shape: (1,)  (Broadcasted across both axis)\n\nc = a + b\nprint(f\"c: {c}\")\n\ngrad_c = np.ones_like(c)\nprint(f\"grad_c: {grad_c}\")\n\n# Since `a` was not broadcasted, the gradient just passes through\ngrad_a = grad_c\nprint(f\"grad_a: {grad_a}\")\n\n# Since `b` was expanded to match both dimensions\n# We **sum over all extra axes `(0,1)`** (`keepdims=False`)\n# to return to shape `(1,)`.\ngrad_b = grad_c.sum(axis=(0, 1), keepdims=False)\nprint(f\"grad_b: {grad_b}\")\n</code></pre> <p>Output:</p> <pre><code>c: [[11 12]\n    [13 14]]\n\ngrad_c: [[1 1]\n         [1 1]]\n\ngrad_a: [[1 1]\n         [1 1]]\n\ngrad_b: 4\n</code></pre> <p>In Scenario 2, <code>b</code> has shape <code>(2,1)</code>, meaning it was broadcasted along axis <code>1</code> to match <code>a</code>'s shape <code>(2,2)</code>. During the backward pass, <code>grad_c</code> has shape <code>(2,2)</code>, so we sum over axis 1 (<code>keepdims=True</code>) to restore <code>b</code>'s original shape <code>(2,1)</code>.</p> <pre><code>a = np.array([[1, 2], \n              [3, 4]])      # Shape: (2, 2)\n\nb = np.array([[10], \n              [20]])        # Shape: (2, 1)  (Broadcasted across axis 1)\n\n# element-wise addition\nc = a + b                   # Shape: (2, 2) (Broadcasting rules)\nprint(f\"c: {c}\")\n\n# generate the initial gradient\n# Shape: (2, 2)\ngrad_c = np.ones_like(c)\nprint(f\"grad_c: {grad_c}\")\n\n# Since `a` was not broadcasted, the gradient just passes through\ngrad_a = grad_c\nprint(f\"grad_a: {grad_a}\")\n\n# Since `b` was **broadcasted along axis 1**, we must **sum** over \n# that axis to reduce it back to `b`'s original shape `(2,1)`\ngrad_b = grad_c.sum(axis=1, keepdims=True)\nprint(f\"grad_b: {grad_b}\")\n</code></pre> <p>Output:</p> <pre><code>c: [[11 12]\n    [23 24]]\n\ngrad_c: [[1 1]\n         [1 1]]\n\ngrad_a: [[1 1]\n         [1 1]]\n\ngrad_b: [[2]\n         [2]]\n</code></pre> <p>We need to compute gradients for A and B in the gradient tree.</p> <p>Since <code>a</code> was not broadcasted, the gradient just passes through:</p> <pre><code>grad_a = grad_c  # Same shape as a (2, 2)\n</code></pre> <p>Since <code>b</code> was broadcasted along axis 1, we must sum over that axis to reduce it back to <code>b</code>'s original shape <code>(2,1)</code>.  </p> <pre><code>grad_b = grad_c.sum(axis=1, keepdims=True)\n</code></pre> <p>The <code>bkwd_broadcast</code> function ensures that gradients are correctly summed over broadcasted dimensions during backpropagation. When an operation involves tensors of different shapes, broadcasting aligns them by expanding dimensions as needed. If extra dimensions were added during this process, they must be summed over in the backward pass to maintain consistency with the original tensor shape. In this case, since <code>B</code> was originally <code>(2,1)</code>, no additional dimensions were introduced (<code>ndim_added = 0</code>), so this step is skipped.  </p> <p>To correctly compute the gradient for <code>B</code>, we must sum over the broadcasted axis. Since <code>B</code> was expanded along axis <code>1</code> to match the shape of <code>A</code>, its corresponding gradient <code>grad_Z</code> retains this extra information across all columns. To revert the gradient to <code>B</code>\u2019s original shape <code>(2,1)</code>, we sum over axis <code>1</code>, ensuring that the total contribution from each row is preserved while eliminating the artificially expanded dimension.</p> <pre><code>@staticmethod\ndef bkwd_broadcast(tensor: \"Tensor\"):\n    r\"\"\"\n    Backward closure function to sum across broadcasted dimensions.\n\n    When performing operations between tensors of different shapes, broadcasting is used\n    to align their shapes. This function ensures that the gradients are correctly summed\n    over the broadcasted dimensions during the backward pass.\n\n    Args:\n        tensor (Tensor): The tensor involved in the operation, used to handle its shape\n                         during backward gradient computation.\n    Returns:\n        _bkwd (function): A function that computes the gradient, summing over broadcasted\n                          dimensions to match the original tensor's shape.\n    \"\"\"\n\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        # Handle scalar tensor case:\n        # Original tensor was a scalar: sum all gradients\n        if tensor.ndim == 0:\n            return np.sum(grad)\n\n        # Handle scalar grad case\n        if grad.ndim == 0:\n            return grad\n\n        # Calculate the number of dimensions *added* to the tensor to achieve\n        # the grad shape. This is where broadcasting might have \"prepended\"\n        # dimensions.\n        ndim_added = max(0, grad.ndim - tensor.ndim)\n\n        if ndim_added &gt; 0:\n            grad = grad.sum(axis=tuple(range(ndim_added)), keepdims=False)\n\n        # Sum over dimensions where tensor was broadcasted (size 1)\n        reduce_axes = tuple(\n            dim for dim in range(tensor.ndim)\n            if tensor.shape[dim] == 1 and grad.shape[dim] &gt; 1\n        )\n\n        if reduce_axes:\n            grad = grad.sum(axis=reduce_axes, keepdims=True)\n\n        # Ensure the final shape matches the tensor shape exactly\n        if grad.shape != tensor.shape:\n            grad = grad.reshape(tensor.shape)\n\n        return grad\n\n    return _bkwd\n</code></pre> <p>Handle Scalar Tensor Case</p> <p>If the original tensor is a scalar (<code>tensor.ndim == 0</code>), it means that during the forward pass, this scalar was broadcasted to match the shape of another tensor. To compute the gradient for a scalar tensor, we need to sum up all the gradients from the larger tensor (e.g., matrix or vector) because the scalar contributes to every element of the result.</p> <pre><code>if tensor.ndim == 0:\n    return np.sum(grad)\n</code></pre> <p>Handle Scalar Gradient Case</p> <p>If the gradient itself is a scalar (<code>grad.ndim == 0</code>), no broadcasting occurred during the forward pass. In this case, the gradient can be returned as-is because there are no dimensions to reduce.</p> <pre><code>if grad.ndim == 0:\n    return grad\n</code></pre> <p>Calculate Dimensions Added by Broadcasting</p> <p>During broadcasting, NumPy may prepend dimensions to the smaller tensor to align its shape with the larger tensor. For example: Forward shapes: <code>(3,) + (5, 3) -&gt; (5, 3)</code> - a new dimension is prepended to the first tensor. And we calculate in <code>ndim_added</code> how many such dimensions were added to the original tensor to match the gradient's shape.</p> <pre><code>ndim_added = max(0, grad.ndim - tensor.ndim)\n</code></pre> <p>Scenario 1 - Sum Over Added Dimensions: These are collapsed using <code>keepdims=False</code> because they don't exist in the original tensor.</p> <pre><code>if ndim_added &gt; 0:\n    grad = grad.sum(axis=tuple(range(ndim_added)), keepdims=False)\n</code></pre> <p>Scenario 2 - Sum Over Broadcasted Dimensions: These are summed while retaining their size as <code>1</code> using <code>keepdims=True</code> to preserve the original tensor's structure.</p> <pre><code>reduce_axes = tuple(\n    dim for dim in range(tensor.ndim)\n    if tensor.shape[dim] == 1 and grad.shape[dim] &gt; 1\n)\n\nif reduce_axes:\n    grad = grad.sum(axis=reduce_axes, keepdims=True)\n</code></pre> <p>Ensure Final Shape Matches: This is a safeguard to ensure that the gradient's shape exactly matches the original tensor's shape. While the previous steps should handle most cases, this ensures correctness in edge cases.</p> <pre><code>if grad.shape != tensor.shape:\n    grad = grad.reshape(tensor.shape)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#add-sub-and-their-friends","title":"<code>add</code>, <code>sub</code> and their friends","text":"<p>The first one is the <code>add</code> method, which will handle element-wise addition of two <code>Tensor</code> objects.</p> <p>The addition operation computes the element-wise sum of two tensors.</p> \\[f(a, b) = a + b\\] <p>The derivative of \\(a + b\\) with respect to \\(a\\) and \\(b\\) is 1:</p> \\[\\frac{d}{da} (a + b) = 1\\] \\[\\frac{d}{db} (a + b) = 1\\] <pre><code>@staticmethod\ndef add(a: \"Tensor\", b: \"Tensor\") -&gt; \"Tensor\":\n    r\"\"\"\n    Add two tensors and return a new tensor containing the result.\n\n    This method performs element-wise addition of two tensors, handling broadcasting \n    if necessary. If either tensor requires gradients, the resulting tensor will also \n    track gradients and backpropagate them correctly.\n\n    Args:\n        a (Tensor): The first tensor to be added.\n        b (Tensor): The second tensor to be added.\n\n    Returns:\n        Tensor: A new tensor that contains the element-wise sum of a and b.\n    \"\"\"\n\n    # Perform element-wise addition of the data of tensors a and b\n    output = a.data + b.data\n\n    # Determine if the result requires gradients (if any input tensor requires it)\n    requires_grad = a.requires_grad or b.requires_grad\n\n    # List to store dependencies (grad functions) for backpropagation\n    dependencies: List[Leaf] = []\n\n    # If tensor a requires gradients, add its gradient function to dependencies\n    # Apply bkwd_broadcast to the tensor a\n    if a.requires_grad:\n        dependencies.append(\n            Leaf(value=a, grad_fn=Tensor._bkwd_broadcast(a))\n        )\n\n    # If tensor b requires gradients, add its gradient function to dependencies\n    # Apply bkwd_broadcast to the tensor b\n    if b.requires_grad:\n        dependencies.append(\n            Leaf(value=b, grad_fn=Tensor._bkwd_broadcast(b))\n        )\n\n    # Return a new tensor with the result, gradient flag, and dependencies\n    return Tensor(output, requires_grad, dependencies)\n</code></pre> <p>Now, we are ready to overload the <code>+</code> and <code>-</code> operations! By implementing these operator overloads, we make tensor arithmetic more intuitive and user-friendly. </p> <p>Additionally, we implement in-place addition (<code>+=</code>) and subtraction (<code>-=</code>) to modify tensors directly without creating new ones. However, note that in-place operations do not track gradients for automatic differentiation.</p> <p>To simplify subtraction, we introduce the <code>__neg__</code> method (<code>-</code> operator), which multiplies the tensor by <code>-1</code>. This allows us to redefine subtraction as adding the negated tensor, replacing <code>a - b</code> with <code>a + (-b)</code>, keeping the logic clean and consistent.</p> <pre><code>def __add__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Overload the `+` operator to perform element-wise tensor addition.\n\n    Args:\n        other (Data): Another tensor or scalar to add.\n\n    Returns:\n        Tensor: The result of element-wise addition.\n    \"\"\"\n\n    return Tensor.add(self, Tensor.data_gate(other))\n\ndef __radd__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Overload the right-hand `+` operator (other + self).\n\n    Args:\n        other (Data): Another tensor or scalar to add.\n\n    Returns:\n        Tensor: The result of element-wise addition.\n    \"\"\"\n\n    return Tensor.add(Tensor.data_gate(other), self)\n\ndef __iadd__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Overload the `+=` operator for in-place addition.\n    WARNING: In-place operations do not track gradients!\n\n    Args:\n        other (Data): Another tensor or scalar to add in-place.\n\n    Returns:\n        Tensor: The updated tensor after in-place addition.\n    \"\"\"\n\n    self.data = self.data + Tensor.build_ndarray(other)\n    return self\n\ndef __neg__(self) -&gt; \"Tensor\":\n    \"\"\"\n    Overload the unary `-` operator to negate a tensor.\n    This allows defining subtraction as addition with negation.\n\n    Returns:\n        Tensor: The negated tensor (-self).\n    \"\"\"\n\n    output = -self.data\n    dependencies: List[Leaf] = []\n\n    # Define the backward function: gradient negation\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(value=self, grad_fn=lambda grad: -grad)\n        )\n\n    return Tensor(output, self.requires_grad, dependencies)\n\ndef __sub__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Overload the `-` operator for element-wise subtraction.\n    Uses addition with negation: a - b \u2192 a + (-b).\n\n    Args:\n        other (Data): Another tensor or scalar to subtract.\n\n    Returns:\n        Tensor: The result of element-wise subtraction.\n    \"\"\"\n\n    return self + (-Tensor.data_gate(other))\n\ndef __rsub__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Overload the right-hand `-` operator (other - self).\n    Uses addition with negation: b - a \u2192 b + (-a).\n\n    Args:\n        other (Data): Another tensor or scalar.\n\n    Returns:\n        Tensor: The result of element-wise subtraction.\n    \"\"\"\n\n    return Tensor.data_gate(other) + (-self)\n\ndef __isub__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Overload the `-=` operator for in-place subtraction.\n    WARNING: In-place operations do not track gradients!\n\n    Args:\n        other (Data): Another tensor or scalar to subtract in-place.\n\n    Returns:\n        Tensor: The updated tensor after in-place subtraction.\n    \"\"\"\n\n    self.data = self.data - Tensor.build_ndarray(other)\n    return self\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#mul","title":"<code>mul</code>","text":"<p>The multiplication operation computes the element-wise product of two tensors.</p> \\[f(a, b) = a \\cdot b\\] <p>The derivative of \\(a \\cdot b\\) with respect to \\(a\\) and \\(b\\) is:</p> \\[\\frac{d}{da} (a \\cdot b) = b\\] \\[\\frac{d}{db} (a \\cdot b) = a\\] <pre><code>@staticmethod\ndef mul(a: \"Tensor\", b: \"Tensor\") -&gt; \"Tensor\":\n    \"\"\"\n    Performs element-wise multiplication between two tensors and returns the result.\n    Handles tensors that require gradients by defining the backward pass for backpropagation.\n\n    Args:\n        a (Tensor): First tensor to be multiplied.\n        b (Tensor): Second tensor to be multiplied.\n\n    Returns:\n        Tensor: A new tensor containing the result of the element-wise multiplication.\n    \"\"\"\n    # Ensure both tensors contain their data correctly, handling any potential gates\n    a = Tensor.data_gate(a)\n    b = Tensor.data_gate(b)\n\n    # Perform element-wise multiplication on the tensor data\n    output = a.data * b.data\n\n    # Determine if the resulting tensor should require gradients\n    requires_grad = a.requires_grad or b.requires_grad\n    dependencies: List[Leaf] = []\n\n    # Define the backward pass function for multiplication\n    def _backward(a: Tensor, b: Tensor):\n        \"\"\"\n        Backward closure function for Mul operation.\n        Computes the gradient of the multiplication operation.\n        \"\"\"\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            \"\"\"\n            The gradient of the multiplication operation.\n            The gradient of a * b is grad * b for a and grad * a for b.\n            \"\"\"\n            # Multiply the gradient by tensor b's data for the gradient w.r.t a\n            grad = grad * b.data\n            # Ensure the gradient is properly reshaped using broadcasting\n            return Tensor._bkwd_broadcast(a)(grad)\n\n        return _bkwd\n\n    # If tensor a requires gradients, add the backward function to the dependencies\n    if a.requires_grad:\n        dependencies.append(\n            Leaf(\n                value=a,\n                grad_fn=_backward(a, b)  # Link tensor a's backward pass\n            )\n        )\n\n    # If tensor b requires gradients, add the backward function to the dependencies\n    if b.requires_grad:\n        dependencies.append(\n            Leaf(\n                value=b,\n                grad_fn=_backward(b, a)  # Link tensor b's backward pass\n            )\n        )\n\n    # Return the result as a new tensor, with the appropriate gradient information\n    return Tensor(output, requires_grad, dependencies)\n</code></pre> <p>Now, we are ready to overload the multiplication operators. This allows us to use <code>*</code> for element-wise multiplication of tensors, additionally, we implement in-place multiplication (<code>*=</code>), which modifies the tensor directly but does not support gradient tracking.  </p> <pre><code>def __mul__(self, other: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Overloads the `*` operator for element-wise multiplication.\n\n    This method ensures that Tensor multiplication can be performed seamlessly \n    with both other Tensors and scalar values.\n\n    Args:\n        other (Data): The other operand, which can be a Tensor or a compatible scalar.\n\n    Returns:\n        Tensor: A new Tensor representing the element-wise product.\n    \"\"\"\n\n    return Tensor.mul(self, Tensor.data_gate(other))\n\ndef __rmul__(self, other: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Overloads the right-hand `*` operator.\n\n    This ensures that multiplication works correctly when a scalar or another\n    compatible type appears on the left side of the `*` operator.\n\n    Args:\n        other (Data): The left-hand operand, which can be a scalar or Tensor.\n\n    Returns:\n        Tensor: A new Tensor representing the element-wise product.\n    \"\"\"\n\n    return Tensor.mul(Tensor.data_gate(other), self)\n\ndef __imul__(self, other: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Overloads the `*=` operator for in-place multiplication.\n\n    This modifies the Tensor\u2019s data directly, which improves efficiency.\n    However, in-place operations do not support automatic differentiation\n    (i.e., gradients will not be tracked).\n\n    Args:\n        other (Data): The operand to multiply with.\n\n    Returns:\n        Tensor: The modified Tensor after in-place multiplication.\n    \"\"\"\n\n    self.data = self.data * Tensor.build_ndarray(other)\n    return self\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#logs-exponents-and-activation-functions","title":"Logs, Exponents, and Activation Functions","text":"","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#log","title":"<code>log</code>","text":"<p>The logarithmic operation computes the natural logarithm of each element in the tensor.</p> \\[f(x) = \\log(x)\\] <p>The derivative of \\(\\log(x)\\) is:</p> \\[\\frac{d}{dx} \\log(x) = \\frac{1}{x}\\] <pre><code>def log(self) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the natural logarithm of all elements in the tensor.\n\n    The logarithm is applied element-wise to the tensor's data. This function assumes that \n    the data values are positive, as the logarithm of non-positive values is undefined.\n\n    Returns:\n        Tensor: A new tensor containing the element-wise natural logarithm of the input tensor.\n\n    The natural logarithm of a value `x` is calculated as:\n        log(x) = ln(x)\n\n    The derivative of log(x) with respect to x is:\n        d/dx log(x) = 1/x\n    \"\"\"\n\n    # Perform logarithmic operation on the data\n    output = np.log(self.data)\n\n    # Initialize an empty list for dependencies (used for backpropagation)\n    dependencies: List[Leaf] = []\n\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"\n        Backward pass for the logarithm operation.\n\n        The derivative of the logarithm is 1/x, so we compute the gradient as:\n            grad(x) = grad(x) / x\n\n        Args:\n            grad (np.ndarray): The gradient propagated from the next layer.\n\n        Returns:\n            np.ndarray: The gradient to propagate backward.\n        \"\"\"\n\n        # The derivative of log(x) is 1/x, so we divide the gradient by the data (x)\n        return grad / self.data\n\n    # If the tensor requires gradients (i.e., it's part of the computation graph), \n    # we store the backward function in the dependencies.\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(\n                value=self,\n                grad_fn=_bkwd\n            )\n        )\n\n    # Return a new Tensor containing the result of the log operation and the necessary dependencies.\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#tanh","title":"<code>tanh</code>","text":"<p>The tanh operation computes the hyperbolic tangent of each element in the tensor.</p> \\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\] <p>The derivative of \\(\\tanh(x)\\) is:</p> \\[\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)\\] <pre><code>def tanh(self) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the hyperbolic tangent (tanh) of all elements in the tensor.\n\n    The hyperbolic tangent function is applied element-wise to the tensor's data. The tanh \n    function maps the input values to the range (-1, 1).\n\n    Returns:\n        Tensor: A new tensor containing the element-wise hyperbolic tangent of the input tensor.\n\n    The hyperbolic tangent function is defined as:\n        tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n\n    The derivative of tanh(x) with respect to x is:\n        d/dx tanh(x) = 1 - tanh(x)^2\n    \"\"\"\n\n    # Perform hyperbolic tangent operation on the data\n    output = np.tanh(self.data)\n\n    # Initialize an empty list for dependencies (used for backpropagation)\n    dependencies: List[Leaf] = []\n\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Backward pass for the tanh operation.\n\n        The derivative of tanh(x) is 1 - tanh(x)^2, so we compute the gradient as:\n            grad(x) = grad(x) * (1 - tanh(x)^2)\n\n        Args:\n            grad (np.ndarray): The gradient propagated from the next layer.\n\n        Returns:\n            np.ndarray: The gradient to propagate backward.\n        \"\"\"\n\n        # The derivative of tanh(x) is 1 - tanh(x)^2, so we multiply the gradient by this value\n        return grad * (1 - output**2)\n\n    # If the tensor requires gradients (i.e., it's part of the computation graph), \n    # we store the backward function in the dependencies.\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(\n                value=self,\n                grad_fn=_bkwd\n            )\n        )\n\n    # Return a new Tensor containing the result of the tanh operation and the necessary dependencies.\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#pow","title":"<code>pow</code>","text":"<p>The power operation raises each element of the tensor to the specified power.</p> \\[f(x) = x^p\\] <p>The derivative of \\(x^p\\) is:</p> \\[\\frac{d}{dx} x^p = p \\cdot x^{p-1}\\] <pre><code>def pow(self, pow: Scalar) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the element-wise power of the tensor's data.\n\n    The operation applies the power function element-wise, raising each element in the tensor \n    to the given power `pow`.\n\n    Args:\n        pow (Scalar): The exponent to which each element in the tensor should be raised.\n\n    Returns:\n        Tensor: A new tensor where each element is raised to the specified power.\n\n    The power function is defined as:\n        y = x^pow, where `x` is the input tensor's element and `pow` is the given exponent.\n\n    The derivative of x^pow with respect to x is:\n        d/dx (x^pow) = pow * x^(pow - 1)\n    \"\"\"\n\n    # Perform element-wise power operation (raise each element to the given power)\n    output = self.data**pow\n\n    # Initialize an empty list for dependencies (used for backpropagation)\n    dependencies: List[Leaf] = []\n\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Backward pass for the power operation.\n\n        The derivative of x^pow with respect to x is:\n            d/dx (x^pow) = pow * x^(pow - 1)\n\n        We multiply the gradient by the derivative to propagate the gradient backward.\n\n        Args:\n            grad (np.ndarray): The gradient propagated from the next layer.\n\n        Returns:\n            np.ndarray: The gradient to propagate backward.\n        \"\"\"\n        # The derivative of x^pow is pow * x^(pow - 1), so we multiply the gradient by this value\n        return grad * (pow * (self.data**(pow - 1)))\n\n    # If the tensor requires gradients (i.e., it's part of the computation graph), \n    # we store the backward function in the dependencies.\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(\n                value=self,\n                grad_fn=_bkwd\n            )\n        )\n\n    # Return a new Tensor containing the result of the power operation and the necessary dependencies.\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>To enable exponentiation using the <code>**</code> operator, we overload the <code>__pow__</code> method. This allows us to perform element-wise power operations naturally, like:</p> <pre><code>tensor ** 2  # Equivalent to tensor.pow(2)\n</code></pre> <p>This's more intuitive syntax: <code>tensor ** 2</code> instead of <code>tensor.pow(2)</code>.</p> <p>Internally, <code>__pow__</code> simply calls the <code>pow</code> method, which handles the power operation while ensuring proper gradient computation.</p> <pre><code>def __pow__(self, pow: Scalar) -&gt; \"Tensor\":\n    \"\"\"\n    Overload the `**` operator for element-wise exponentiation.\n\n    Args:\n        pow (Scalar): The exponent to raise the tensor to.\n\n    Returns:\n        Tensor: A new tensor with each element raised to the given power.\n    \"\"\"\n\n    return self.pow(pow)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#division","title":"division","text":"<p>The last set of operator overloads covers division. These methods ensure that the <code>/</code> operator works naturally with Tensors and scalars, handling both standard and in-place division. Since division can be expressed as multiplication by the reciprocal, we reuse the <code>**</code> operator to compute the inverse.  </p> <pre><code>def __truediv__(self, other: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Overloads the `/` operator for element-wise division.\n\n    Instead of direct division, this method multiplies the Tensor by the\n    reciprocal of `other`, ensuring compatibility with automatic differentiation.\n\n    Args:\n        other (Data): The divisor, which can be a Tensor or a scalar.\n\n    Returns:\n        Tensor: A new Tensor representing the division result.\n    \"\"\"\n\n    other = Tensor.data_gate(other)\n    return self * (other**-1)\n\ndef __rtruediv__(self, other: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Overloads the right-hand `/` operator.\n\n    This ensures that division works correctly when a scalar or another\n    compatible type appears on the left side of the `/` operator.\n\n    Args:\n        other (Data): The numerator, which can be a scalar or Tensor.\n\n    Returns:\n        Tensor: A new Tensor representing the division result.\n    \"\"\"\n\n    other = Tensor.data_gate(other)\n    return other * (self**-1)\n\ndef __itruediv__(self, other: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Overloads the `/=` operator for in-place division.\n\n    This modifies the Tensor\u2019s data directly, improving efficiency.\n    However, in-place operations do not support automatic differentiation\n    (i.e., gradients will not be tracked).\n\n    Args:\n        other (Data): The divisor for in-place division.\n\n    Returns:\n        Tensor: The modified Tensor after in-place division.\n    \"\"\"\n\n    self.data = self.data / Tensor.build_ndarray(other)\n    return self\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#exp","title":"<code>exp</code>","text":"<p>The exponential operation computes the exponent (base \\(e\\)) of each element in the tensor.</p> \\[\\exp(x) = e^x\\] <p>The derivative of \\(e^x\\) is:</p> \\[\\frac{d}{dx} e^x = e^x\\] <pre><code>def exp(self) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the element-wise exponential of the tensor's data.\n\n    The operation applies the exponential function element-wise, raising the constant e \n    (Euler's number) to the power of each element in the tensor.\n\n    Returns:\n        Tensor: A new tensor where each element is the exponential of the corresponding element \n        in the input tensor.\n\n    The exponential function is defined as:\n        y = e^x, where `e` is Euler's number (approximately 2.71828), and `x` is the input tensor's element.\n\n    The derivative of e^x with respect to x is:\n        d/dx (e^x) = e^x\n    \"\"\"\n\n    # Perform element-wise exponential operation (raise e to the power of each element)\n    output = np.exp(self.data)\n\n    # Initialize an empty list for dependencies (used for backpropagation)\n    dependencies: List[Leaf] = []\n\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Backward pass for the exponential operation.\n\n        The derivative of e^x with respect to x is:\n            d/dx (e^x) = e^x\n\n        We multiply the gradient by e^x to propagate the gradient backward.\n\n        Args:\n            grad (np.ndarray): The gradient propagated from the next layer.\n\n        Returns:\n            np.ndarray: The gradient to propagate backward.\n        \"\"\"\n        # The derivative of e^x is e^x, so we multiply the gradient by the output value\n        return grad * output\n\n    # If the tensor requires gradients (i.e., it's part of the computation graph), \n    # we store the backward function in the dependencies.\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(\n                value=self,\n                grad_fn=_bkwd\n            )\n        )\n\n    # Return a new Tensor containing the result of the exponential operation and the necessary dependencies.\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#simple-activation-functions","title":"Simple activation functions","text":"<p>Because we prepared the derivative computations inside the <code>Tensor</code> class we don't need anything but the <code>forward</code> implemenatation for the activation functions! It would be a great example of the autogradient power.</p> <pre><code>class Tanh(Module):\n    def forward(self, input: Tensor) -&gt; Tensor:\n        return input.tanh()\n\nclass Sigmoid(Module):\n    def forward(self, input: Tensor) -&gt; Tensor:\n        self.output = 1 / (1 + Tensor.exp(-input))\n        return self.output\n</code></pre> <p>The <code>backward</code> step is fully on the <code>Tensor</code> class side. It's beautiful, isn't it?</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#more-ops","title":"More Ops","text":"<p>In this section, we extend the functionality of the <code>Tensor</code> class by adding more fundamental operations. These operations are essential for building neural networks. For each operation, we provide the reasoning behind it, the derivative (for backpropagation), and the implementation of the forward and backward passes.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#squeeze-and-unsqueeze-operations","title":"Squeeze and Unsqueeze Operations","text":"<p>The <code>squeeze</code> and <code>unsqueeze</code> methods simplify reshaping operations, making it easier to adjust tensor shapes. These methods modify tensor dimensions by removing or adding singleton dimensions while preserving gradient tracking.</p> <p>The <code>squeeze</code> method removes dimensions of size 1 from a specified axis, and its backward function ensures gradients are expanded back to their original shape during backpropagation. If <code>axis</code> is specified we only remove certain singleton dimensions, so during the backward pass, we must restore those specific dimensions. If <code>axis=None</code> we reshape the gradient to the original tensor's shape.</p> <pre><code>def squeeze(self, axis: Union[int, Tuple[int], None] = None) -&gt; \"Tensor\":\n    r\"\"\"\n    Removes dimensions of size 1 from the specified axis.\n\n    Args:\n        dim (int or Tuple[int]): The axis or axes to squeeze. Defaults to None\n\n    Returns:\n        Tensor: The squeezed tensor with tracked dependencies.\n    \"\"\"\n\n    output = np.squeeze(self.data, axis=dim)\n    dependencies: List[Leaf] = []\n\n    if self.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Backward function for squeeze operation.\n\n            Args:\n                grad (np.ndarray): Incoming gradient.\n\n            Returns:\n                np.ndarray: Expanded gradient to match original dimensions.\n            \"\"\"\n\n            if axis is None:\n                # Reshape the gradient to the original tensor's shape\n                return grad.reshape(self.shape)\n            return np.expand_dims(grad, axis=axis)\n\n        dependencies.append(\n            Leaf(value=self, grad_fn=_bkwd)\n        )\n\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>The <code>unsqueeze</code> method adds a singleton dimension at the specified axis. The backward function removes this dimension during gradient propagation.</p> <pre><code>def unsqueeze(self, dim: int) -&gt; \"Tensor\":\n    r\"\"\"\n    Adds a singleton dimension at the specified axis.\n\n    Args:\n        dim (Tuple[int]): The axis at which to insert a new dimension\n\n    Returns:\n        Tensor: The unsqueezed tensor with tracked dependencies.\n    \"\"\"\n\n    output = np.expand_dims(self.data, axis=dim)\n    dependencies: List[Leaf] = []\n\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"\n        Backward function for unsqueeze operation.\n\n        Args:\n            grad (np.ndarray): Incoming gradient.\n\n        Returns:\n            np.ndarray: Squeezed gradient to remove added dimension.\n        \"\"\"\n\n        return np.squeeze(grad, axis=dim)  # Correctly squeeze the axis\n\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(value=self, grad_fn=_bkwd)\n        )\n\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>These methods ensure proper gradient tracking while reshaping tensors, making them useful in various neural network operations.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#view","title":"<code>view</code>","text":"<p>The <code>view</code> method allows a <code>tensor</code> to be reshaped without copying the underlying data, provided the new shape is compatible with the original memory layout. This operation is efficient because it does not involve new memory allocation</p> <pre><code>def view(self, shape: Tuple[int, ...]) -&gt; \"Tensor\":\n    r\"\"\"\n    Reshape the tensor without changing its underlying data.\n\n    This method returns a new `Tensor` object that shares the same data but is represented \n    with a different shape. The operation is efficient as it does not involve memory reallocation.\n\n    Args:\n        shape (Tuple[int, ...]): The desired shape of the tensor.\n\n    Returns:\n        Tensor: A new tensor with the same data but a different shape.\n\n    Example:\n        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6]]), requires_grad=True)\n        &gt;&gt;&gt; y = x.view((3, 2))\n        &gt;&gt;&gt; print(y)\n        Tensor([[1, 2],\n                [3, 4],\n                [5, 6]], requires_grad=True, shape=(3,2))\n\n    Notes:\n        - The returned tensor shares the same data as the original tensor.\n        - If `requires_grad=True`, the backward function ensures gradients are reshaped correctly.\n    \"\"\"\n\n    # This will be a new view object if possible; otherwise, it will be a copy\n    output = self.data.reshape(shape)\n    dependencies: List[Leaf] = []\n\n    if self.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Backward pass for tensor view operation.\n\n            This function ensures that the incoming gradient is reshaped \n            back to the original shape of the tensor.\n\n            Args:\n                grad (np.ndarray): The incoming gradient.\n\n            Returns:\n                np.ndarray: The gradient reshaped to match the original tensor.\n            \"\"\"\n\n            return grad.reshape(self.shape)\n\n        dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n\n    return Tensor(output, self.requires_grad, dependencies, dtype=self.dtype)\n</code></pre> <p>Example Usage:</p> <pre><code># Create a 2x3 tensor\nx = Tensor(np.array([[1, 2, 3], [4, 5, 6]]), requires_grad=True)\n\n# Reshape into a 3x2 tensor\ny = x.view((3, 2))\n\n# Print reshaped tensor\nprint(y)\n# Output:\n# Tensor([[1, 2],\n#         [3, 4],\n#         [5, 6]], requires_grad=True, shape=(3,2))\n\n# Backward pass\ny.backward(Tensor(np.ones((3, 2))))  # Gradient of ones\n\n# Check gradients\nprint(x.grad)\n# Output should be reshaped correctly to match original shape:\n# [[1. 1. 1.]\n#  [1. 1. 1.]]\n</code></pre> <p>Example with backward:</p> <pre><code>a = Tensor([[1, 2],\n            [3, 4]], requires_grad=True)\n\n# a.view((4, 1)).unsqueeze(0).squeeze(0).backward(np.ones_like(a.data))\n# a.view((4, 1)).unsqueeze(1).squeeze(1).unsqueeze(0).squeeze(0).backward(np.ones_like(a.data))\n\nb = a.view((1, 4)).unsqueeze(1).squeeze(1).unsqueeze(0).squeeze(0)\nb.backward(np.ones_like(b.data))\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#clip","title":"<code>clip</code>","text":"<p>The <code>clip()</code> method allows you to restrict the values of a tensor to a specific range. This is particularly useful in deep learning, where you may want to avoid exploding or vanishing gradients by limiting the range of tensor values during backpropagation. The <code>clip()</code> method ensures that all values of the tensor remain within a given minimum and maximum value.</p> <p>This method clips (limits) the values of a tensor to a specified range, <code>[min_value, max_value]</code>. Any values below <code>min_value</code> are set to <code>min_value</code>, and any values above <code>max_value</code> are set to <code>max_value</code>. If either bound is not specified (i.e., None), no limit is applied on that side.</p> <p>Gradient Behavior: when <code>requires_grad=True</code>, this method tracks the gradient of the clipping operation. For values that fall within the clipping range, the gradient is passed through unchanged. For values outside the clipping range, the gradient is set to zero because the clipping operation is not differentiable at the boundaries.</p> <pre><code>def clip(self, min_value: Optional[float] = None, max_value: Optional[float] = None) -&gt; \"Tensor\":\n    r\"\"\"\n    Clip the tensor's values to the range [min_value, max_value].\n\n    Args:\n        min_value (Optional[float]): The minimum value to clip to. If None, no lower bound is applied.\n        max_value (Optional[float]): The maximum value to clip to. If None, no upper bound is applied.\n\n    Returns:\n        Tensor: A new tensor with values clipped to the specified range.\n    \"\"\"\n\n    # Perform clipping on the data\n    output = np.clip(self.data, min_value, max_value)\n\n    # Track dependencies if requires_grad is True\n    dependencies: List[Leaf] = []\n\n    if self.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Backward function for the clip operation.\n\n            The gradient is passed through for values within the range [min_value, max_value].\n            For values outside this range, the gradient is zero because the operation is not differentiable\n            at the boundaries.\n\n            Args:\n                grad (np.ndarray): The gradient passed from the downstream operation.\n\n            Returns:\n                np.ndarray: The gradient for the input tensor.\n            \"\"\"\n\n            # Create a mask for values within the clipping range\n            mask = np.ones_like(self.data)\n\n            # Apply the mask\n            if min_value is not None:\n                mask[self.data &lt;= min_value] = 0\n            if max_value is not None:\n                mask[self.data &gt;= max_value] = 0\n\n            # Multiply the gradient by the mask\n            return grad * mask\n\n        dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n\n    # Return a new tensor with the clipped values and dependencies\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>Example:</p> <pre><code># Example 1: Clipping tensor values to a range [0, 1]\na = Tensor([[0.5, -0.3], [1.2, 2.0]], requires_grad=True)\nclipped_a = a.clip(min_value=0.0, max_value=1.0)\n\nprint(\"Original Tensor:\\n\", a.data)\nprint(\"Clipped Tensor:\\n\", clipped_a.data)\n\n# Perform backward pass to test gradients\ngrad_output = np.ones_like(a.data)\nclipped_a.backward(grad_output)\nprint(\"Gradients:\\n\", a.grad)\n</code></pre> <p>Output:</p> <pre><code>Original Tensor:\n [[ 0.5 -0.3]\n [ 1.2  2. ]]\nClipped Tensor:\n [[0.5 0. ]\n [1.  1. ]]\nGradients:\n [[1. 0.]\n [1. 0.]]\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#where","title":"<code>where</code>","text":"<p>The <code>where</code> method is a powerful element-wise selection operation for tensors, similar to NumPy's <code>np.where</code>. It allows conditional selection of elements from two tensors (<code>a</code> and <code>b</code>) based on a boolean condition. This is particularly useful in deep learning for masking values, implementing conditional operations, and handling gradients properly in automatic differentiation.</p> <p>This function ensures that during backpropagation, gradients are propagated only to the selected elements of <code>a</code> and <code>b</code>, preventing unnecessary computations.</p> <p>The comparison operators allow us to create boolean tensors by comparing tensor values element-wise. These operators are essential for conditional operations like <code>where</code>.</p> <pre><code># Comparison Operators\ndef __lt__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Less than operator (&lt;).\n\n    Creates a boolean tensor where each element is True if the corresponding\n    element in self is less than the corresponding element in other.\n\n    Args:\n        other (Data): Value to compare against. Can be a Tensor or a compatible data type.\n\n    Returns:\n        Tensor: A boolean tensor with the comparison results.\n    \"\"\"\n\n    other = Tensor.data_gate(other)\n    return Tensor(self.data &lt; other.data)\n\ndef __gt__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Greater than operator (&gt;).\n\n    Creates a boolean tensor where each element is True if the corresponding\n    element in self is greater than the corresponding element in other.\n\n    Args:\n        other (Data): Value to compare against. Can be a Tensor or a compatible data type.\n\n    Returns:\n        Tensor: A boolean tensor with the comparison results.\n    \"\"\"\n\n    other = Tensor.data_gate(other)\n    return Tensor(self.data &gt; other.data)\n\ndef __eq__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Equal to operator (==).\n\n    Creates a boolean tensor where each element is True if the corresponding\n    element in self is equal to the corresponding element in other.\n\n    Args:\n        other (Data): Value to compare against. Can be a Tensor or a compatible data type.\n\n    Returns:\n        Tensor: A boolean tensor with the comparison results.\n    \"\"\"\n\n    other = Tensor.data_gate(other)\n    return Tensor(self.data == other.data)\n\ndef __le__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Less than or equal to operator (&lt;=).\n\n    Creates a boolean tensor where each element is True if the corresponding\n    element in self is less than or equal to the corresponding element in other.\n\n    Args:\n        other (Data): Value to compare against. Can be a Tensor or a compatible data type.\n\n    Returns:\n        Tensor: A boolean tensor with the comparison results.\n    \"\"\"\n\n    other = Tensor.data_gate(other)\n    return Tensor(self.data &lt;= other.data)\n\ndef __ge__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Greater than or equal to operator (&gt;=).\n\n    Creates a boolean tensor where each element is True if the corresponding\n    element in self is greater than or equal to the corresponding element in other.\n\n    Args:\n        other (Data): Value to compare against. Can be a Tensor or a compatible data type.\n\n    Returns:\n        Tensor: A boolean tensor with the comparison results.\n    \"\"\"\n\n    other = Tensor.data_gate(other)\n    return Tensor(self.data &gt;= other.data)\n\ndef __ne__(self, other: Data) -&gt; \"Tensor\":\n    \"\"\"\n    Not equal to operator (!=).\n\n    Creates a boolean tensor where each element is True if the corresponding\n    element in self is not equal to the corresponding element in other.\n\n    Args:\n        other (Data): Value to compare against. Can be a Tensor or a compatible data type.\n\n    Returns:\n        Tensor: A boolean tensor with the comparison results.\n    \"\"\"\n\n    other = Tensor.data_gate(other)\n    return Tensor(self.data != other.data)\n</code></pre> <p>Example:</p> <pre><code># Create two tensors\na = Tensor(np.array([1, 2, 3, 4]))\nb = Tensor(np.array([2, 2, 0, 5]))\n\n# Compare the tensors\nresult_lt = a &lt; b\nprint(result_lt.data)\n# Output: [True False False True]\n\nresult_eq = a == b\nprint(result_eq.data)\n# Output: [False True False False]\n\n# Use with where operation\nc = Tensor.where(a &gt; b, a, b)\nprint(c.data)\n# Output: [2 2 3 5]\n</code></pre> <p>The <code>where</code> method executes the main logic of selecting between two tensors <code>a</code> and <code>b</code> based on a condition. Gradient for <code>a</code>: If a value from tensor <code>a</code> was chosen (when <code>condition == True</code>), the gradient should flow to <code>a</code>. Otherwise, the gradient for <code>a</code> is zero.</p> <pre><code>def _bkwd_a(grad: np.ndarray) -&gt; np.ndarray:\n    return np.where(condition.data, grad, 0.0)\n</code></pre> <p><code>np.where(condition.data, grad, 0.0)</code> means: pass the gradient to <code>a</code> where the condition is <code>True</code> and zero out the gradient where the condition is <code>False</code>.</p> <p>Gradient for <code>b</code>: If a value from tensor <code>b</code> was chosen (when <code>condition == False</code>), the gradient should flow to <code>b</code>. Otherwise, the gradient for <code>b</code> is zero.</p> <pre><code>def _bkwd_b(grad: np.ndarray) -&gt; np.ndarray:\n    return np.where(condition.data, 0.0, grad)\n</code></pre> <p>Implementation:</p> <pre><code>@staticmethod\ndef where(condition: \"Tensor\", a: \"Tensor\", b: \"Tensor\") -&gt; \"Tensor\":\n    r\"\"\"\n    Performs element-wise selection based on a condition.\n\n    This function returns a tensor where each element is taken from `a` if the corresponding \n    element in `condition` is True, otherwise from `b`. It supports automatic differentiation.\n\n    Args:\n        condition (Tensor): A boolean tensor where True selects from `a`, and False selects from `b`.\n        a (Tensor): The tensor providing values where `condition` is True.\n        b (Tensor): The tensor providing values where `condition` is False.\n\n    Returns:\n        Tensor: A tensor with elements from `a` where `condition` is True, and from `b` otherwise.\n\n    Example:\n        &gt;&gt;&gt; cond = Tensor(np.array([[True, False], [False, True]]))\n        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), requires_grad=True)\n        &gt;&gt;&gt; y = Tensor(np.array([[10, 20], [30, 40]]), requires_grad=True)\n        &gt;&gt;&gt; result = Tensor.where(cond, x, y)\n        &gt;&gt;&gt; print(result)\n        Tensor([[ 1, 20],\n                [30,  4]], requires_grad=True)\n\n    Notes:\n        - The returned tensor has `requires_grad=True` if either `a` or `b` requires gradients.\n        - The backward function ensures gradients are passed only to the selected elements.\n    \"\"\"\n\n    output = np.where(condition.data, a.data, b.data)  # Element-wise selection\n    requires_grad = a.requires_grad or b.requires_grad\n    dependencies: List[Leaf] = []\n\n    if a.requires_grad:\n        def _bkwd_a(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Backward function for `a`.\n\n            This ensures gradients flow only to the elements selected from `a`.\n\n            Args:\n                grad (np.ndarray): Gradient of the output tensor.\n\n            Returns:\n                np.ndarray: Gradient for `a`, masked where `condition` is False.\n            \"\"\"\n            return np.where(condition.data, grad, 0.0)\n\n        dependencies.append(Leaf(a, _bkwd_a))\n\n    if b.requires_grad:\n        def _bkwd_b(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Backward function for `b`.\n\n            This ensures gradients flow only to the elements selected from `b`.\n\n            Args:\n                grad (np.ndarray): Gradient of the output tensor.\n\n            Returns:\n                np.ndarray: Gradient for `b`, masked where `condition` is True.\n            \"\"\"\n            return np.where(condition.data, 0.0, grad)\n\n        dependencies.append(Leaf(b, _bkwd_b))\n\n    return Tensor(output, requires_grad, dependencies)\n</code></pre> <p>Example</p> <pre><code># Define a condition tensor\ncondition = Tensor(np.array([[True, False], [False, True]]))\n\n# Define input tensors\nx = Tensor(np.array([[1, 2], [3, 4]]), requires_grad=True)\ny = Tensor(np.array([[10, 20], [30, 40]]), requires_grad=True)\n\n# Apply the where function\nresult = Tensor.where(condition, x, y)\n\n# Print result\nprint(result)\n# Output:\n# Tensor([[ 1, 20],\n#         [30,  4]], requires_grad=True)\n\n# Backward pass\ngrad_output = Tensor(np.array([[1.0, 1.0], [1.0, 1.0]]))\nresult.backward(grad_output)\n\n# Check gradients\nprint(x.grad)  # Should have gradients only where condition is True\n# [[1. 0.]\n#  [0. 1.]]\n\nprint(y.grad)  # Should have gradients only where condition is False\n# [[0. 1.]\n#  [1. 0.]]\n\n# You can use the conditional operation\n# Create two tensors\na = Tensor(np.array([1, 2, 3, 4]))\nb = Tensor(np.array([2, 2, 0, 5]))\n\n# Compare the tensors using a conditional operation\nresult_lt = a &lt; b\nprint(result_lt.data)\n# Output: [True False False True]\n\n# Use the `where` operation to select values based on the comparison result\nresult = Tensor.where(result_lt, a, b)\nprint(result.data)\n# Output: [1 2 3 5]\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#expanding-where-more-useful-tensor-operations","title":"Expanding <code>where</code> - More Useful Tensor Operations","text":"<p>With the static method <code>where</code>, we can implement several useful tensor operations like <code>maximum</code> and <code>minimum</code>.</p> <pre><code>@staticmethod\ndef maximum(a: Data, b: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Apply element-wise max operation: max(a: \"Tensor\", b: \"Tensor\") -&gt; \"Tensor\"\n    Returns a Tensor with the result of element-wise maximum.\n    \"\"\"\n\n    a, b = Tensor.data_gate(a), Tensor.data_gate(b)\n\n    return Tensor.where(a &gt; b, a, b)\n\n@staticmethod\ndef minimum(a: Data, b: Data) -&gt; \"Tensor\":\n    r\"\"\"\n    Apply element-wise min operation: min(a: \"Tensor\", b: \"Tensor\") -&gt; \"Tensor\"\n    Returns a Tensor with the result of element-wise minimum.\n    \"\"\"\n\n    a, b = Tensor.data_gate(a), Tensor.data_gate(b)\n\n    return Tensor.where(a &lt; b, a, b)\n</code></pre> <p>These implementations are clean and use where for efficient, element-wise comparisons. We can push this further and implement advanced operations in a concise way. For example - the <code>threshold</code> method sets values above a given threshold while leaving others unchanged.</p> <pre><code>def threshold(self, threshold: float, value: float) -&gt; \"Tensor\":\n    return Tensor.where(self &gt; threshold, self, Tensor(value))\n</code></pre> <p>The <code>masked_fill</code> method replaces values based on a boolean mask.</p> <pre><code>def masked_fill(self, mask: \"Tensor\", value: float) -&gt; \"Tensor\":\n    return Tensor.where(mask, Tensor(value), self)\n</code></pre> <p>The <code>sign</code> method returns the sign of each element in the tensor</p> <pre><code>def sign(self) -&gt; \"Tensor\":\n    return Tensor.where(\n        self &gt; 0, Tensor(1),\n        Tensor.where(self &lt; 0, Tensor(-1), Tensor(0))\n    )\n</code></pre> <p>We can also rewrite the <code>clip</code> method using where. This simplifies the code by handling both the forward and backward passes through the <code>where</code> implementation.</p> <pre><code>def clip(self, min_value: Optional[float] = None, max_value: Optional[float] = None) -&gt; \"Tensor\":\n    return Tensor.where(\n        self &lt; min_value, Tensor(min_value),\n        Tensor.where(self &gt; max_value, Tensor(max_value), self)\n    )\n</code></pre> <p>By using <code>where</code>, we reduce code duplication and ensure consistency across these tensor operations.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#indexing","title":"Indexing","text":"<p>Indexing in the <code>Tensor</code> class allows selecting specific elements from the tensor using another tensor or a NumPy array as an index. This operation is useful for extracting sub-tensors or performing advanced slicing operations.</p> <p>When <code>requires_grad=True</code>, the backward pass ensures that gradients are propagated correctly by constructing a zero tensor and filling only the indexed positions with the incoming gradient.</p> <p>Valid Python index types:</p> <ul> <li><code>int</code> (for scalar indexing)</li> <li><code>slice</code> (for range selection like x[1:3])</li> <li><code>Tuple</code> (for multi-dimensional indexing)</li> <li><code>List</code> (for list-based indexing)</li> </ul> <p>Also we can include the <code>np.ndarray</code> and <code>Tensor</code> for flexibility! </p> <pre><code>IndexType = Union[int, slice, Tuple, List[int], np.ndarray, \"Tensor\"]\n\ndef __getitem__(self, index: IndexType) -&gt; \"Tensor\":\n    r\"\"\"\n    Perform indexing on the tensor.\n\n    This method allows for selecting specific elements from the tensor using \n    another tensor or a NumPy array as an index.\n\n    Args:\n        index (IndexType): The indices to select from the tensor.\n\n    Returns:\n        Tensor: A new tensor containing the selected elements.\n\n    Example:\n        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6]], requires_grad=True)\n        &gt;&gt;&gt; idx = Tensor([0, 2])\n        &gt;&gt;&gt; y = x[0, idx]  # Selects elements [1, 3]\n        &gt;&gt;&gt; print(y)\n        Tensor([1, 3], requires_grad=True, shape=(2,))\n\n    Notes:\n        - If `requires_grad=True`, the backward function ensures that the gradient \n          is only applied to the indexed positions.\n    \"\"\"\n\n    # Normalize tensor-based indexing to numpy arrays\n    if isinstance(index, (Tensor, np.ndarray)):\n        index = Tensor.data_gate(index).data\n\n    output = self.data[index]  # Perform indexing operation\n    dependencies = []\n\n    if self.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Backward pass for tensor indexing.\n\n            This function constructs a zero tensor of the same shape as the original \n            tensor and assigns the incoming gradient only to the indexed positions.\n\n            Args:\n                grad (np.ndarray): Gradient from the next layer.\n\n            Returns:\n                np.ndarray: Gradient propagated back to the indexed positions.\n            \"\"\"\n\n            full_grad = np.zeros_like(self.data)\n            # Handle multiple uses of the same index correctly\n            np.add.at(full_grad, index, grad)\n            return full_grad\n\n        dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>Example Usage:</p> <pre><code># Create a tensor\nx = Tensor([[10, 20, 30], [40, 50, 60]], requires_grad=True)\n\n# Indexing operation\nidx = Tensor([0, 2])  # Selecting elements at index 0 and 2\ny = x[0, idx]  # Retrieves [10, 30]\n\n# Print result\nprint(y)  # Tensor([10, 30], requires_grad=True, shape=(2,))\n\n# Backward pass\ny.backward(Tensor([1.0, 1.0]))  # Set gradient to ones\n\n# Check gradients\nprint(x.grad)  \n# Output should propagate gradients only to the selected positions:\n# [[1.  0.  1.]\n#  [0.  0.  0.]]\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#more-tensor-ops","title":"More Tensor ops","text":"","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#abs","title":"<code>abs</code>","text":"<p>The absolute value operation computes the magnitude of each element in a tensor, disregarding the sign. </p> \\[\\text{abs}(x) = |x|\\] <p>The derivative of \\(\\text{abs}(x)\\) is:</p> \\[\\frac{d}{dx} |x| = \\text{sgn}(x)\\] <p>where the sign function \\(\\text{sgn}(x)\\) is defined as:</p> \\[\\text{sgn}(x) = \\begin{cases}   1, &amp; \\text{if } x &gt; 0 \\\\   -1, &amp; \\text{if } x &lt; 0 \\\\   0, &amp; \\text{if } x = 0 \\end{cases}\\] <p>Python Code for <code>abs</code> Operation with the Derivative</p> <pre><code>def abs(self) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the absolute value of the tensor's elements.\n\n    The absolute value of each element is computed element-wise, and the result is returned as a new tensor.\n\n    Returns:\n        Tensor: A new tensor containing the absolute values of the input tensor's elements.\n\n    The derivative of the absolute value function is handled in the backward pass using the sign of the input tensor.\n    The gradient for positive values is 1, for negative values is -1, and the gradient is undefined at zero.\n    \"\"\"\n\n    # Perform absolute value operation on the data\n    output = np.abs(self.data)\n\n    # Initialize the list of dependencies for gradient calculation\n    dependencies: List[Leaf] = []\n\n    # Backward function to compute the gradient for the absolute value operation\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"\n        Compute the gradient of the absolute value operation.\n\n        Args:\n            grad (np.ndarray): The gradient passed from the downstream operation.\n\n        Returns:\n            np.ndarray: The gradient for the input tensor.\n\n        The gradient of abs(x) is the sign of x:\n        - If x &gt; 0, the gradient is 1.\n        - If x &lt; 0, the gradient is -1.\n        - The gradient is undefined at x = 0.\n        \"\"\"\n\n        # The derivative of abs(x) is the sign of x: 1 for positive x, -1 for negative x\n        return grad * np.sign(self.data)\n\n    # If the tensor requires gradients, add the backward function to the dependencies list\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(\n                value=self,  # The input tensor\n                grad_fn=_bkwd  # The backward function to compute the gradients\n            )\n        )\n\n    # Return a new tensor containing the absolute values, with the gradient dependencies if needed\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>We can also leverage <code>where</code> to implement the <code>abs</code> method. This approach removes the need for an explicit backward implementation since <code>where</code> already handles it internally. Here's a clean, one-liner implementation:</p> <pre><code>def abs(self) -&gt; \"Tensor\":\n    return Tensor.where(self &gt;= 0, self, -self)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#max","title":"<code>max</code>","text":"<p>The max operation returns the maximum value of a tensor along a specified axis. If no axis is specified, it returns the maximum value from the entire tensor.</p> <p>For differentiation, the gradient of the maximum function is defined as:</p> \\[\\frac{d}{dx} \\max(X) = \\begin{cases}   1, &amp; \\text{if } x \\text{ is the maximum value} \\\\   0, &amp; \\text{otherwise} \\end{cases}\\] <p>During backpropagation, only the maximum value(s) receive a gradient, while all other elements receive 0. But things are more complicated and to deconvolve here are the general steps for backward calculation:</p> <ul> <li> <p>Identify which values are the maximum across the specified axis (or globally).</p> </li> <li> <p>Create a mask that indicates which elements of the original tensor were involved in the maximum value (i.e., the <code>1</code>'s).</p> </li> <li> <p>Distribute gradients only to those maximum values, propagating the gradient from downstream only to the maximum values.</p> </li> <li> <p>Handle multiple maxima (if two values are the same and are both maximum, gradients are split equally between them).</p> </li> </ul> <p>For the forward pass it's pretty obvious:</p> <pre><code>def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -&gt; \"Tensor\":\n    # Calculate the maximum values in forward mode\n    output = np.max(self.data, axis=axis, keepdims=keepdims)\n\n    dependencies = []\n\n    # backward is here...\n\n    # Return a new tensor containing the absolute values, with the gradient dependencies if needed\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>And for the backward let's take an approach where we work through each test case, identify what it requires from the <code>max</code> method, and implement the corresponding logic step by step.</p> <p>Case 1: Simple case - max with no axis (scalar output)</p> <pre><code>a1 = Tensor([1, 2, 3, 4], requires_grad=True)\nb1 = a1.max()  # Should be 4\n\nb1.backward()\nprint(a1.grad)  # Expected: [0, 0, 0, 1]\n</code></pre> <p>During backpropagation, assign a gradient of 1 to the position of the maximum values by identifying the position of the max value using a mask and distributing the incoming gradient across the max values.</p> <pre><code>if self.requires_grad:\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        # Create a mask where max values are True\n        mask = (self.data == output)\n        return mask * grad\n\n    dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n</code></pre> <p>The gradient should match the expected value: <code>[0, 0, 0, 1]</code>. </p> <p>Case 2: Max with Duplicate Maximums</p> <pre><code>a2 = Tensor([1, 4, 4, 2], requires_grad=True)\nb2 = a2.max()\nb2.backward()\nprint(a2.grad)  # Expected: [0, 0.5, 0.5, 0]\n</code></pre> <p>When there are multiple maximum values, we need to split the gradient equally among them. We handle the backward pass by identifying the position of the max value using a mask and distributing the incoming gradient across the max values.</p> <pre><code># Normalize by the number of max occurrences\ncount = np.sum(mask)\n# Split the gradient equally\nreturn mask * (grad / count)\n</code></pre> <p>Case 3: Max Along a Specific Axis like <code>axis=0</code></p> <pre><code>a3 = Tensor([[1, 5],\n             [5, 4]], requires_grad=True)\nb3 = a3.max(axis=0)  # Should be [5, 5]\nb3.backward(np.array([1, 1]))\nprint(a3.grad)  # Expected: [[0, 1], [1, 0]]\n</code></pre> <p>Compute the maximum along a specified axis and expand the gradient correctly along that axis during backpropagation.</p> <p>We update the <code>_bkwd</code> function:</p> <pre><code>if axis is None:\n    # For flattened tensor max, just count total max elements\n    count = np.sum(mask)\n    return mask * (grad / count)\n\n# Count max occurrences\ncount = np.sum(mask, axis=axis, keepdims=True)\ngrad_expanded = np.expand_dims(grad, axis=axis)\n\n# Normalize and Apply gradient\nreturn mask * (grad_expanded / count)\n</code></pre> <p>Case 4: Handling <code>keepdims=True</code></p> <p>We use <code>keepdims</code> to ensure the gradient shape aligns with the input tensor during backpropagation. </p> <pre><code>a4 = Tensor([[1, 5],\n             [5, 4]], requires_grad=True)\nb4 = a4.max(axis=0, keepdims=True)\nb4.backward(np.ones_like(b4.data))\n\nprint(a4.grad)  # Expected: [[0, 1], [1, 0]]\n</code></pre> <p>If <code>keepdims=True</code> the output already has the correct shape, so no expansion is needed. This preserves the dimensions and ensures the gradient is applied correctly. If <code>keepdims=False</code> the output is reduced along the specified axis, so we must expand the gradient to match the original shape for proper broadcasting during multiplication.</p> <pre><code>grad_expanded = grad if keepdims else np.expand_dims(grad, axis=axis)\n</code></pre> <p>Or we can create a short version:</p> <pre><code>count = np.sum(mask) if axis is None \\\n    else np.sum(mask, axis=axis, keepdims=True)\n\ngrad_expanded = grad if keepdims or axis is None \\\n    else np.expand_dims(grad, axis=axis)\n\nreturn mask * (grad_expanded / count)\n</code></pre> <p>Full implementation:</p> <pre><code>def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the maximum value along the specified axis.\n\n    This function returns the maximum value(s) of the tensor, either element-wise (if no axis is specified) \n    or along a given axis. The backward pass ensures that only the maximum elements receive gradients.\n\n    Args:\n        axis (Optional[Union[int, Tuple[int]]]): The axis along which to compute the maximum.\n            If None, the maximum of the entire tensor is returned.\n        keepdims (bool): If True, retains reduced dimensions with size 1.\n\n    Returns:\n        Tensor: A new tensor containing the maximum values along the given axis.\n\n    The gradient is computed during backpropagation by assigning a gradient of 1 \n    to the maximum element(s) and 0 elsewhere.\n    \"\"\"\n\n    # Calculate the maximum values\n    output = np.max(self.data, axis=axis, keepdims=keepdims)\n\n    dependencies = []\n\n    if self.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            # Handle multi-dimensional case\n            output_expanded = output if keepdims or axis is None else np.expand_dims(output, axis=axis)\n            # Create a mask where only the max values are True\n            mask = (self.data == output_expanded)\n\n            if axis is None:\n                # For flattened tensor max, just count total max elements\n                count = np.sum(mask)\n                return mask * (grad / count)\n\n            # Count max occurrences\n            count = np.sum(mask, axis=axis, keepdims=True)\n            grad_expanded = grad if keepdims else np.expand_dims(grad, axis=axis)\n\n            # Normalize and Apply gradient\n            return mask * (grad_expanded / count)\n\n        dependencies.append(Leaf(value=self, grad_fn=_bkwd))\n\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>Example:</p> <pre><code>print(\"============ Testing max backward ============\")\n\n# Test 1: Simple case - max with no axis (scalar result)\nprint(\"\\nTest 1: max with no axis (scalar output)\")\na1 = Tensor([1, 2, 3, 4], requires_grad=True)\nb1 = a1.max()  # Should be 4\nb1.backward()\nprint(f\"Input tensor: {a1.data}\")\nprint(f\"Max value: {b1.data}\")\nprint(f\"Gradient: {a1.grad}\")\n# Expected: Only the position with max value (4) should have gradient 1, others 0\nexpected_grad1 = np.array([0, 0, 0, 1])\nprint(f\"Test passed: {np.allclose(a1.grad, expected_grad1)}\")\n\n# Test 2: Max with duplicate maximum values\nprint(\"\\nTest 2: max with duplicate maximums\")\na2 = Tensor([1, 4, 4, 2], requires_grad=True)\nb2 = a2.max()\nb2.backward()\nprint(f\"Input tensor: {a2.data}\")\nprint(f\"Max value: {b2.data}\")\nprint(f\"Gradient: {a2.grad}\")\n# Expected: Positions with max value (4) should have gradient 0.5 each (1/count)\nexpected_grad2 = np.array([0, 0.5, 0.5, 0])\nprint(f\"Test passed: {np.allclose(a2.grad, expected_grad2)}\")\n\n# Test 3: Max along a specific axis\nprint(\"\\nTest 3: max along axis=0\")\na3 = Tensor([[1, 5],\n                [5, 4]], requires_grad=True)\nb3 = a3.max(axis=0)  # Should be [5, 5]\nb3.backward(np.array([1, 1]))\nprint(f\"Input tensor:\\n{a3.data}\")\nprint(f\"Max value: {b3.data}\")\nprint(f\"Gradient:\\n{a3.grad}\")\n# Expected: First column: [0, 1], Second column: [1, 0]\nexpected_grad3 = np.array([[0, 1], [1, 0]])\nprint(f\"Test passed: {np.allclose(a3.grad, expected_grad3)}\")\n\n# Test 4: Max along axis with keepdims=True\nprint(\"\\nTest 4: max along axis=0, keepdims=True\")\na4 = Tensor([[1, 5],\n                [5, 4]], requires_grad=True)\nb4 = a4.max(axis=0, keepdims=True)\nb4.backward(np.ones_like(b4.data))\nprint(f\"Input tensor:\\n{a4.data}\")\nprint(f\"Max value:\\n{b4.data}\")\nprint(f\"Gradient:\\n{a4.grad}\")\nexpected_grad4 = np.array([[0, 1], [1, 0]])\nprint(f\"Test passed: {np.allclose(a4.grad, expected_grad4)}\")\n\n# Test 5: Max along axis with duplicate maximums\nprint(\"\\nTest 5: max along axis with duplicate maximums\")\na5 = Tensor([[5, 3],\n                [5, 4]], requires_grad=True)\nb5 = a5.max(axis=0)  # Should be [5, 4]\nb5.backward(np.array([1, 1]))\nprint(f\"Input tensor:\\n{a5.data}\")\nprint(f\"Max value: {b5.data}\")\nprint(f\"Gradient:\\n{a5.grad}\")\n# Expected: First column has two 5s, so grad = [0.5, 0.5], Second column: [0, 1]\nexpected_grad5 = np.array([[0.5, 0], [0.5, 1]])\nprint(f\"Test passed: {np.allclose(a5.grad, expected_grad5)}\")\n\n# Test 6: Max over multiple dimensions\nprint(\"\\nTest 6: max over multiple dimensions\")\na6 = Tensor([[[1, 2], [3, 4]],\n                [[5, 6], [7, 8]]], requires_grad=True)\nb6 = a6.max()  # Should be 8\nb6.backward()\nprint(f\"Input tensor shape: {a6.shape}\")\nprint(f\"Max value: {b6.data}\")\nprint(f\"Gradient at max position: {a6.grad[1,1,1]}\")\n# Only position of max value (8) should have gradient = 1\nexpected_position = (1, 1, 1)  # In 0-indexed, (1,1,1) is the last element\nexpected_grad6 = np.zeros((2, 2, 2))\nexpected_grad6[expected_position] = 1\nprint(f\"Test passed: {np.allclose(a6.grad, expected_grad6)}\")\n\n# Test 7: Another axis test with 3D tensor\nprint(\"\\nTest 7: max on axis in 3D tensor\")\na7 = Tensor([[[1, 2], [3, 4]],\n                [[5, 6], [7, 8]]], requires_grad=True)\nb7 = a7.max(axis=1)  # Max along middle dimension\nprint(f\"Input tensor shape: {a7.shape}\")\nprint(f\"Output shape: {b7.shape}\")\nprint(f\"Output data: \\n{b7.data}\")\nb7.backward(np.ones_like(b7.data))\nprint(f\"Gradient: \\n{a7.grad}\")\nexpected_grad7 = np.array([[[0, 0], [1, 1]],\n                            [[0, 0], [1, 1]]])\nprint(f\"Test passed: {np.allclose(a7.grad, expected_grad7)}\")\n\n# Return overall test result\nprint(\"\\n============ Summary ============\")\nprint(\"All tests should pass if each element of the gradient is correct.\")\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#min","title":"<code>min</code>","text":"<p>The min operation returns the minimum value of a tensor along a specified axis. If no axis is specified, it returns the minimum value from the entire tensor.</p> <p>For differentiation, the gradient of the minimum function is defined as:</p> \\[\\frac{d}{dx} \\min(X) = \\begin{cases} 1, &amp; \\text{if } x \\text{ is the minimum value} \\\\ 0, &amp; \\text{otherwise} \\end{cases}\\] <p>The gradient is the same as for the <code>max</code> operation, we can create a <code>bkwd</code> method, that can serve for both methods:</p> <pre><code>def bkwd_minmax(\n    self,\n    output: np.ndarray,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False\n) -&gt; np.ndarray:\n    def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n        count = np.sum(mask) if axis is None \\\n            else np.sum(mask, axis=axis, keepdims=True)\n\n        grad_expanded = grad if keepdims or axis is None \\\n            else np.expand_dims(grad, axis=axis)\n\n        return mask * (grad_expanded / count)\n\n    return _bkwd\n</code></pre> <p>Then we can make refactoring for the <code>max</code>:</p> <pre><code>def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the maximum value along the specified axis.\n\n    This function returns the maximum value(s) of the tensor, either element-wise (if no axis is specified) \n    or along a given axis. The backward pass ensures that only the maximum elements receive gradients.\n\n    Args:\n        axis (Optional[Union[int, Tuple[int]]]): The axis along which to compute the maximum.\n            If None, the maximum of the entire tensor is returned.\n        keepdims (bool): If True, retains reduced dimensions with size 1.\n\n    Returns:\n        Tensor: A new tensor containing the maximum values along the given axis.\n\n    The gradient is computed during backpropagation by assigning a gradient of 1 \n    to the maximum element(s) and 0 elsewhere.\n    \"\"\"\n\n    # Calculate the maximum values\n    output = np.max(self.data, axis=axis, keepdims=keepdims)\n\n    dependencies = []\n\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(value=self, grad_fn=self.bkwd_minmax(output, axis, keepdims))\n        )\n\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>And now we can implement <code>min</code>, the only difference lay in the forward step, calculate the minimum values for the output:</p> <pre><code>def min(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the minimum value along the specified axis.\n\n    This function returns the minimum value(s) of the tensor, either element-wise (if no axis is specified) \n    or along a given axis. The backward pass ensures that only the minimum elements receive gradients.\n\n    Args:\n        axis (Optional[int]): The axis along which to compute the minimum.\n            If None, the minimum of the entire tensor is returned.\n        keepdims (bool): If True, retains reduced dimensions with size 1.\n\n    Returns:\n        Tensor: A new tensor containing the minimum values along the given axis.\n\n    The gradient is computed during backpropagation by assigning a gradient of 1 \n    to the minimum element(s) and 0 elsewhere.\n    \"\"\"\n\n    # Calculate the minimum values\n    output = np.min(self.data, axis=axis, keepdims=keepdims)\n\n    dependencies = []\n\n    if self.requires_grad:\n        dependencies.append(\n            Leaf(value=self, grad_fn=self.bkwd_minmax(output, axis, keepdims))\n        )\n\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>Example:</p> <pre><code># Create a tensor\nx = Tensor(np.array([[1, 3, 5], [2, 8, 4]]), requires_grad=True)\n\n# Compute the minimum along axis 1\nresult = x.min(axis=1, keepdims=True)\n\n# Print the result\nprint(result)\n# Output:\n# Tensor([[1],\n#         [2]], requires_grad=True)\n\n# Backward pass\ngrad_output = Tensor(np.array([[1.0], [1.0]]))\nresult.backward(grad_output)\n\n# Check gradients\nprint(x.grad)\n# Expected Output:\n# [[1. 0. 0.]\n#  [1. 0. 0.]]\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#sum","title":"<code>sum</code>","text":"<p>The sum operation computes the sum of all elements or along a specified axis. The formula for the sum is straightforward:</p> \\[\\text{sum}(x) = \\sum_{i} x_i\\] <p>The derivative of the sum operation with respect to each element is simply 1:</p> \\[\\frac{d}{dx} \\sum x_i = 1\\] <p>In the forward pass, we use <code>np.sum()</code> to compute the sum of tensor elements along the given axis (or across all elements if no axis is specified).</p> <pre><code>def sum(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -&gt; \"Tensor\":\n    # Compute the sum of the tensor along the specified axis (or over all elements)\n    output = np.sum(self.data, axis=axis, keepdims=keepdims)\n\n    dependencies = []\n\n    if self.requires_grad:\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            # ...\n    # ...\n\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre> <p>In the backward pass we need to compute the gradient with respect to the sum of tensor elements. First, we initialize <code>full_grad</code> as an array of ones with the same shape as the input tensor. This represents the gradient that will be propagated back through the tensor.</p> <pre><code>def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n    # Initialize a gradient array of the same shape as the input\n    full_grad = np.ones_like(self.data)\n</code></pre> <p>When <code>axis is None</code>, we've summed over all elements</p> <pre><code>if axis is None:\n    return full_grad * grad\n</code></pre> <p>If <code>axis</code> is specified and <code>keepdims=True</code>, we maintain the shape of the input tensor during the backward pass. If there are multiple elements summed over an axis, each element gets a share of the gradient proportional to its contribution.</p> <pre><code>grad_expanded = grad if keepdims else np.expand_dims(grad, axis=axis)\n\nreturn full_grad * grad_expanded\n</code></pre> <p>Full implementation:</p> <pre><code>def sum(self, axis: int = None, keepdims: bool = False) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the sum of all elements in the tensor along a specified axis.\n\n    Args:\n        axis (int or tuple of ints, optional): Axis or axes along which a sum is performed. \n            The default is to sum over all dimensions.\n        keepdims (bool, optional): If True, the reduced axes are retained with length 1, \n            otherwise the result is reshaped to eliminate the reduced axes. Default is False.\n\n    Returns:\n        Tensor: A new tensor with the summed values and the gradient dependencies.\n\n    The sum operation accumulates all elements along a given axis (or all elements if axis is None).\n    The gradient for this operation is computed by broadcasting the incoming gradient across \n    the axis and summing it back up for each element.\n    \"\"\"\n\n    # Perform summation over specified axis\n    output = np.sum(self.data, axis=axis, keepdims=keepdims)\n\n    # Initialize the list of dependencies for gradient calculation\n    dependencies: List[Leaf] = []\n\n    if self.requires_grad:\n        # Backward function to calculate the gradients for the sum operation\n        def _bkwd(grad: np.ndarray) -&gt; np.ndarray:\n            r\"\"\"\n            Compute the gradient of the sum operation. The gradient is summed along the specified axis.\n\n            Args:\n                grad (np.ndarray): The gradient passed from the downstream operation.\n\n            Returns:\n                np.ndarray: The gradient for the input tensor.\n\n            If `keepdims` is True, the gradient is broadcasted to match the original tensor's dimensions.\n            \"\"\"\n\n            # Initialize a gradient array of the same shape as the input\n            full_grad = np.ones_like(self.data)\n\n            # When axis is None, we've summed over all elements\n            if axis is None:\n                return full_grad * grad\n\n            grad_expanded = grad if keepdims else np.expand_dims(grad, axis=axis)\n\n            return full_grad * grad_expanded\n\n        # Add the backward function to the dependencies list\n        dependencies.append(\n            Leaf(\n                value=self,  # The input tensor\n                grad_fn=_bkwd  # The backward function to compute the gradients\n            )\n        )\n\n    # Return a new tensor containing the sum, with the gradient dependencies if needed\n    return Tensor(output, self.requires_grad, dependencies)\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#mean","title":"<code>mean</code>","text":"<p>The mean operation computes the average of all elements or along a specified axis. The formula for the mean is:</p> \\[\\text{mean}(x) = \\frac{1}{n} \\sum_{i} x_i\\] <p>where \\(n\\) is the number of elements.</p> <p>The derivative of the mean operation with respect to each element is:</p> \\[\\frac{d}{dx} \\frac{1}{n} \\sum x_i = \\frac{1}{n}\\] <pre><code>def mean(self, axis: Optional[int] = None, keepdims: bool = False) -&gt; \"Tensor\":\n    r\"\"\"\n    Computes the mean of elements along a specified axis.\n\n    The mean is calculated as the sum of the elements divided by the number of elements along the given axis.\n\n    Args:\n        axis (Optional[int]): Axis along which to compute the mean. If None, computes the mean of all elements.\n        keepdims (bool): Whether to keep the reduced dimensions in the output.\n\n    Returns:\n        Tensor: A new tensor containing the mean of the elements along the specified axis.\n\n    The mean is calculated using the formula:\n        mean = sum(elements) / number_of_elements\n    \"\"\"\n\n    count = self.data.shape[axis] if axis is not None else self.size\n    return self.sum(axis=axis, keepdims=keepdims) / count\n</code></pre>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#more-activation-functions","title":"More Activation Functions!","text":"<p>With our enhanced <code>Tensor</code> class, we can now build complex functions with ease. Let's explore essential activation functions \u2014 ReLU, LeakyReLU and Softmax \u2014 which are widely used in modern neural network architectures.</p> <pre><code>class ReLU(Module):\n    def forward(self, input: Tensor) -&gt; Tensor:\n        # Apply ReLU: max(0, x)\n        return Tensor.maximum(0, input)\n\nclass LeakyReLU(Module):\n    def __init__(self, alpha: float = 0.01):\n        super().__init__()\n        self.alpha = alpha\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        # Apply LeakyReLU: max(0, x) + alpha * min(0, x)\n        return Tensor.maximum(0, input) + self.alpha * Tensor.minimum(0, input)\n\nclass Softmax(Module):\n    def __init__(self, dim: int = -1):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        # For numerical stability, subtract the maximum value along the specified dimension.\n        exp_input = (input - input.max(axis=self.dim, keepdims=True)).exp()\n        return exp_input / exp_input.sum(axis=self.dim, keepdims=True)\n</code></pre> <p>These implementations highlight the power of the <code>Tensor</code> class - with minimal code, we can define functions that form the building blocks of deep learning models. Thanks to the design of our framework, we only need to implement the forward method - backpropagation is handled automatically!</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/04/03/microtorch---deep-learning-from-scratch/#summary","title":"Summary","text":"<p>We now have a solid foundation for building the remaining tools needed to construct and train our Deep Neural Network. These fundamental tensor operations will enable automatic differentiation, making it easier to implement anything we need.</p>","tags":["Autograd","Automatic Differentiation","Backpropagation","Computational Graphs","Tensor Operations","Framework Development"]},{"location":"2025/02/05/classification---cross-entropy--softmax/","title":"Classification - Cross-Entropy & Softmax","text":"<p>Fashion-MNIST is a dataset created by Zalando Research as a drop-in replacement for <code>MNIST</code>. It consists of 70,000 grayscale images (28\u00d728 pixels) categorized into 10 different classes of clothing, such as shirts, sneakers, and coats. Your mission? Train a model to classify these fashion items correctly!</p> <p></p> <p>Fashion-MNIST Dataset Visualization</p>","tags":["Deep Learning","Gradient Descent","SGD","Neural Networks","Optimization","Machine Learning","AI Training","Fashion-MNIST"]},{"location":"2025/02/05/classification---cross-entropy--softmax/#check-the-jupyter-notebook-for-this-chapter","title":"Check the jupyter notebook for this chapter","text":"<p>You can check my post Dive into Learning from Data - MNIST Video Adventure to understand the original <code>MNIST</code> challenge. <code>Fashion-MNIST</code> is a similar challenge - but this time, we're classifying clothing items instead of handwritten digits.</p> <p>Unlike <code>MNIST</code>, where a simple Feed-Forward Neural Network might achieve near-perfect accuracy, <code>Fashion-MNIST</code> is a bit more challenging due to the complexity of clothing patterns. But that won't stop us from trying! Let's download the dataset and plot some sample images.</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\n\n# Fetch the Fashion MNIST dataset from OpenML\nfashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n\n# Separate the features (images) and labels\nX, y = fashion_mnist['data'], fashion_mnist['target']\n\n# Convert labels to integers (since OpenML may return them as strings)\ny = y.astype(int)\n\n\n# Define label names for Fashion MNIST classes\nlabel_names = [\n    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n]\n\n# Plot some sample images\nfig, axes = plt.subplots(3, 5, figsize=(10, 6))\naxes = axes.ravel()\n\nfor i in range(15):\n    img = X[i].reshape(28, 28)  # Reshape the 1D array into a 28x28 image\n    axes[i].imshow(img, cmap='gray')  # Display in grayscale\n    axes[i].set_title(label_names[y[i]])  # Set label as title\n    axes[i].axis('off')  # Hide axis\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Fashion-MNIST Dataset Visualization</p> <p>We need to prepare the data for training, so let's split the dataset into training and testing sets. This helps us train our model on one part of the data and test it on another. The main goal of training is to create a generalized solution that works on new, unseen data. To measure performance on unseen data, we set aside a portion of the dataset that won't be used during training. Instead, we use it after training to evaluate the model's accuracy.</p> <pre><code># Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Convert labels to integers\ny_train, y_test = y_train.astype(int), y_test.astype(int)\n</code></pre> <p>Let's also convert the labels to one-hot format. One-hot labels are used because they make training more stable and efficient. They allow neural networks to treat each class independently, preventing unintended relationships between class indices. This is especially useful for classification tasks with softmax activation, ensuring proper probability distribution and better gradient flow.</p> <pre><code># Convert labels to one-hot encoding\nnum_classes = 10\ny_train_one_hot = np.eye(num_classes)[y_train]\ny_test_one_hot = np.eye(num_classes)[y_test]\n</code></pre> <code>np.eye</code> and one-hot encoding <p>We use <code>np.eye(num_classes)</code>, which creates an identity matrix of size <code>num_classes \u00d7 num_classes</code>.</p> <pre><code>np.eye(4)\n</code></pre> <p>Output:</p> <pre><code>[[1. 0. 0. 0.]\n[0. 1. 0. 0.]\n[0. 0. 1. 0.]\n[0. 0. 0. 1.]]\n</code></pre> <p>Indexing with<code>y_train = [2, 0, 3, 1]</code> (<code>np.eye(num_classes)[y_train]</code>) <pre><code>np.eye(4)[np.array([2, 0, 3, 1])]\n</code></pre> This selects the 2nd, 0th, 3rd, and 1st rows from the identity matrix.</p> <p>Final One-Hot Encoded Output:</p> <pre><code>[[0. 0. 1. 0.]  # Class 2\n[1. 0. 0. 0.]  # Class 0\n[0. 0. 0. 1.]  # Class 3\n[0. 1. 0. 0.]] # Class 1\n</code></pre> <p><code>np.eye(num_classes)</code> gives a lookup table where each row is a one-hot vector, indexing with <code>y_train</code> selects the correct rows for the given labels. It's a fast and memory-efficient way to convert class indices to one-hot encoding.</p> <p>For data preprocessing let's use the <code>MinMaxScaler</code>. In short - <code>MinMaxScaler</code> scales features to a fixed range, usually [0, 1], improving model performance and stability.</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\n# Rescale\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n</code></pre> <p>You can check my post Dive into Learning from Data - MNIST Video Adventure, where we cracked the <code>MNIST</code> challenge and analyzed the image data.</p>","tags":["Deep Learning","Gradient Descent","SGD","Neural Networks","Optimization","Machine Learning","AI Training","Fashion-MNIST"]},{"location":"2025/02/05/classification---cross-entropy--softmax/#crossentropyloss-and-softmax","title":"CrossEntropyLoss and Softmax","text":"<p>In this problem, we have 10 different classes of clothing: <code>[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]</code> </p> <p>Since we are dealing with multiple classes instead of just two, we need to scale up the entropy from <code>BinaryEntropyLoss</code> to <code>CrossEntropyLoss</code>. And the <code>Sigmoid</code> function is not the best choice for this task. <code>Sigmoid</code> outputs probabilities for each class independently, which is not good for multi-class classification. Instead, we need to assign probabilities across multiple classes, ensuring they sum to 1. A much better approach is to use the <code>Softmax</code> function, which converts raw model outputs (logits) into a probability distribution over all classes. This allows our model to make more accurate predictions by selecting the class with the highest probability.</p> <p>In multiclass classification, the combination of Softmax + Cross-Entropy Loss has a unique property that simplifies the backward pass.</p> <p>The Softmax function is defined as: \\(S_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\\) and its derivative forms a Jacobian matrix:</p> \\[\\frac{\\partial S_i}{\\partial z_j} = \\begin{cases} S_i (1 - S_i) &amp; \\text{if } i = j \\\\ - S_i S_j &amp; \\text{if } i \\neq j \\end{cases}\\] <p>This Jacobian matrix is \\(N \\times N\\) (where \\(N\\) is the number of classes), which makes direct backpropagation inefficient.</p> <p>But, the Cross-Entropy Loss \\(L = -\\sum_{i} y_i \\log(S_i)\\), and its gradient after softmax is simply:</p> \\[\\frac{\\partial L}{\\partial z} = S - y\\] <p>The Softmax Jacobian cancels out with the Cross-Entropy derivative, so we avoid computing the full Jacobian. Instead, Softmax directly passes the gradient from Cross-Entropy, making backpropagation simpler and more efficient!</p> Why does the derivative of Cross-Entropy take the form \\(\\frac{\\partial L}{\\partial z_i} = S_i - y_i\\)? <p>The Cross-Entropy Loss function is \\(L = -\\sum_{i} y_i \\log(S_i)\\), where \\(y_i\\) is the one-hot encoded true label (\\(y_i = 1\\) for the correct class, 0 otherwise). \\(S_i\\) is the softmax output (predicted probability for class \\(i\\)).</p> <p>Now, let's compute the derivative of \\(L\\) with respect to \\(S_i\\):</p> \\[\\frac{\\partial L}{\\partial S_i} = -\\frac{y_i}{S_i}\\] <p>However, the goal is to compute the gradient with respect to \\(z_i\\) (the input logits), not \\(S_i\\). This is where the <code>Softmax</code> derivative comes in. Softmax is defined as:</p> \\[S_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\\] <p>The derivative of \\(S_i\\) with respect to \\(z_j\\) gives a Jacobian matrix:</p> \\[ \\frac{\\partial S_i}{\\partial z_j} = \\begin{cases} S_i (1 - S_i) &amp; \\text{if } i = j \\quad \\text{(diagonal terms)}\\\\ - S_i S_j &amp; \\text{if } i \\neq j \\quad \\text{(off-diagonal terms)} \\end{cases} \\] <p>This means that if we want to find how the loss \\(L\\) changes with respect to \\(z_i\\), we need to apply the chain rule:</p> \\[\\frac{\\partial L}{\\partial z_i} = \\sum_{j} \\frac{\\partial L}{\\partial S_j} \\frac{\\partial S_j}{\\partial z_i}\\] <p>Substituting:</p> \\[\\frac{\\partial L}{\\partial S_j} = -\\frac{y_j}{S_j}\\] <p>and</p> \\[ \\frac{\\partial S_j}{\\partial z_i} = \\begin{cases} S_j (1 - S_j) &amp; \\text{if } i = j \\\\ - S_j S_i &amp; \\text{if } i \\neq j \\end{cases} \\] <p>Let's expand:</p> \\[ \\frac{\\partial L}{\\partial z_i} = \\sum_{j} -\\frac{y_j}{S_j} \\cdot \\frac{\\partial S_j}{\\partial z_i} \\] <p>Breaking it into cases:</p> <ol> <li>Diagonal term (\\(i = j\\)):</li> </ol> \\[ -\\frac{y_i}{S_i} \\cdot S_i (1 - S_i) = - y_i (1 - S_i) \\] <ol> <li>Off-diagonal terms (\\(i \\neq j\\)):</li> </ol> \\[ -\\frac{y_j}{S_j} \\cdot (- S_j S_i) = y_j S_i \\] <p>Summing over all \\(j\\), we get:</p> \\[ \\frac{\\partial L}{\\partial z_i} = - y_i (1 - S_i) + \\sum_{j \\neq i} y_j S_i \\] <p>Since \\(y\\) is a one-hot vector, only one \\(y_j = 1\\), and all others are 0, meaning:</p> \\[ \\frac{\\partial L}{\\partial z_i} = S_i - y_i \\] <p>Intuition Behind Cancellation</p> <p>Instead of explicitly computing the full Softmax Jacobian, the multiplication of the Cross-Entropy derivative and the Softmax Jacobian simplifies directly to \\(S - y\\).</p> <ul> <li>This happens because the off-diagonal terms in the Jacobian sum cancel out in the chain rule application.</li> <li>The result is a simple gradient computation without the need for the full Jacobian matrix.</li> </ul> <p>This is why, in backpropagation, the Softmax layer doesn't need to explicitly compute its Jacobian. Instead, we can directly use:</p> \\[\\frac{\\partial L}{\\partial z} = S - y\\] <p>to efficiently update the parameters in neural network training.</p> <p>CrossEntropyLoss Implementation</p> <pre><code>class CrossEntropyLoss(Module):\n    def forward(self, pred: np.ndarray, target: np.ndarray, epsilon: float = 1e-7) -&gt; float:\n        r\"\"\"\n        Compute the Cross-Entropy loss for multiclass classification.\n\n        Args:\n            pred (np.ndarray): The predicted class probabilities from the model (output of softmax).\n            target (np.ndarray): The one-hot encoded true target values.\n            epsilon (float): A small value to avoid log(0) for numerical stability.\n\n        Returns:\n            float: The computed Cross-Entropy loss. Scalar for multiclass classification.\n        \"\"\"\n        # Clip predictions to avoid log(0)\n        pred = np.clip(pred, epsilon, 1. - epsilon)\n\n        # Compute cross-entropy loss for each example\n        loss = -np.sum(target * np.log(pred), axis=1)  # sum over classes for each example\n\n        # Return the mean loss over the batch\n        return np.mean(loss)\n\n    def backward(self, pred: np.ndarray, target: np.ndarray, epsilon: float = 1e-7) -&gt; np.ndarray:\n        r\"\"\"\n        Compute the gradient of the Cross-Entropy loss with respect to the predicted values.\n\n        Args:\n            pred (np.ndarray): The predicted class probabilities from the model (output of softmax).\n            target (np.ndarray): The one-hot encoded true target values.\n            epsilon (float): A small value to avoid division by zero for numerical stability.\n\n        Returns:\n            np.ndarray: The gradient of the loss with respect to the predictions.\n        \"\"\"\n\n        # Clip predictions to avoid division by zero\n        pred = np.clip(pred, epsilon, 1. - epsilon)\n\n        # Compute the gradient of the loss with respect to predictions\n        grad = pred - target  # gradient of cross-entropy w.r.t. predictions\n\n        return grad\n</code></pre> <p>Now, let's see how this is efficiently handled in the backward pass of Softmax. The naive approach would be to compute the full Softmax Jacobian matrix, which has \\(N \\times N\\) elements (where \\(N\\) is the number of classes). However, explicitly storing and multiplying by this matrix is computationally expensive. Instead, we take a more efficient approach using vectorized computation.</p> <p>In the backward pass, we receive \\(d_{\\text{out}} = S - y\\), which is the gradient of Cross-Entropy Loss with respect to Softmax outputs. The goal is to compute \\(\\frac{\\partial L}{\\partial z}\\), the gradient of the loss with respect to logits.</p> <p>The key observation is that for each example in the batch, the gradient of <code>Softmax</code> with respect to logits can be expressed as:</p> \\[\\frac{\\partial S_i}{\\partial z_j} d_{\\text{out}_j}\\] <p>Summing over all \\(j\\), we get:</p> \\[\\frac{\\partial L}{\\partial z_i} = S_i \\left( d_{\\text{out}_i} - \\sum_j d_{\\text{out}_j} S_j \\right)\\] <p>where \\(d_{\\text{out}} = S - y\\) is the gradient from Cross-Entropy and the term \\(\\sum_j d_{\\text{out}_j} S_j\\) efficiently accounts for the interaction between all class probabilities.</p> <p>Finally the Softmax.backward() function:</p> <pre><code>def backward(self, d_out: np.ndarray) -&gt; np.ndarray:\n    return self.output * (d_out - np.sum(d_out * self.output, axis=1, keepdims=True))\n</code></pre> <p>Instead of explicitly constructing the Jacobian, we directly compute the Jacobian-vector product, which is all we need for backpropagation. This avoids unnecessary computations, making the <code>Softmax</code> backward pass efficient and numerically stable.</p> <p>Softmax Implementation</p> <pre><code>class Softmax(Module):\n    \"\"\"Softmax function and its derivative for backpropagation.\"\"\"\n\n    def forward(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute the Softmax of the input.\n        Args:\n            x (np.ndarray): Input array of shape (batch_size, n_classes).\n        Returns:\n            np.ndarray: Softmax probabilities.\n        \"\"\"\n\n        # Subtract max for numerical stability\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n        return self.output\n\n    def backward(self, d_out: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute the gradient of the loss with respect to the input of the softmax.\n        Args:\n            d_out (np.ndarray): Gradient of the loss with respect to the softmax output.\n                                Shape: (batch_size, n_classes).\n        Returns:\n            np.ndarray: Gradient of the loss with respect to the input of the softmax.\n                        Shape: (batch_size, n_classes).\n        \"\"\"\n\n        # Compute batch-wise Jacobian-vector product without explicit Jacobian computation\n        return self.output * (d_out - np.sum(d_out * self.output, axis=1, keepdims=True))\n</code></pre> <p>The combination of Softmax + Cross-Entropy Loss simplifies backpropagation significantly. Instead of computing the full Jacobian, the Softmax layer directly propagates the gradient. This is why deep learning frameworks implement Softmax and Cross-Entropy together, optimizing for both performance and numerical stability!</p>","tags":["Deep Learning","Gradient Descent","SGD","Neural Networks","Optimization","Machine Learning","AI Training","Fashion-MNIST"]},{"location":"2025/02/05/classification---cross-entropy--softmax/#sgd-vs-fashion-mnist","title":"<code>SGD</code> vs <code>Fashion-MNIST</code>","text":"<p>Let's prepare the model, loss function and use the <code>SGD</code> optimizer.</p> <pre><code>input_dims = 784\n\nmodel = Sequential([\n    Linear(input_dims, 784, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(784, 256, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(256, 128, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(128, 10, init_method=\"xavier\"),  # 10 output logits for Fashion-MNIST\n    Softmax()\n])\n\nbce = CrossEntropyLoss()\noptimizer = SGD(lr=0.01, momentum=0.9)\n</code></pre> <p>Now, let's build and run our training loop for <code>Fashion-MNIST</code>:</p> <pre><code>from sklearn.metrics import accuracy_score\n\n\n# Hyperparameters\nepochs = 20\nbatch_size = 128\n\n\n# Training loop\nfor epoch in range(epochs):\n    # Shuffle training data - can help prevent overfitting!\n    # Stochastic batch of data for the training process!\n    indices = np.random.permutation(X_train.shape[0])\n    X_train_shuffled, y_train_shuffled = X_train[indices], y_train_one_hot[indices]\n\n    total_loss = 0\n    num_batches = X_train.shape[0] // batch_size\n\n    for i in range(0, X_train.shape[0], batch_size):\n        # Use a stochastic batch of data for training\n        X_batch = X_train_shuffled[i:i+batch_size]\n        y_batch = y_train_shuffled[i:i+batch_size]\n\n        #############\n        # Core steps!\n        #############\n\n        # Forward pass\n        preds = model(X_batch)\n        loss = bce(preds, y_batch)\n\n        # Zero grad before the backward pass!\n        model.zero_grad()\n\n        # Backward pass\n        d_loss = bce.backward(preds, y_batch)\n        model.backward(d_loss)\n\n        # Update weights\n        optimizer.step(model)\n\n        total_loss += loss\n\n    # Compute average loss\n    avg_loss = total_loss / num_batches\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n\n# Evaluation\ny_pred = model(X_test)\ny_pred_labels = np.argmax(y_pred, axis=1)\naccuracy = accuracy_score(y_test, y_pred_labels)\n\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n</code></pre> <p>I shuffled the training data, which can help prevent overfitting because the training algorithm might make sense of the sequence of batches and start to adjust the weights in the direction of the training batches. Remember, we use a stochastic batch of data for the training process and compute the gradient direction of this mini-batch of data.</p> <p>Output:</p> <pre><code>Epoch 1/20, Loss: 0.5885\nEpoch 2/20, Loss: 0.4156\nEpoch 3/20, Loss: 0.3807\nEpoch 4/20, Loss: 0.3570\nEpoch 5/20, Loss: 0.3370\nEpoch 6/20, Loss: 0.3236\nEpoch 7/20, Loss: 0.3089\nEpoch 8/20, Loss: 0.3052\nEpoch 9/20, Loss: 0.2970\nEpoch 10/20, Loss: 0.2837\nEpoch 11/20, Loss: 0.2757\nEpoch 12/20, Loss: 0.2712\nEpoch 13/20, Loss: 0.2636\nEpoch 14/20, Loss: 0.2608\nEpoch 15/20, Loss: 0.2513\nEpoch 16/20, Loss: 0.2448\nEpoch 17/20, Loss: 0.2438\nEpoch 18/20, Loss: 0.2357\nEpoch 19/20, Loss: 0.2325\nEpoch 20/20, Loss: 0.2311\nTest Accuracy: 89.94%\n</code></pre> <p>Also we can check the extended metrics:</p> <pre><code>import pandas as pd\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Calculate precision, recall, and F1 scores per class\nprecision_per_class = precision_score(y_test, y_pred_labels, average=None)\nrecall_per_class = recall_score(y_test, y_pred_labels, average=None)\nf1_per_class = f1_score(y_test, y_pred_labels, average=None)\n\n# Create a DataFrame to store the metrics per class\nmetrics_df = pd.DataFrame({\n    'Class': range(len(precision_per_class)),\n    'Precision': precision_per_class,\n    'Recall': recall_per_class,\n    'F1-Score': f1_per_class\n})\n\n# Display the table\nprint(metrics_df)\n</code></pre> <p>Output:</p> <pre><code>   Class  Precision    Recall  F1-Score\n0      0   0.830812  0.866571  0.848315\n1      1   0.991254  0.970043  0.980534\n2      2   0.870843  0.800284  0.834074\n3      3   0.877632  0.920635  0.898619\n4      4   0.791472  0.875461  0.831351\n5      5   0.973629  0.968254  0.970934\n6      6   0.759445  0.700071  0.728550\n7      7   0.939844  0.977189  0.958153\n8      8   0.984733  0.961252  0.972851\n9      9   0.978556  0.954672  0.966467\n</code></pre> <p>Not bad for <code>SGD</code>! We can use <code>SGD</code> with <code>momentum</code> and <code>gradient clipping</code> as an optimization baseline. From here, we can aim to surpass this baseline by exploring more advanced optimization techniques!</p>","tags":["Deep Learning","Gradient Descent","SGD","Neural Networks","Optimization","Machine Learning","AI Training","Fashion-MNIST"]},{"location":"2024/12/28/cross-entropy-loss/","title":"Cross Entropy Loss","text":"<p>Cross-Entropy is a widely used loss function, especially in classification tasks, that measures the difference between two probability distributions. </p> <p></p> <p>Binary Cross-Entropy (BCE)</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"<p>Loss measures how different the true data labels are from the model's predictions. Consider two binary vectors, where \"binary\" means the vectors have only two possible values: 0 and 1. If we have two vectors</p> <p>True labels (\\(y\\)):</p> \\[y = \\left[0, 1, 1, 1, 0, 1, 1, 1 \\right]\\] <p>And predicted labels (\\(\\hat{y}\\)):</p> \\[\\hat{y} = \\left[1, 1, 1, 0, 0, 1, 1, 0 \\right]\\] <p>The loss function provides a universal measure of the difference between these vectors. This is the general purpose of any loss function and the role of Binary Cross-Entropy in particular.</p> <p>The Cross-Entropy might look complex, but once we break it down, each part becomes clear and manageable. Let's begin with the foundation: the logarithmic function. </p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#log-function","title":"Log function","text":"<p>To understand Cross-Entropy, let's first recall the concept of the logarithm:</p> \\[\\tag{log} \\label{eq:log} \\log_a(b) = c \\quad \\iff \\quad a^c = b\\] <p>In simpler terms, the logarithm answers the question: \"To what power \\(c\\) should we raise the base \\(a\\) to obtain the value \\(b\\)?\"</p> <p>When you take the logarithm of a value in range <code>(0, 1)</code> (which is common for predicted probabilities), the result is always negative. The closer the probability is to 0, the more negative the log becomes.</p> <p>High-probability predictions (closer to 1) will have a logarithmic value that is closer to 0. For example if the predicted probability is 0.8, the log will be approximately -0.223. This is still negative, but relatively close to 0.</p> <p></p> <p>Low-probability predictions (closer to 0) will result in highly negative log values. For example:</p> <p>If the probability is 0.2, the log value will be -1.609.</p> <p></p> <p>If the probability is 0.01, the log will be close to -4.605.</p> <p></p> <p>You can experiment with this function on your own check the jupyter notebook</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#log-function-high-vs-low","title":"Log function: high vs low","text":"<p>This plot illustrates the behavior of the logarithmic function in two regions: low probabilities (close to 0) and high probabilities (closer to 1).</p> <p></p> <p>High vs Low</p> <ul> <li> <p>The first subplot shows the entire log function, with a vertical line at \\(x = 0.5\\) highlighting the transition point. </p> </li> <li> <p>The second subplot zooms in on the low-probability region (0.01 to 0.5), where the log function steeply declines.</p> </li> <li> <p>The third subplot focuses on high probabilities (0.5 to 1), where the slope of the log function flattens.</p> </li> </ul> <p>This visualization demonstrates how the log function's steepness varies across different probability ranges.</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#binary-cross-entropy-bce","title":"Binary Cross-Entropy (BCE)","text":"<p>The binary cross-entropy (BCE) formula (for two classes, 0 and 1) is given by:</p> \\[\\tag{BCE} \\label{eq:bce} \\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\] <p>\\(y_i\\) is the true label for the \\(i\\)-th sample, can either be 0 or 1. The model predicts \\(p_i\\), the predicted probability for the \\(i\\)-th sample, it's the model's output, typically a value between 0 and 1. \\(N\\) represents the number of samples in the dataset or batch. The loss is averaged over all samples, and we divide the sum result by the \\(N\\).</p> <p>Now let's break down the logarithmic terms. Since \\(y_i\\) can be either 0 or 1, the expression:</p> \\[y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\\] <p>activates only one of the terms depending on the value of \\(y_i\\).</p> <p>First Term \\(y_i \\log(p_i)\\): Active when \\(y_i = 1\\), it evaluates the predicted probability for the true class. A high \\(p_i\\) (close to 1) results in a small penalty, while a low \\(p_i\\) (close to 0) leads to a large penalty. The logarithm amplifies errors for low probabilities, encouraging confident and correct predictions.  </p> <p>Second Term \\((1 - y_i) \\log(1 - p_i)\\): Active when \\(y_i = 0\\), it evaluates the predicted probability for the incorrect class. A low \\(p_i\\) (close to 0) keeps the penalty small, while a high \\(p_i\\) (close to 1) results in a large penalty. The logarithm amplifies errors for high probabilities, penalizing confident but incorrect predictions.  </p> <p></p> <p>Log function</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#forward-implementation","title":"Forward Implementation","text":"<pre><code>import numpy as np\n\n\nclass BinaryCELoss:\n    r\"\"\"\n    Binary Cross-Entropy Loss module.\n\n    Computes the Binary Cross-Entropy loss and its gradient during the forward and backward passes.\n    This is typically used for binary classification tasks.\n    \"\"\"\n\n    def forward(self, pred: np.ndarray, target: np.ndarray, epsilon: float = 1e-7) -&gt; np.ndarray:\n        r\"\"\"\n        Compute the Binary Cross-Entropy loss between the predicted values and the target values.\n\n        Args:\n            pred (np.ndarray): The predicted values from the model.\n            target (np.ndarray): The true target values.\n            epsilon (float): A small value to avoid log(0) which can lead to numerical instability.\n\n        Returns:\n            np.ndarray: The computed Binary Cross-Entropy loss. Scalar for binary classification.\n        \"\"\"\n\n        loss = -(target * np.log(pred + epsilon) + (1 - target) * np.log(1 - pred + epsilon))\n\n        # Average the loss over the batch size\n        return np.mean(loss)\n</code></pre>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#binary-cross-entropy-derivative","title":"Binary Cross-Entropy derivative","text":"<p>For a single sample \\(i\\), the BCE loss is:</p> \\[\\ell_i = - \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\\] <p>Remember the log derivative:</p> \\[\\frac{d}{dx}\\left(\\log(x)\\right) = \\frac{1}{x}\\] <p>Taking the partial derivative with respect to \\(p_i\\):</p> \\[\\frac{\\partial \\ell_i}{\\partial p_i} = - \\left[ \\frac{y_i}{p_i} - \\frac{1 - y_i}{1 - p_i} \\right]\\] <p>Expanding and simplifying:</p> \\[\\frac{\\partial \\ell_i}{\\partial p_i} = \\frac{p_i - y_i}{p_i (1 - p_i)}\\] <p>This gradient follows your notation and describes how the BCE loss changes with respect to the predicted probabilities \\(p_i\\).</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#backward-implementation","title":"Backward Implementation","text":"<pre><code>import numpy as np\n\n\nclass BinaryCELoss:\n    # ... forward code\n\n    def backward(self, pred: np.ndarray, target: np.ndarray, epsilon: float = 1e-7) -&gt; np.ndarray:\n        r\"\"\"\n        Compute the gradient of the Binary Cross-Entropy loss with respect to the predicted values.\n\n        Args:\n            pred (np.ndarray): The predicted values from the model.\n            target (np.ndarray): The true target values.\n            epsilon (float): A small value to avoid log(0) which can lead to numerical instability.\n\n        Returns:\n            np.ndarray: The gradient of the loss with respect to the predictions.\n        \"\"\"\n\n        d_out = (pred - target) / (pred * (1 - pred) + epsilon)\n\n        # you should not average the gradients!\n        # instead, you should return the gradient for each example,\n        # as gradients represent how much the loss changes with\n        # respect to each individual prediction.\n\n        return d_out\n</code></pre>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#binary-cross-entropy-plot","title":"Binary Cross-Entropy plot","text":"<p>The best way to understand how cross-entropy works is to visualize it. Let's build the widely recognized plot with crossing lines that represents the binary case with two classes.</p> <p></p> <p>The plot visualizes the behavior of cross-entropy loss for binary classification, focusing on how the predicted probability \\(\\hat{y}\\) impacts the loss for each true class (\\(y = 1\\) and \\(y = 0\\)). </p> <p>For \\(y = 1\\) (blue curve), the loss is low when the predicted probability is close to 1, indicating a confident correct prediction. As \\(\\hat{y}\\) approaches 0, the loss increases sharply, penalizing confident incorrect predictions. </p> <p>For \\(y = 0\\) (red curve), the loss is low when \\(\\hat{y}\\) is close to 0, reflecting an accurate prediction for the negative class. When \\(\\hat{y}\\) nears 1, the loss grows steeply, again emphasizing penalties for confidently wrong predictions. </p> <p>The annotated arrows highlight key points, showing regions of low and high loss for both classes, and the overall behavior underscores the role of cross-entropy in guiding models to make confident and correct predictions.</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#deivative-plot","title":"Deivative plot","text":"<p>This plot visualizes the gradient of the binary cross-entropy loss with respect to the predicted probability \\(\\hat{y}\\), for two cases: when the true label \\(y = 1\\) and when \\(y = 0\\). The gradient indicates how the loss changes as the predicted probability moves closer to or farther from the true label. </p> <p></p> <p>Cross-Entropy Derivative plot</p> <p>For \\(y = 1\\) (blue line), the gradient is steep (large positive values) near \\(\\hat{y} = 0\\). This signifies a strong correction when the model predicts a probability far from the true label. As \\(\\hat{y}\\) approaches 1, the gradient becomes smaller, meaning minimal adjustments are needed since the prediction aligns well with the true label.</p> <p>For \\(y = 0\\) (red line), the pattern is reversed. The gradient is steep near \\(\\hat{y} = 1\\), penalizing predictions that incorrectly favor the positive class. When \\(\\hat{y}\\) approaches 0, the gradient diminishes, reflecting minimal penalties for predictions that align with the true label.</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#multiclass-cross-entropy","title":"Multiclass Cross-Entropy","text":"<p>For multiclass classification, the cross-entropy loss extends from the binary case to handle more than two classes. The multiclass cross-entropy formula for a single sample is:</p> \\[\\mathcal{L} = - \\sum_{i=1}^{C} y_i \\log(p_i)\\] <p>Where \\(C\\) is the number of classes, \\(y_i\\) is the true label, encoded as a one-hot vector (i.e., one class has a label of 1, and all others have 0) and \\(p_i\\) is the predicted probability for class \\(i\\).</p> <p>For a batch of \\(N\\) examples, the loss is averaged over the batch:</p> \\[\\mathcal{L} = - \\frac{1}{N} \\sum_{n=1}^N \\sum_{i=1}^C y_i^{(n)} \\log(p_i^{(n)})\\] <p>Where \\(y_i^{(n)}\\) and \\(p_i^{(n)}\\) are the true label and predicted probability for class \\(i\\) of the \\(n\\)-th example.</p> <p>Unlike binary cross-entropy, where only one term in the loss function is active, multiclass cross-entropy considers all classes. The loss is computed across all classes, but the log term only \"activates\" for the correct class.</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#multiclass-ce-derivative","title":"Multiclass CE derivative","text":"<p>Write the summation explicitly:</p> \\[\\mathcal{L} = -\\left(y_1 \\log(p_1) + y_2 \\log(p_2) + \\dots + y_C \\log(p_C)\\right)\\] <p>Since \\(y_i\\) is non-zero only for the true class (in one-hot encoding), most terms in the summation vanish except for the term where \\(y_i = 1\\). Consider the term where \\(y_i = 1\\): assume \\(y_k = 1\\) for some class \\(k\\), and \\(y_j = 0\\) for \\(j \\neq k\\). The loss simplifies to:</p> \\[\\mathcal{L} = - \\log(p_k)\\] <p>Thus, for the true class \\(k\\), the relevant term is \\(-y_k \\log(p_k)\\). Differentiate with respect to \\(p_i\\):</p> \\[\\frac{\\partial \\mathcal{L}}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left(-\\log(p_k)\\right) = -\\frac{1}{p_k}\\] <p>For all other classes \\(j \\neq k\\), \\(y_j = 0\\), so:</p> \\[\\frac{\\partial \\mathcal{L}}{\\partial p_j} = 0\\] <p>Final Derivative: Using the general summation form, the derivative of the loss with respect to \\(p_i\\) is:</p> \\[\\frac{\\partial \\mathcal{L}}{\\partial p_i} = -\\frac{y_i}{p_i}\\] <p>This adjusts the predicted probability for each class based on the difference between the predicted probability and the true label.</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#implementation-of-ce-loss","title":"Implementation of CE loss:","text":"<pre><code>import numpy as np\n\nclass CrossEntropyLoss:\n    def forward(self, pred: np.ndarray, target: np.ndarray, epsilon: float = 1e-7) -&gt; float:\n        \"\"\"\n        Compute the Cross-Entropy loss for multiclass classification.\n\n        Args:\n            pred (np.ndarray): The predicted class probabilities from the model (output of softmax).\n            target (np.ndarray): The one-hot encoded true target values.\n            epsilon (float): A small value to avoid log(0) for numerical stability.\n\n        Returns:\n            float: The computed Cross-Entropy loss. Scalar for multiclass classification.\n        \"\"\"\n        # Clip predictions to avoid log(0)\n        pred = np.clip(pred, epsilon, 1. - epsilon)\n\n        # Compute cross-entropy loss for each example\n        loss = -np.sum(target * np.log(pred), axis=1)  # sum over classes for each example\n\n        # Return the mean loss over the batch\n        return np.mean(loss)\n\n    def backward(self, pred: np.ndarray, target: np.ndarray, epsilon: float = 1e-7) -&gt; np.ndarray:\n        \"\"\"\n        Compute the gradient of the Cross-Entropy loss with respect to the predicted values.\n\n        Args:\n            pred (np.ndarray): The predicted class probabilities from the model (output of softmax).\n            target (np.ndarray): The one-hot encoded true target values.\n            epsilon (float): A small value to avoid division by zero for numerical stability.\n\n        Returns:\n            np.ndarray: The gradient of the loss with respect to the predictions.\n        \"\"\"\n\n        # Clip predictions to avoid division by zero\n        pred = np.clip(pred, epsilon, 1. - epsilon)\n\n        # Compute the gradient of the loss with respect to predictions\n        grad = -target / pred  # gradient of cross-entropy w.r.t. predictions\n\n        return grad\n</code></pre>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2024/12/28/cross-entropy-loss/#summary","title":"Summary","text":"<p>Binary Cross-Entropy (BCE) is used for two-class problems, where only one term is active at a time. Multiclass Cross-Entropy handles more than two classes, using all the predicted probabilities, but the true class has a label of 1, and all others are 0. The loss for multiclass classification accumulates over all classes, making it more complex, but the underlying principle of penalizing incorrect predictions remains the same.</p> <p>Cross-Entropy Loss is a powerful tool in classification tasks, particularly useful in penalizing confident but incorrect predictions. Logarithmic function amplifies the loss for wrong predictions made with high certainty, helping models learn more effectively.</p>","tags":["Cross-Entropy","Binary Cross-Entropy","BCE","Logarithm","Classification","Deep Learning"]},{"location":"2025/01/28/solving-non-linear-patterns-with-deep-neural-network/","title":"Solving Non-Linear Patterns with Deep Neural Network","text":"<p>The Perceptron, created by Frank Rosenblatt in the 1950s, was one of the first neural networks designed to classify patterns. Initially celebrated, it became a foundational milestone in machine learning.</p> <p></p> <p>Frank Rosenblatt and the Perceptron, a simple neural network machine designed to classify patterns</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/28/solving-non-linear-patterns-with-deep-neural-network/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"<p>To understand the Perceptron, imagine it analyzing an image of the number \"5.\" Each pixel serves as an input, connected to a weight. The Perceptron calculates a weighted sum, or dot product, between inputs (\\(x_i\\)) and weights (\\(w_i\\)):  </p> \\[x_w = \\sum_{i=0}^n x_i w_i\\] <p>The activation function decides the output based on this sum:  </p> \\[f(x_w) =  \\begin{cases}  1, &amp; \\text{if } x_w \\geq 0 \\\\  -1, &amp; \\text{otherwise} \\end{cases}\\] <p>Here, \\(x_0\\) is a bias term set to \\(1\\), balancing the model.</p> <p>Despite its promise, the Perceptron had a major limitation: it could only classify linearly separable patterns. For example, in 2D space, two sets of points (red and blue) are linearly separable if a single straight line can separate them.</p> <p></p> <p>Example: Linearly Separable Pattern</p> <p>This limitation made the Perceptron unsuitable for real-world problems with complex patterns. In 1969, Marvin Minsky and Seymour Papert demonstrated these limitations in their book Perceptrons.</p> <p></p> <p>Perceptrons Book Cover</p> <p>After the devastating criticism of the Perceptron, the first AI winter began - a time when numerous AI projects lost funding and support leaving them frozen for better days to come. The limitations of early models like the Perceptron highlighted the need for more advanced architectures to tackle complex problems.</p> <p>In my previous post, I used the Linear Layer as the model for the <code>SGD</code> optimizer. While it successfully handled linearly separable patterns, I can demonstrate that it fails with more complex non-linear patterns. For instance, a single Linear Layer cannot solve the spiral problem - a classic example that requires non-linear transformations.</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/28/solving-non-linear-patterns-with-deep-neural-network/#spiral-dataset","title":"Spiral Dataset","text":"<p>The <code>make_spiral_dataset</code> function generates a dataset of two intertwined spirals, which is a common benchmark problem for testing machine learning models on non-linearly separable data.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom typing import Tuple\n\n\ndef make_spiral_dataset(\n    n_samples: int = 100,\n    noise: float = 0.2,\n    seed: int = None,\n    x_range: Tuple[int, int] = (-1, 1),\n    y_range: Tuple[int, int] = (-1, 1)\n):\n    # Install the random seed\n    if seed:\n        np.random.seed(seed)\n\n    n = n_samples // 2  # Split samples between two spirals\n\n    # Generate first spiral\n    theta1 = np.sqrt(np.random.rand(n)) * 4 * np.pi\n    r1 = 2 * theta1 + np.pi\n    x1 = np.stack([r1 * np.cos(theta1), r1 * np.sin(theta1)], axis=1)\n\n    # Generate second spiral\n    theta2 = np.sqrt(np.random.rand(n)) * 4 * np.pi\n    r2 = -2 * theta2 - np.pi\n    x2 = np.stack([r2 * np.cos(theta2), r2 * np.sin(theta2)], axis=1)\n\n    # Combine spirals and add noise\n    X = np.vstack([x1, x2])\n    X += np.random.randn(n_samples, 2) * noise\n\n    # Scale X to fit within the specified x and y ranges\n    X[:, 0] = np.interp(X[:, 0], (X[:, 0].min(), X[:, 0].max()), x_range)\n    X[:, 1] = np.interp(X[:, 1], (X[:, 1].min(), X[:, 1].max()), y_range)\n\n    # Create labels\n    y_range = np.zeros(n_samples)\n    y_range[:n] = 0  # First spiral\n    y_range[n:] = 1  # Second spiral\n\n    return X, y_range\n</code></pre> <p>Let's generate an example of the spiral dataset:</p> <pre><code># Generate synthetic classification data\nn_samples = 500\nfeatures = 2\n\n# Usage example:\nx, y_target = make_spiral_dataset(n_samples=n_samples, noise=1.5, seed=1)\n\ny_target = y_target.reshape(-1, 1)\n\n# visualize in 2D\nplt.figure(figsize=(5, 5))\nplt.scatter(x[:, 0], x[:, 1], c=y_target, s=20, cmap=\"jet\")\nplt.xlim(x[:, 0].min() - 0.2, x[:, 0].max() + 0.2)\nplt.ylim(x[:, 1].min() - 0.2, x[:, 1].max() + 0.2)\nplt.show()\n</code></pre> <p></p> <p>Spiral dataset</p> <p>Now, we can apply the <code>Linear</code> model to the <code>spiral</code> dataset and observe its performance.</p> <pre><code>model = Linear(input_size=features, output_size=1, init_method=\"xavier\")\nactivation = Sigmoid()\nbce = BCELoss()\noptimizer = SGD(lr=0.01, momentum=0.9)\n</code></pre> <p>Training loop for 100 epoch:</p> <pre><code>n_epoch = 100\n\nfor epoch in range(n_epoch):\n    # Forward\n    output = model(x)\n    y_pred = activation(output)\n    loss = bce(y_pred, y_target)\n\n    model.zero_grad()\n\n    # Backward\n    grad = bce.backward(y_pred, y_target)\n    grad = activation.backward(grad)\n    model.backward(grad)\n\n    optimizer.step(model)\n\n    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Epoch 0, Loss: 0.6395\nEpoch 1, Loss: 0.6352\nEpoch 2, Loss: 0.6320\nEpoch 3, Loss: 0.6317\nEpoch 4, Loss: 0.6339\nEpoch 5, Loss: 0.6358\n...\nEpoch 96, Loss: 0.6310\nEpoch 97, Loss: 0.6310\nEpoch 98, Loss: 0.6310\nEpoch 99, Loss: 0.6310\n</code></pre> <p>Plotting the decision boundaries reveals that the <code>SGD</code> optimizer is stuck in a local minimum, and the model's complexity is insufficient to solve such a pattern:</p> <pre><code>plot_decision_boundaries(model, x, y_target)\n</code></pre> <p></p> <p>Linear Layer Failed</p> <p>Using our previously implemented Linear Layer, we train the model for 100 epochs and, there is no meaningful progress - the model is too simple to handle the spiral problem effectively.</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/28/solving-non-linear-patterns-with-deep-neural-network/#universal-approximator","title":"Universal Approximator","text":"<p>Minsky and Puppet in their book briefly discussed about the multi-layer nets.</p> <p></p> <p>A quote from the book Perceptron about the Multi-Layer Perceptron.</p> <p>They have not investigated this direction, because back in the day it was barelly possible to build such model, considering the hardware limitation. The cost of the <code>RAM</code> in (1957-1973):</p> Date (X) $/Mbyte (Y) Date Ref Page Company Size (KByte) Cost (US $) Speed (nsec) Memory Type JDR Chip Prices 1957 411,041,792 1957 Phister 366 C.C.C. 0.00098 392.00 10000 Transistor Flip-Flop 1959 67,947,725 1959 Phister 366 E.E.Co. 0.00098 64.80 10000 Vacuum Tube Flip-Flop 1960 5,242,880 1960 Phister 367 IBM 0.00098 5.00 11500 IBM 1401 Core Memory 1965 2,642,412 1965 Phister 367 IBM 0.00098 2.52 2000 IBM 360/30 Core Memory 1970 734,003 1970 Phister 367 IBM 0.00098 0.70 770 IBM 370/135 Core Memory 1973 399,360 1973 Jan PDP8/e User Price List DEC 12 4680.00 Core Memory 8KwordX12 bit <p>Source: Memory Prices</p> <p>The multi-layer models have the ability to approximate non-linear patterns. This principle lies at the heart of the Deep Learning revolution. These architectures act as a Universal Approximator Function, enabling them to model and learn highly complex relationships in data, thanks to the Universal Approximation Theorem! This theorem proves that a feedforward neural network with at least one hidden layer can approximate any continuous function to an arbitrary degree of accuracy, given the right configuration and activation functions. It underscores the power and flexibility of neural networks in modeling complex relationships in data.</p> Universal Approximation Theorem (UAT) <p>Universal Approximation Theorem (UAT) asserts that feedforward neural networks with at least one hidden layer and a non-constant, bounded, continuous, and monotonically increasing activation function (e.g., sigmoid, tanh) can approximate any continuous function on a compact subset of \\(\\mathbb{R}^n\\) to any desired accuracy. Formally, for any continuous function \\(f\\) and any \\(\\epsilon &gt; 0\\), there exists a neural network \\(\\hat{f}\\) such that \\(|f(x) - \\hat{f}(x)| &lt; \\epsilon\\) for all \\(x\\) in the set.</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/28/solving-non-linear-patterns-with-deep-neural-network/#deep-neural-network","title":"Deep Neural Network","text":"<p>The solution to the spiral pattern problem lies in the Deep Neural Network, which we can easily build using our framework, since we already have all the necessary building blocks! To recall the key components, check out my previous post: Mastering Neural Network - Linear Layer and SGD</p> <p>First, let's introduce the <code>LeakyReLU</code> activation function, which we will use between the layers. Mathematically, it is defined as a piecewise function:</p> \\[f(x) = \\begin{cases}  x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases}\\] <p>Here, \\(\\alpha\\) is a small positive constant that controls the slope of the function when \\(x\\) is negative.</p> <p>The derivative of the <code>LeakyReLU</code> activation function is:</p> \\[f'(x) = \\begin{cases}  1 &amp; \\text{if } x &gt; 0 \\\\ \\alpha &amp; \\text{if } x \\leq 0 \\end{cases}\\] <p>For positive inputs (\\(x &gt; 0\\)), the derivative is simply 1, meaning the function behaves like the identity function for positive values. For negative inputs (\\(x \\leq 0\\)), the derivative is \\(\\alpha\\), where \\(\\alpha\\) is a small constant (typically around 0.01), which controls the slope of the function in the negative domain.</p> <p>Implementation:</p> <pre><code>class LeakyReLU(Module):\n    def __init__(self, alpha: float = 0.01):\n        self.alpha = alpha\n\n    def forward(self, x: np.ndarray):\n        self.input = x\n        return np.where(x &gt; 0, x, self.alpha * x)\n\n    def backward(self, d_out: np.ndarray):\n        dx = np.ones_like(self.input)\n        dx[self.input &lt;= 0] = self.alpha\n        return d_out * dx\n</code></pre> <p>The second piece that we need to add is the <code>Sequential</code> class, which allows us to compose multiple layers into a single sequential model. This class facilitates both the <code>forward</code> and <code>backward</code> propagation of data through the layers of the network and makes managing the parameters of all layers in one unified structure.</p> <p>Here's the code for the <code>Sequential</code> class:</p> <pre><code>class Sequential(Module):\n    r\"\"\"\n    A class that represents a sequence of layers, used to build the multi-layer network.\n\n    This class allows layers to be stacked in a sequential manner, where data flows from one layer to the next.\n    It supports both forward and backward passes through the layers, as well as managing parameters for optimization.\n\n    Attributes:\n      - layers (List[Module]): A list of layers that compose the sequential model.\n    \"\"\"\n\n    def __init__(self, layers: List[Module]):\n        r\"\"\"\n        Initializes the Sequential model with the given list of layers.\n\n        Args:\n          - layers (List[Module]): List of layers to be included in the sequential model.\n        \"\"\"\n\n        self.layers = layers\n\n    def forward(self, x: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"\n        Performs the forward pass through all layers in the sequence.\n\n        Args:\n          - x (np.ndarray): Input data to the model.\n\n        Returns:\n          - np.ndarray: Output after passing through all layers in the sequence.\n        \"\"\"\n\n        for layer in self.layers:\n            x = layer.forward(x)\n        return x\n\n    def backward(self, d_out: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"\n        Performs the backward pass through all layers in reverse order.\n\n        Args:\n          - d_out (np.ndarray): Gradient of the loss with respect to the output.\n\n        Returns:\n          - np.ndarray: Gradient of the loss with respect to the input of the first layer.\n        \"\"\"\n\n        for layer in reversed(self.layers):\n            d_out = layer.backward(d_out)\n        return d_out\n\n    def parameters(self) -&gt; List[Parameter]:\n        r\"\"\"\n        Retrieves all parameters from the layers in the sequence, including weights and biases.\n\n        This method appends a unique prefix to the parameter name to differentiate the parameters of different layers\n        when optimizing.\n\n        Returns:\n          - List[Parameter]: A list of parameters (weights, biases, etc.) from all layers.\n        \"\"\"\n\n        params = []\n        for i, layer in enumerate(self.layers):\n            for param in layer.parameters():\n                # Add a unique prefix name for optimization step\n                param.name = f\"layer_{i}_{param.name}\"\n                params.append(param)\n        return params\n</code></pre> <p>Now, let's use the <code>Sequential</code> and compose the Multi-Layer model</p> <pre><code># Model architecture\nmodel = Sequential([\n    Linear(x.shape[1], 128, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(128, 64, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(64, 1, init_method=\"xavier\"),\n    Sigmoid()\n])\n\nbce = BCELoss()\noptimizer = SGD(lr=0.001, momentum=0.9)\n</code></pre> <p>And we are ready to run the training loop:</p> <pre><code>n_epoch = 1000\n\nfor epoch in range(n_epoch):\n    # Forward\n    y_pred = model(x)\n    loss = bce(y_pred, y_target)\n\n    model.zero_grad() \n\n    # Backward\n    grad = bce.backward(y_pred, y_target)\n    model.backward(grad)\n\n    optimizer.step(model)\n\n    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Epoch 0, Loss: 0.7561\nEpoch 1, Loss: 0.6769\nEpoch 2, Loss: 0.6555\nEpoch 3, Loss: 0.6653\nEpoch 4, Loss: 0.6509\nEpoch 5, Loss: 0.6272\n...\nEpoch 996, Loss: 0.0037\nEpoch 997, Loss: 0.0037\nEpoch 998, Loss: 0.0037\nEpoch 999, Loss: 0.0037\n</code></pre> <p>We get almost perfect score! The decision boundaries shows the solutions:</p> <pre><code>plot_decision_boundaries(model, x, y_target)\n</code></pre> <p></p> <p>Multi-Layer Model Cracked the Spiral!</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/28/solving-non-linear-patterns-with-deep-neural-network/#the-training-process-with-animated-decision-boundaries","title":"The Training Process with animated decision boundaries","text":"","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/28/solving-non-linear-patterns-with-deep-neural-network/#sign-off","title":"Sign Off","text":"<p>Deep learning has transformed from a theoretical concept to a practical tool, enabling solutions to problems once thought impossible. With the right architectures of the network the potential is truly limitless!</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/","title":"Dive into Learning from Data - MNIST Video Adventure","text":"<p>Hey there, data enthusiasts! Today, we're diving into the fascinating world of Machine Learning Classification using one of the most iconic datasets out there - the MNIST dataset. MNIST stands for Modified National Institute of Standards and Technology.</p> <p>We're diving into the realm of MNIST - a dataset that's like a treasure map for budding data scientists. It contains thousands of handwritten digits from 0 to 9. Each image is a snapshot of someone's attempt to scribble a number, and our mission is to make sense of these.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#whats-classification","title":"What's Classification?","text":"<p>Classification is like teaching your computer to distinguish between cats and dogs in photos. You feed it images, labeled \"cat\" or \"dog\", and it learns to predict which label to slap on a new photo.</p> <p></p> <p>Cat VS Dog</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#lets-meet-mnist","title":"Let's Meet MNIST","text":"<p>MNIST stands for Modified National Institute of Standards and Technology database. It's not about cats and dogs, but it's just as exciting. This dataset contains images of handwritten digits (0 through 9). Your mission is to train a model to identify these digits correctly.</p> <p></p> <p>MNIST digits</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#setup-and-data-exploration","title":"Setup and Data Exploration","text":"<p>First, you'll need to set up your environment. I'm rocking a Jupyter Notebook inside VS Code, but any Python environment will do the trick. Let's get our hands dirty with some code:</p> <pre><code># Import necessary libraries\nfrom sklearn.datasets import fetch_openml\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Fetch the MNIST dataset\nmnist = fetch_openml('mnist_784', version=1)\n\n# Check what we've got in our dataset\nprint(mnist.keys())\n\n# Separate the pixel information (images) and labels\nX, y = mnist['data'], mnist['target']\n\n# Let's peek at the data\nprint(X.shape)  # Should print (70000, 784) - 70k images, each 784 pixels\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#understanding-pixel-data","title":"Understanding Pixel Data","text":"<p>Notice how most of the data in <code>X</code> is close to zero? That's because most of the image is empty space:</p> <pre><code># Check the range of pixel intensities\nprint(\"Minimum pixel intensity:\", X.min().min())\nprint(\"Maximum pixel intensity:\", X.max().max())\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#data-insights","title":"Data Insights:","text":"<ul> <li>Why 784 pixels? Each image in MNIST is 28x28 pixels, which totals to 784 when flattened.</li> <li>Pixel Intensity: A value of 0 means the pixel is as black as a shadow, while 255 is a light as bright as the sun. In between, there's a spectrum of grays.</li> <li>Data Dimensionality: With 70,000 images, each with 784 features, we're dealing with a lot of information. But how much of it is truly useful?</li> </ul> <p>Direct radio image of a supermassive black hole at the core of Messier 87</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#focusing-on-the-relevant-data","title":"Focusing on the Relevant Data","text":"<p>Most of these images are like the night sky, mostly dark with occasional stars (activated pixels). Here's how we peek at the center of these images:</p> <pre><code># Look at the middle band of the images where digits usually reside\nX.iloc[:, 400:500]\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore","title":"Questions to Explore:","text":"<ul> <li>How does a computer learn from pixels what humans recognize by patterns?</li> <li>What patterns do we humans overlook that machines might find fascinating?</li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#visualizing-the-data","title":"Visualizing The Data","text":"<p>Images are typically represented as matrices where each cell might represent the intensity of a pixel. Here's how we can look at what we're dealing with:</p> <pre><code># Display the first image\nsome_digit = X.iloc[0].values.reshape(28, 28)\nplt.imshow(some_digit, cmap='binary')\nplt.axis(\"off\")\nplt.show()\n\nprint(\"Label for this image:\", y[0])\n</code></pre> <p>This code reshapes the <code>X</code> data into a 28x28 matrix (since 28 * 28 = 784), which is the size of our digit images. We use <code>matplotlib</code> to visualize it.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#a-gallery-of-digits","title":"A Gallery of Digits","text":"<p>Alright, let's take a moment to appreciate the art of handwritten numbers! This snippet of code takes the first ten images from our dataset and lays them out in a grid.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Setting up a 2x5 grid of subplots\nfig, axes = plt.subplots(2, 5, figsize=(10, 5))\n\n# Loop through our subplots\nfor i, ax in enumerate(axes.flat):\n    # Display the image in the subplot\n    ax.imshow(X.iloc[i].values.reshape(28, 28), cmap=\"gray\")\n    # Set the title of each subplot to the digit label\n    ax.set_title(y[i])\n    # Turn off the axis ticks\n    ax.axis(\"off\")\n\n# Adjust the layout to prevent overlapping\nplt.tight_layout()\n# Show the gallery\nplt.show()\n</code></pre> <p></p> <p>MNIST grid example</p> <p>You've turned raw pixel data into a visual representation that's not only informative but also engaging, allowing us to see the variety in how digits are handwritten.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_1","title":"Questions to Explore:","text":"<ul> <li> <p>How does the choice of color map (cmap) in <code>plt.imshow()</code> affect our perception and interpretation of the digit images? Does using <code>cmap='binary'</code> versus <code>cmap='gray'</code> or other colormaps reveal different nuances in the digits that might influence a model's learning process?</p> </li> <li> <p>What does the distribution of pixel intensities across different digits tell us about the writing styles or variations in the dataset? Could analyzing this distribution help in preprocessing steps like normalization or in feature engineering for better classification?</p> </li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#understanding-logistic-regression","title":"Understanding Logistic Regression","text":"<p>Once we've got our dataset ready, it's time to apply a classification algorithm. We'll use logistic regression, which, despite its name, is designed for classification. It's not about predicting a continuous outcome; instead, it helps us decide whether an image represents a specific digit or not.</p> <p>Logistic regression works by transforming input values through a sigmoid function, which squeezes any real number into a range between 0 and 1, effectively representing a probability. Here's how it looks:</p> <ul> <li>If the output of the sigmoid function is less than a certain threshold (commonly 0.5), we classify the input as belonging to one class.</li> <li>If it's greater than or equal to that threshold, we classify it into another class.</li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#the-sigmoid-function","title":"The Sigmoid Function","text":"<p>The sigmoid function takes any real-valued number and squashes it into a range from 0 to 1. It's defined as:</p> \\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] <p>Here's how you can define and visualize it:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    \"\"\"\n    Compute the sigmoid of x.\n\n    Parameters:\n    x (float or numpy array): Input value or array of values.\n\n    Returns:\n    float or numpy array: The sigmoid output.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# Generate a range of x values\ny = np.linspace(-10, 10, 100)\n\n# Apply sigmoid function to generate y values\nx = sigmoid(y)\n\n# Plot the sigmoid function\nplt.figure(figsize=(8, 6))\nplt.plot(y, x, label=r\"$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\", color='blue')\nplt.axhline(y=0.5, linestyle=\"--\", color=\"grey\", label='Threshold = 0.5')  # Adding threshold line\n\nplt.title(\"Sigmoid Function: The Heart of Logistic Regression\")\nplt.xlabel(\"Input (x)\")\nplt.ylabel(\"Output (Probability)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Remember, this isn't just about plotting a curve; it's about understanding how logistic regression decides on class boundaries using the sigmoid function to transform our input into a probability of belonging to a certain class.</p> <p></p> <p>This simple plot shows how the sigmoid function takes inputs from <code>-10</code> to <code>10</code> and transforms them into a probability curve. The gray dashed line represents our decision boundary at 0.5.</p> <ul> <li> <p>Why this shape? The sigmoid function gives us a smooth transition from 0 to 1, which is ideal for interpreting the output as probability. It's symmetric around <code>x = 0</code>, where <code>sigmoid(0) = 0.5</code>, making this the natural choice for our classification threshold.</p> </li> <li> <p>Why is this useful? When we're dealing with images of digits, this function allows us to convert the raw pixel data into something more interpretable\u2014a probability that the image belongs to a particular class.</p> </li> </ul> <p>By understanding this function, we gain insight into how logistic regression makes its classifications, turning raw data into decisions in a way that's both mathematically sound and intuitively understandable.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_2","title":"Questions to Explore:","text":"<ul> <li> <p>How does the choice of the threshold in logistic regression affect the classification results? Can adjusting this threshold improve the model's performance for certain classes over others?</p> </li> <li> <p>What are the implications of using logistic regression with the sigmoid function for multi-class classification problems like MNIST, where we have more than two classes? How does the softmax function, an extension of the sigmoid for multiple classes, compare in this context?</p> </li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#how-logistic-regression-works","title":"How Logistic Regression Works:","text":"<p>Logistic regression initially calculates a linear combination of the input features:</p> \\[ z = w_1x_1 + w_2x_2 + ... + w_nx_n + b \\] <p>Where:</p> <ul> <li> <p>\\(w_1, w_2, ..., w_n\\) are the weights for each feature.</p> </li> <li> <p>\\(x_1, x_2, ..., x_n\\) are the feature values.</p> </li> <li> <p>\\(b\\) is the bias term.</p> </li> </ul> <p>Then, we apply the sigmoid function to this linear combination:</p> \\[ \\hat{y} = \\sigma(z) \\] <p>Where:</p> <ul> <li> <p>\\(\\sigma(z)\\) is our sigmoid function.</p> </li> <li> <p>\\(\\hat{y}\\) is the predicted probability that the input belongs to class 1.</p> </li> </ul> <p>Based on this probability, we can classify data points by setting a threshold (often 0.5):</p> <ul> <li> <p>If \\(\\hat{y} &lt; 0.5\\), predict class 0.</p> </li> <li> <p>If \\(\\hat{y} \\geq 0.5\\), predict class 1.</p> </li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#sigmoid-in-action","title":"Sigmoid in action","text":"<p>Interpreting the Sigmoid Output:</p> <ul> <li>As <code>x</code> goes to negative infinity, the output of the sigmoid function approaches 0.</li> <li>As <code>x</code> goes to positive infinity, the output approaches 1.</li> <li>A common threshold for classification is 0.5. If the output is less than 0.5, we might classify it as class 0, and if it's greater than 0.5, as class 1.</li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#simulating-logistic-regression","title":"Simulating Logistic Regression","text":"<p>Let's delve deeper into how logistic regression processes data to make predictions. We'll simulate this process using Python to illustrate the transformation from input to output.</p> <p>Setting Up Our Simulation: First, we define our parameters:</p> <ul> <li>Number of Samples: 100</li> <li>Number of Features: 3</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nnum_samples, num_features = 100, 3\n</code></pre> <p>Generating Weights, Bias, and Input Data:</p> <pre><code># Initialize weights and bias randomly\nweights = np.random.randn(num_features)\nbias = np.random.randn()\n\n# Create input data X\nX = np.random.randn(num_samples, num_features)\n</code></pre> <p>Computing the Linear Combination:</p> <pre><code># Compute Z, which is our linear combination of weights and features plus bias\nZ = X @ weights + bias  # Using @ for matrix multiplication\nprint(f\"Shape of Z: {Z.shape}\")  # This should be a 1-dimensional array of length 100\n</code></pre> <p>The <code>Z</code> array represents the linear combination of our input features with the weights, plus the bias. Here, <code>@</code> performs the dot product between <code>X</code> (100 samples by 3 features) and <code>weights</code> (3 features), resulting in a vector of 100 values, one for each sample.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#if-its-not-clear-why-we-have-a-vector-with-100-items-you-might-need-a-review-of-matrix-multiplication-and-broadcasting-rules","title":"If it's not clear why we have a vector with 100 items, you might need a review of matrix multiplication and broadcasting rules.","text":"<p>Applying the Sigmoid Function: Now we'll apply the sigmoid function to transform <code>Z</code> into probabilities:</p> <pre><code># Apply sigmoid to Z to get probabilities (y_hat)\ny_hat = sigmoid(Z)\n</code></pre> <p>Plotting the Results: We'll now plot the sigmoid curve along with our simulated predictions:</p> <pre><code># Generate data for the sigmoid plot\nz_sigmoid = np.linspace(-10, 10, 100)\nsigmoid_values = sigmoid(z_sigmoid)\n\nplt.figure(figsize=(10, 6))\n# Plot the sigmoid function\nplt.plot(z_sigmoid, sigmoid_values, label=r\"$\\sigma(z) = \\frac{1}{1 + e^{-x}}$\", color='blue')\n\n# Plot our predicted values\nplt.scatter(Z, y_hat, color=\"red\", label=\"Predicted Values\", alpha=0.8)\nplt.axhline(y=0.5, linestyle=\"--\", color=\"grey\")  # Threshold line\n\nplt.title(\"Simulated Predictions for Logistic Regression\")\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\hat{y}$\")\n\nplt.legend()\nplt.tight_layout()\n\nplt.show()\n</code></pre> <p></p> <p>This simulation visualizes how logistic regression uses the sigmoid function to convert a linear combination of features into class probabilities. Keep in mind, this is a simulation with random weights and bias, not an optimized model, but it helps to illustrate the concept.</p> <p>Explaining the Plot:</p> <ul> <li>The blue line is the theoretical sigmoid function, showing how inputs are transformed into probabilities.</li> <li>The red dots represent our simulated model's predictions, where each dot corresponds to a sample's <code>Z</code> value mapped to its predicted probability <code>y_hat</code>.</li> <li>The horizontal grey dashed line at <code>y=0.5</code> is our decision boundary; points above this line would be classified as class 1, and below as class 0.</li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_3","title":"Questions to Explore:","text":"<ul> <li> <p>How does the distribution of <code>Z</code> values affect the classification accuracy? What happens if most <code>Z</code> values are clustered around zero?</p> </li> <li> <p>What would happen if we were to change the threshold from 0.5 to another value? How would this affect the classification of samples near the threshold?</p> </li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#logistic-regression-with-sklearn","title":"Logistic Regression with <code>sklearn</code>","text":"<p>Now that we've understood the basics of logistic regression, let's apply this knowledge using scikit-learn's implementation. Remember, we've already set up our data with <code>X</code> containing the pixel data and <code>y</code> representing the labels. </p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# We defined X and y earlier:\nX, y = mnist['data'], mnist['target']\n\n# Here we convert y to integers because mnist['target'] might be strings\ny = y.astype(int)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <p>Splitting the Data: We use <code>train_test_split</code> to divide our data. This is crucial to evaluate how well our model generalizes to unseen data. Here, we've decided to use 20% of the data for testing, keeping the rest for training.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#training-the-model","title":"Training the Model","text":"<p>Next, we'll set up our logistic regression model with an increased number of iterations to ensure it converges:</p> <pre><code># Creating and training the model\nmodel = LogisticRegression(max_iter=2000)\nmodel.fit(X_train, y_train)\n</code></pre> <p><code>max_iter</code>: Set to 2000 to give the model enough iterations to converge. Increasing this can be useful if the model doesn't converge with fewer iterations.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#making-predictions-and-evaluating","title":"Making Predictions and Evaluating","text":"<p>Now, let's use the model to predict on the test set:</p> <pre><code># Predicting on the test set\ny_pred = model.predict(X_test)\n\n# Evaluating the model\nscore = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy Score: {score:.2%}\")\n</code></pre> <p>This gives us a straightforward metric to assess our model's performance.</p> <pre><code>Accuracy Score: 0.9150714285714285\n</code></pre> <p>This indicates that the model correctly predicts the digit in about <code>91%</code> of cases, which is quite good for a simple model like logistic regression on this dataset. </p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#model-convergence","title":"Model Convergence","text":"<p>This error:</p> <pre><code>d:\\anaconda3\\envs\\au2grad\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n</code></pre> <p>indicating the model failed to converge.</p> <p>We've achieved a pretty good score for a straightforward model like logistic regression with 91%. However, there's an indication that our model hasn't fully converged because it hit the iteration limit. This suggests we could either increase <code>max_iter</code> to, say, 5,000 or more, or scale the data as we've discussed in data preprocessing. </p> <p>The complexity of our data might exceed what logistic regression can effectively handle. Rather than getting lost in the maze of hyperparameter tuning or further optimization, let\u2019s pivot and analyze the results we have.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_4","title":"Questions to Explore:","text":"<ul> <li> <p>How does the choice of solver in logistic regression (<code>lbfgs</code>, <code>liblinear</code>, <code>newton-cg</code>, etc.) influence model convergence and performance on MNIST?</p> </li> <li> <p>What impact does changing the <code>test_size</code> in <code>train_test_split</code> have on model performance metrics like accuracy? Could this reveal overfitting or underfitting issues in our model?</p> </li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#visualizing-misclassified-images","title":"Visualizing Misclassified Images","text":"<p>Now let's look at how we can visualize the images our model got wrong:</p> <pre><code>misclassified_idxs, = np.where(y_test != y_pred)\nprint(misclassified_idxs)\n\nfig, axes = plt.subplots(2, 5, figsize=(10, 5))\n\nfor y, ax in enumerate(axes.flat):\n    idx = misclassified_idxs[y]\n    ax.imshow(X_test.iloc[idx].values.reshape(28, 28), cmap=\"gray\")\n    ax.set_title(f\"True: {y_test.iloc[idx]}\\nPred: {y_pred[idx]}\")\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>In this code, we:</p> <ul> <li> <p>Identify the indices where the model's predictions do not match the actual labels using <code>np.where</code>. This gives us the <code>misclassified_idxs</code>.</p> </li> <li> <p>Set up a figure with 2 rows and 5 columns to display up to 10 misclassified digits.</p> </li> <li> <p>Loop through each subplot, displaying the misclassified image, its true label, and the model's prediction.</p> </li> </ul> <p>Visualizing these errors helps us understand the types of mistakes our model makes. For example, we might see an image that looks clearly like an '8', but our model predicted it as '5'. Sometimes, the digits might be ambiguous even to human eyes, which could explain why our model gets confused.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#confusion-matrix-a-deeper-look-at-model-errors","title":"Confusion Matrix: A Deeper Look at Model Errors","text":"<p>Moving beyond simple visualization, a confusion matrix provides a comprehensive view of our model's performance:</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#what-does-a-2x2-confusion-matrix-look-like","title":"What Does a 2x2 Confusion Matrix Look Like?","text":"Predicted Positive Predicted Negative Actual Positive True Positive (TP) False Negative (FN) Actual Negative False Positive (FP) True Negative (TN)","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#example","title":"Example:","text":"<p>Consider a medical test for detecting a disease: out of 100 people, 60 actually have the disease, and 40 do not.</p> <p>The confusion matrix would look like this:</p> Predicted Positive Predicted Negative Actual Positive 50 (TP) 10 (FN) Actual Negative 5 (FP) 35 (TN) <ul> <li>True Positive (TP): 50 people correctly identified as having the disease.</li> <li>False Negative (FN): 10 people with the disease were not detected (a type of error).</li> <li>False Positive (FP): 5 healthy people were incorrectly diagnosed with the disease.</li> <li>True Negative (TN): 35 people correctly identified as not having the disease.</li> </ul> <p>This matrix is invaluable because:</p> <ul> <li>It shows us not just how often we're right or wrong, but the specifics of our errors. </li> <li>False Negatives and False Positives indicate where the model might be overly cautious or not cautious enough.</li> <li>Metrics like accuracy, precision, recall, and F1 score can be derived from these numbers, providing quantitative measures of performance.</li> </ul> <p>For our MNIST dataset, the confusion matrix expands to a <code>10x10</code> table since we have 10 classes (0-9), which will give us an even more detailed picture of how our model performs across all digits.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_5","title":"Questions to Explore:","text":"<ul> <li>Which digits are most often misclassified, and which are most commonly confused with each other? What might these confusions tell us about the</li> <li>How does the performance vary across different classes? Could this variation suggest that some digits are inherently harder to classify due to their visual complexity or similarity with other digits?</li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#visualize-the-confusion-matrix","title":"Visualize the Confusion Matrix","text":"<p>When dealing with 10 digits, our confusion matrix becomes a <code>10x10</code> grid. Each cell represents how often an image from one digit (row) was classified as another digit (column). Here's how we can visualize this:</p> <pre><code>from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(y_true, y_preds):\n    # Generate confusion matrix\n    cm = confusion_matrix(y_true, y_preds)\n\n    # Plot the confusion matrix using matplotlib\n    plt.figure(figsize=(8, 8))\n    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n\n    # Add title and axis labels\n    plt.title('Confusion Matrix for MNIST Dataset')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n\n    # Add tick marks (integer labels) on x and y axis\n    tick_marks = range(len(set(y_true)))\n    plt.xticks(tick_marks)\n    plt.yticks(tick_marks)\n\n    # Annotate each cell with the numeric value of the confusion matrix\n    for y in range(cm.shape[0]):\n        for x in range(cm.shape[1]):\n            plt.text(x, y, format(cm[y, x], 'd'),\n                     ha=\"center\", va=\"center\",\n                     color=\"white\" if cm[y, x] &gt; cm.max() / 2 else \"black\")\n\n    # Display the plot\n    plt.tight_layout()\n    plt.show()\n\n# Plotting the confusion matrix using our test labels and predictions\nplot_confusion_matrix(y_test, y_pred)\n</code></pre> <p>This code:</p> <ul> <li>Computes the confusion matrix for our MNIST dataset.</li> <li>Creates a visual representation with matplotlib, using a blue color map where darker colors indicate higher occurrences.</li> <li>Labels the axes with 'Predicted Label' and 'True Label' for clarity.</li> <li>Places the number of occurrences in each cell, which helps to identify patterns in misclassification.</li> </ul> <p></p> <p>Confusion Matrix for MNIST</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_6","title":"Questions to Explore:","text":"<ul> <li> <p>Which class pairs exhibit the highest confusion rates? What characteristics of these digits might contribute to their frequent misclassification?</p> </li> <li> <p>How would changing the model's decision threshold or perhaps using a different probability calibration method affect the entries in this confusion matrix?</p> </li> <li> <p>If we were to implement data augmentation techniques (like rotations, scaling, or noise addition), how might this influence the distribution of values in the confusion matrix? Could this help in reducing misclassifications for certain digit pairs?</p> </li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#understanding-the-confusion-matrix-for-mnist","title":"Understanding the Confusion Matrix for MNIST","text":"<ul> <li> <p>Diagonal Elements: These show true positives where the model correctly classified an image. For example, if our model correctly identified 1284 instances of '0', this would be on the diagonal of the matrix.</p> </li> <li> <p>Off-Diagonal Elements: These indicate misclassifications:</p> </li> <li>Columns: When reading down a column, you see how often a digit was predicted as this digit. For instance, in the '0' column, any non-zero value except for the diagonal represents a false positive, meaning digits other than '0' were incorrectly labeled as '0'.</li> <li>Rows: Reading across a row shows how often a digit was actually this digit but was misclassified. This represents false negatives for that specific digit.</li> </ul> <p>Here are some insights from the matrix:</p> <ul> <li>Class '0': The model might have 1284 correct identifications (on the diagonal), but perhaps 8 '0's were misclassified as '1' (row 0, column 1).</li> <li>Class '5': If the model correctly identified 1094 '5's, it might have confused '5' with '6' 18 times (row 5, column 6), '7' 6 times (row 5, column 7), and '8' 48 times (row 5, column 8).</li> </ul> <p>By analyzing this matrix, we can pinpoint where the model struggles:</p> <ul> <li>Ambiguities: Certain digits like '5' and '8' might be easily confused due to similar shapes.</li> <li>Model Bias: If the model consistently predicts one digit over another, this might suggest a bias in the model or in the training data.</li> </ul> <p>The confusion matrix thus serves as a critical tool for diagnosing model performance, allowing us to delve into the specifics of classification errors and refine our approach accordingly.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_7","title":"Questions to Explore:","text":"<ul> <li> <p>If we were to look at the misclassification patterns, can we hypothesize which parts of the digit images (e.g., strokes, curvatures, or loops) the model might be overlooking or overemphasizing?</p> </li> <li> <p>How does the prevalence of certain digits in the training data correlate with misclassification rates? Could an imbalance in training data lead to higher off-diagonal values for certain classes?</p> </li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#calculating-accuracy-from-the-confusion-matrix","title":"Calculating Accuracy from the Confusion Matrix","text":"<p>Let's compute the accuracy by examining the confusion matrix:</p> <pre><code># Import confusion_matrix from sklearn.metrics\nfrom sklearn.metrics import confusion_matrix\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Calculate accuracy by taking the sum of the diagonal (correct predictions) \n# and dividing by the total number of predictions\naccuracy = cm.trace() / cm.sum()\n\nprint(f\"Accuracy: {accuracy}\")\n</code></pre> <p>Output:</p> <pre><code>0.9150714285714285\n</code></pre> <p>Here, we:</p> <ul> <li>Use <code>cm.trace()</code> to sum up the diagonal elements of the confusion matrix, which are the true positives for each class.</li> <li>Divide this sum by the total sum of all elements in the matrix <code>cm.sum()</code>, which represents the total number of instances.</li> </ul> <p>This gives us the accuracy, which should match what we've seen before using <code>sklearn</code>'s accuracy score:</p> \\[ \\text{Accuracy} = \\frac{\\sum \\text{Diagonal elements}}{\\text{Total number of instances}} \\]","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#exploring-precision","title":"Exploring Precision","text":"<ul> <li>Formula: \\(\\text{Precision}_i = \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FP}_i}\\)</li> <li>Example: If our model predicts 100 images to be \"3\", and 90 of these are truly \"3\", the precision for class \"3\" is \\(\\frac{90}{100} = 0.90\\) or 90%. This means our model is quite precise when it says an image is a \"3\".</li> </ul> <p>Precision is another key metric, particularly useful when the costs of false positives are high. Here's how we calculate it with <code>sklearn</code>:</p> <pre><code># Import precision_score from sklearn.metrics\nfrom sklearn.metrics import precision_score\n\n# Calculate precision for each class\nprecision_per_class = precision_score(y_test, y_pred)\n\nfor idx, score in enumerate(precision_per_class):\n    print(f'Precision for class {idx}: {score:.4f}')\n</code></pre> <p>Output:</p> <pre><code>Precision for class 0: 0.9561\nPrecision for class 1: 0.9534\nPrecision for class 2: 0.9029\nPrecision for class 3: 0.8909\nPrecision for class 4: 0.9186\nPrecision for class 5: 0.8865\nPrecision for class 6: 0.9316\nPrecision for class 7: 0.9250\nPrecision for class 8: 0.8842\nPrecision for class 9: 0.8921\n</code></pre> <ul> <li>Precision Calculation: For each class \\(i\\), precision is calculated as:</li> </ul> \\[ \\text{Precision}_i = \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FP}_i} \\] <p>Where:   - TP (True Positives) are the correct predictions for the class.   - FP (False Positives) are instances incorrectly predicted as class \\(i\\).</p> <ul> <li>Parameters for <code>precision_score</code>:</li> <li><code>average=None</code> ensures we get the precision for each class independently.</li> <li><code>zero_division=1</code> handles cases where there might be no predictions for a class to avoid division by zero.</li> </ul> <p>This breakdown shows us how precise our model is at predicting each digit. For instance, our model is highly precise for class 0 (95.61%), but less so for class 5 (88.65%). This information can guide us in understanding where our model might need improvement or where data might be inherently more difficult to classify.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#calculating-precision-from-confusion-matrix","title":"Calculating Precision from Confusion Matrix","text":"<p>Let's isolate our precision calculation in a separate step:</p> <pre><code>from sklearn.metrics import confusion_matrix\n\n# Generate the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Initialize an empty list to store precision scores\nprecision_per_class = []\n\n# Loop through all classes\nfor i in range(len(cm)):\n    # True Positives (TP) are on the diagonal\n    TP = cm[i, i]\n\n    # Sum all predictions for this class (True Positives + False Positives)\n    FP_TP = cm[:, i].sum()\n\n    # Precision calculation with safeguard against division by zero\n    precision = TP / FP_TP if FP_TP &gt; 0 else 0\n    precision_per_class.append(precision)\n\n# Print precision for each class\nfor idx, prec in enumerate(precision_per_class):\n    print(f'Precision for class {idx}: {prec:.4f}')\n</code></pre> <p>Output:</p> <pre><code>Precision for class 0: 0.9561\nPrecision for class 1: 0.9534\nPrecision for class 2: 0.9029\nPrecision for class 3: 0.8909\nPrecision for class 4: 0.9186\nPrecision for class 5: 0.8865\nPrecision for class 6: 0.9316\nPrecision for class 7: 0.9250\nPrecision for class 8: 0.8842\nPrecision for class 9: 0.8921\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_8","title":"Questions to Explore:","text":"<ul> <li>How does the confusion matrix help in understanding the trade-offs between precision and other metrics like recall for each class?</li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#moving-to-recall","title":"Moving to Recall","text":"","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#textrecall_i-fractexttp_itexttp_i-textfn_i","title":"\\(\\text{Recall}_i = \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FN}_i}\\)","text":"<p>Recall Example: If the model correctly identifies 90 out of 120 actual \"3\"s, the recall for class \"3\" is 90/120 = 0.75, or 75%. Here's how we calculate it with <code>sklearn</code>:</p> <pre><code>from sklearn.metrics import recall_score\n\n# Calculate recall for each class\nrecall_per_class = recall_score(y_test, y_pred, average=None)\n\nfor idx, score in enumerate(recall_per_class):\n    print(f'Recall for class {idx}: {score:.4f}')\n</code></pre> <p>Output:</p> <pre><code>Recall for class 0: 0.9561\nRecall for class 1: 0.9719\nRecall for class 2: 0.8899\nRecall for class 3: 0.8946\nRecall for class 4: 0.9151\nRecall for class 5: 0.8594\nRecall for class 6: 0.9463\nRecall for class 7: 0.9348\nRecall for class 8: 0.8666\nRecall for class 9: 0.9021\n</code></pre> <p>Let's isolate our recall calculation in a separate step:</p> <pre><code>from sklearn.metrics import confusion_matrix\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Initialize an empty list to store recall scores\nrecall_per_class = []\n\n# Loop through all classes\nfor i in range(len(cm)):\n    # True Positives (TP) are on the diagonal\n    TP = cm[i, i]\n\n    # Sum all actual instances of this class (True Positives + False Negatives)\n    FN_TP = cm[i, :].sum()\n\n    # Recall calculation with safeguard against division by zero\n    recall = TP / FN_TP if FN_TP &gt; 0 else 0\n    recall_per_class.append(recall)\n\n# Print recall for each class\nfor idx, score in enumerate(recall_per_class):\n    print(f'Recall for class {idx}: {score:.4f}')\n</code></pre> <p>Output:</p> <pre><code>Recall for class 0: 0.9561\nRecall for class 1: 0.9719\nRecall for class 2: 0.8899\nRecall for class 3: 0.8946\nRecall for class 4: 0.9151\nRecall for class 5: 0.8594\nRecall for class 6: 0.9463\nRecall for class 7: 0.9348\nRecall for class 8: 0.8666\nRecall for class 9: 0.9021\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#questions-to-explore_9","title":"Questions to Explore:","text":"<ul> <li>Why might recall be particularly important in certain applications, like medical diagnosis or spam detection?</li> <li>How does the distribution of classes in the dataset influence recall scores? Could an imbalanced dataset lead to lower recall for minority classes?</li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#understanding-the-f1-score","title":"Understanding the F1-Score","text":"<p>The F1 score is a crucial metric in classification tasks, especially when you need to balance between precision (the accuracy of positive predictions) and recall (the ability to find all the positive instances). It's the harmonic mean of these two, ensuring both metrics are high for a high F1 score.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#mathematical-insight","title":"Mathematical Insight","text":"<p>F1-Score for class \\(i\\):</p> \\[ \\text{F1-Score}_i = 2 \\times \\frac{\\text{Precision}_i \\times \\text{Recall}_i}{\\text{Precision}_i + \\text{Recall}_i} \\] <p>Example:</p> <p>When we predict 100 images as \"3\" and 90 are actually \"3\":</p> <ul> <li> <p>Precision for class \"3\" = \\( \\frac{90}{100} = 0.90\\)</p> </li> <li> <p>If the model identifies 90 out of true 120 \"3\"s:</p> </li> <li> <p>Recall for class \"3\" = \\( \\frac{90}{120} = 0.75\\)</p> </li> <li> <p>Thus, F1 for class \"3\" = \\( 2 \\times \\frac{0.90 \\times 0.75}{0.90 + 0.75} \\approx 0.818 \\)</p> </li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#coding-the-f1-score","title":"Coding the F1-Score","text":"<p>Let's implement this in Python:</p> <pre><code>from sklearn.metrics import f1_score\n\n# Calculate f1 for each class\nf1_per_class = f1_score(y_test, y_pred, average=None)\n\nfor idx, score in enumerate(f1_per_class):\n    print(f'F1 for class {idx}: {score:.4f}')\n</code></pre> <p>Output:</p> <pre><code>F1 for class 0: 0.9561\nF1 for class 1: 0.9626\nF1 for class 2: 0.8964\nF1 for class 3: 0.8928\nF1 for class 4: 0.9168\nF1 for class 5: 0.8728\nF1 for class 6: 0.9389\nF1 for class 7: 0.9298\nF1 for class 8: 0.8753\nF1 for class 9: 0.8971\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#manual-calculation-of-f1-score","title":"Manual Calculation of F1-Score","text":"<p>Here's how we manually compute the F1 score from a confusion matrix:</p> <pre><code>from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nf1_per_class = []\n\nfor i in range(len(cm)):\n    TP = cm[i, i]  # True positive for class i\n    FN_TP = cm[i, :].sum()\n    FP_TP = cm[:, i].sum()\n\n    precision = TP / FP_TP if FP_TP &gt; 0 else 0  # Avoid division by 0\n    recall = TP / FN_TP if FN_TP &gt; 0 else 0  # Avoid division by 0\n\n    f1 = 2 * ((precision * recall) / (precision + recall)) if (precision + recall) &gt; 0 else 0\n\n    f1_per_class.append(f1)\n\nfor idx, score in enumerate(f1_per_class):\n    print(f'F1 for class {idx}: {score:.4f}')\n</code></pre> <p>Output:</p> <pre><code>F1 for class 0: 0.9561\nF1 for class 1: 0.9626\nF1 for class 2: 0.8964\nF1 for class 3: 0.8928\nF1 for class 4: 0.9168\nF1 for class 5: 0.8728\nF1 for class 6: 0.9389\nF1 for class 7: 0.9298\nF1 for class 8: 0.8753\nF1 for class 9: 0.8971\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#visualizing-metrics","title":"Visualizing Metrics","text":"<p>Now, let's visualize precision, recall, and F1-score in a tabular format:</p> <pre><code>import pandas as pd\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Training and Evaluation (assuming X_train, y_train, X_test, y_test are defined)\nmodel = LogisticRegression(max_iter=1000, class_weight='balanced')\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nscore = accuracy_score(y_test, y_pred)\nprint(f\"Score: {score}\")\n\n# Calculate metrics per class\nprecision_per_class = precision_score(y_test, y_pred, average=None)\nrecall_per_class = recall_score(y_test, y_pred, average=None)\nf1_per_class = f1_score(y_test, y_pred, average=None)\n\n# Create a DataFrame to store the metrics per class\nmetrics_df = pd.DataFrame({\n    'Class': range(len(precision_per_class)),\n    'Precision': precision_per_class,\n    'Recall': recall_per_class,\n    'F1-Score': f1_per_class\n})\n\nprint(metrics_df)\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#analysis","title":"Analysis","text":"<ul> <li>Class Imbalance: Even with <code>class_weight='balanced'</code>, some classes still show lower performance. This could be due to inherent differences in the dataset's class representation or image quality.</li> <li>Metric Insights: The table helps in identifying which classes might need more attention, either in data preprocessing or feature engineering.</li> </ul> <p>By understanding and applying the F1 score correctly, we ensure our model's performance is well-rounded across all classes, which is essential for robust machine learning models.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#enhancing-model-performance-through-data-scaling","title":"Enhancing Model Performance through Data Scaling","text":"","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#the-challenge-convergence-issues","title":"The Challenge: Convergence Issues","text":"<p>Failure to Converge: Our model might not converge if we stick to the default number of iterations. However, simply increasing iterations isn't always the solution. Let's dive into data preprocessing with scaling:</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#data-scaling-why-it-matters","title":"Data Scaling: Why It Matters","text":"<p>Logistic regression uses the sigmoid function which works best when data is centered around zero. Here's what happens without scaling:</p> <ul> <li>Sigmoid Function Bias: With pixel values from 0 to 255, we're pushing the data to the upper end of the sigmoid curve, making gradient descent less effective.</li> </ul>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#implementing-scaling","title":"Implementing Scaling","text":"<p>Let's scale our data using <code>StandardScaler</code>:</p> <pre><code>from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre> <p>Now, let's check how scaling affects our data:</p> <pre><code>print(X_train.values[0, 400:500])  # Original data\nprint(X_train_scaled[0, 400:500])  # Scaled data\n</code></pre> <p>The scaling moves the data to a range around zero with a standard deviation of one.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#visualizing-scaled-images","title":"Visualizing Scaled Images","text":"<p>To see the effect visually:</p> <pre><code># Choose digits for visualization\nchosen_digits = [0, 5, 1, 8, 9, 7, 4, 3]\ndigit_indices = np.where(np.isin(y_train, list(chosen_digits)))[0]\n\n# Select and scale images\nX_train_selected = X_train.iloc[digit_indices]\nX_train_scaled_selected = X_train_scaled[digit_indices]\n\nnum_images = min(len(digit_indices), 5)  # Visualize up to 5 images\n\nfig, axes = plt.subplots(num_images, 2, figsize=(12, 12))\n\nfor i in range(num_images):\n    original_image = X_train_selected.iloc[i].values.reshape(28, 28)\n    scaled_image = X_train_scaled_selected[i].reshape(28, 28)\n\n    axes[i, 0].imshow(original_image, cmap='gray')\n    axes[i, 0].set_title(f\"Original Image (Digit {y_train.iloc[digit_indices[i]]})\")\n    axes[i, 0].axis('off')\n\n    axes[i, 1].imshow(scaled_image, cmap='gray')\n    axes[i, 1].set_title(\"Scaled Image\")\n    axes[i, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#performance-check-with-standardscaler","title":"Performance Check with StandardScaler","text":"<p>After scaling, we retrain and evaluate our model:</p> <pre><code>model = LogisticRegression(max_iter=1000, class_weight='balanced')\nmodel.fit(X_train_scaled, y_train)\ny_pred = model.predict(X_test_scaled)\n\nscore = accuracy_score(y_test, y_pred)\nprint(f\"Score: {score}\")\n\n# Metrics per class\nmetrics_df = pd.DataFrame({\n    'Class': range(len(precision_per_class)),\n    'Precision': precision_per_class,\n    'Recall': recall_per_class,\n    'F1-Score': f1_per_class\n})\nprint(metrics_df)\n</code></pre> <p>Results:</p> <pre><code>Score: 0.9168571428571428\n   Class  Precision    Recall  F1-Score\n0      0   0.956003  0.954579  0.955291\n1      1   0.950337  0.968750  0.959455\n2      2   0.898477  0.897826  0.898152\n3      3   0.904286  0.883461  0.893752\n4      4   0.921947  0.921236  0.921591\n5      5   0.871894  0.882168  0.877001\n6      6   0.933663  0.947708  0.940633\n7      7   0.927165  0.940120  0.933598\n8      8   0.895945  0.862933  0.879129\n9      9   0.899225  0.898592  0.898908\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#moving-to-minmaxscaler","title":"Moving to MinMaxScaler","text":"<p>StandardScaler might not preserve the relative relationships within the data as well for image data. Let's try <code>MinMaxScaler</code>:</p> <pre><code>scaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre> <p>Visualizing again:</p> <pre><code># ... (same visualization code as above)\n</code></pre> <p>And checking the performance:</p> <pre><code>Score: 0.921\n   Class  Precision    Recall  F1-Score\n0      0   0.962908  0.966493  0.964697\n1      1   0.950246  0.966875  0.958488\n2      2   0.910767  0.894928  0.902778\n3      3   0.903683  0.890440  0.897012\n4      4   0.923254  0.928958  0.926097\n5      5   0.874131  0.889238  0.881620\n6      6   0.944051  0.954871  0.949430\n7      7   0.930263  0.940785  0.935495\n8      8   0.898318  0.865881  0.881801\n9      9   0.903385  0.902113  0.902748\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#conclusion","title":"Conclusion","text":"<ul> <li>StandardScaler: Provided a slight improvement, but introduced artifacts in image representation.</li> <li>MinMaxScaler: Showed better preservation of data relationships and slightly improved performance metrics.</li> </ul> <p>By choosing the right scaling method, we can enhance the performance of our model, particularly for image data where maintaining the structure is crucial. Remember, the choice of scaler can significantly affect how your model interprets the data, impacting both training convergence and prediction accuracy.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#dimensionality-reduction-and-feature-expansion-the-magic-of-pca-and-polynomial-features","title":"Dimensionality Reduction and Feature Expansion: The Magic of PCA and Polynomial Features","text":"","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#introduction-to-dimensionality-reduction-with-pca","title":"Introduction to Dimensionality Reduction with PCA","text":"<p>When dealing with image data, like handwritten digits from the MNIST dataset, much of the data consists of empty space. This is where Principal Component Analysis (PCA) becomes invaluable. PCA's Role: It reduces the dimensions of your dataset by capturing the variance in the data through new features called principal components.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#implementing-pca","title":"Implementing PCA","text":"<p>To apply PCA:</p> <pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=30)  # Initially set to 30 components\n\nX_train_scaled_pca = pca.fit_transform(X_train_scaled)\nX_test_scaled_pca = pca.transform(X_test_scaled)\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#visualizing-pca-effects","title":"Visualizing PCA Effects","text":"<p>To observe how PCA changes our images:</p> <pre><code>X_train_scaled_pca_restored = pca.inverse_transform(X_train_scaled_pca)\n\n# Define the digit to visualize, e.g., only \"5\"s\nchosen_digit = 1\nchosen_digits = [0, 5, 1, 8, 9, 7, 4, 3]\n\n\n# Find indices of the chosen digit in y_train\n# digit_indices = np.where(y_train == chosen_digit)[0]\ndigit_indices = np.where(np.isin(y_train, list(chosen_digits)))[0]\n\nX_train_selected = X_train.iloc[digit_indices]\nX_train_scaled_selected = X_train_scaled[digit_indices]\nX_train_scaled_pca_restored_selected = X_train_scaled_pca_restored[digit_indices]\n\n# Set the number of images to visualize (use min to handle cases with fewer examples)\nnum_images = min(len(digit_indices), 5)  # For example, visualize up to 5 images\n\n# Create a figure for visualization\nfig, axes = plt.subplots(num_images, 3, figsize=(12, 12))\n\nfor i in range(num_images):\n    original_image = X_train_selected.iloc[i].values.reshape(28, 28)\n    scaled_image = X_train_scaled_selected[i].reshape(28, 28)\n    pca_restored_image = X_train_scaled_pca_restored_selected[i].reshape(28, 28)\n\n    # Plot original image\n    axes[i, 0].imshow(original_image, cmap='gray')\n    axes[i, 0].set_title(f\"Original Image (Digit {chosen_digit})\")\n    axes[i, 0].axis('off')\n\n    # Plot scaled image\n    axes[i, 1].imshow(scaled_image, cmap='gray')\n    axes[i, 1].set_title(\"Scaled Image\")\n    axes[i, 1].axis('off')\n\n    # Plot PCA restored image\n    axes[i, 2].imshow(pca_restored_image, cmap='gray')\n    axes[i, 2].set_title(\"PCA restored Image\")\n    axes[i, 2].axis('off')\n\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Note: With fewer components, the images look less clear but still recognizable. Increasing components improves clarity.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#evaluating-pca-on-model-performance","title":"Evaluating PCA on Model Performance","text":"<p>After reducing dimensions, we train our model:</p> <pre><code>model = LogisticRegression(max_iter=1000, class_weight='balanced')\nmodel.fit(X_train_scaled_pca, y_train)\ny_pred = model.predict(X_test_scaled_pca)\n\n# ... (code for scoring and metrics)\n</code></pre> <p>Results: PCA reduced the dimensionality, and although we lost some detail, the performance was nearly as good as with unscaled data.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#enhancing-with-polynomial-features","title":"Enhancing with Polynomial Features","text":"<p>To catch non-linear relationships:</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\nX_train_poly = poly.fit_transform(X_train_scaled_pca)\nX_test_poly = poly.transform(X_test_scaled_pca)\n</code></pre> <p>Example: If we had 2 components reduced to <code>x1</code> and <code>x2</code>, polynomial features would expand this to <code>x1</code>, <code>x2</code>, <code>x1^2</code>, <code>x2^2</code>, <code>x1*x2</code>.</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#visualizing-polynomial-features","title":"Visualizing Polynomial Features","text":"<p>We can't visualize these new features as images, but we can:</p> <pre><code># Plot a histogram of polynomial features\naxes[i, 3].hist(X_train_poly_digit[i], bins=50, color='skyblue', edgecolor='black')\naxes[i, 3].set_title(\"PCA-Poly Features Histogram\")\naxes[i, 3].axis('off')\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#training-with-polynomial-features","title":"Training with Polynomial Features","text":"<pre><code>model = LogisticRegression(max_iter=1000, class_weight='balanced')\nmodel.fit(X_train_poly, y_train)\ny_pred = model.predict(X_test_poly)\n\nscore = accuracy_score(y_test, y_pred)\nprint(f\"Score: {score}\")\n\n# ... (code to display metrics)\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#results","title":"Results","text":"<pre><code>Score: 0.9805\n   Class  Precision    Recall  F1-Score\n0      0   0.987332  0.986597  0.986965\n1      1   0.991261  0.992500  0.991880\n2      2   0.974747  0.978986  0.976862\n3      3   0.981026  0.974180  0.977591\n4      4   0.977029  0.985328  0.981161\n5      5   0.977183  0.975648  0.976415\n6      6   0.990688  0.990688  0.990688\n7      7   0.980106  0.983367  0.981734\n8      8   0.976048  0.960943  0.968437\n9      9   0.967832  0.974648  0.971228\n</code></pre>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2024/11/26/dive-into-learning-from-data---mnist-video-adventure/#conclusion_1","title":"Conclusion","text":"<ul> <li>Logistic Regression provides a solid baseline for classification.</li> <li>Data Scaling is crucial for performance and convergence.</li> <li>PCA reduces dimensionality effectively.</li> <li>Polynomial Features introduce non-linearity, significantly boosting accuracy.</li> </ul> <p>This journey through the MNIST dataset has shown us how even with relatively simple models like logistic regression, strategic preprocessing and feature engineering can yield impressive results. Remember, the journey in data science is as much about understanding your data as it is about applying algorithms. Keep experimenting, keep learning, and above all, enjoy the process.</p> <p>Happy coding!</p>","tags":["Classification","PCA (Principal Component Analysis)","Logistic Regression","Machine Learning","MNIST","Python","Data Preprocessing","Feature Engineering","sklearn","numpy"]},{"location":"2025/02/01/sgd-momentum--exploding-gradient/","title":"SGD, Momentum & Exploding Gradient","text":"<p>Gradient descent is fundamental method in training a deep learning network. It aims to minimize the loss function \\(\\mathcal{L}\\) by updating model parameters in the direction that reduces the loss. By using only batch of the data we can compute the direction of the steepest descent. However, for large networks or more complicated challenges, this algorithm may not be successful! Let's find out why this happens and how we can fix this.</p> <p></p> <p>Training Failure: <code>SGD</code> can't classify the spiral pattern</p>","tags":["Deep Learning","Gradient Descent","SGD","Neural Networks","Optimization","Machine Learning","AI Training"]},{"location":"2025/02/01/sgd-momentum--exploding-gradient/#check-the-jupyter-notebook-for-this-chapter","title":"Check the jupyter notebook for this chapter","text":"<p>Check the code from the previous post Solving Non-Linear Patterns with Deep Neural Networks and try experimenting with the learning rate for the <code>optimizer</code>. For example, setting <code>lr=0.01</code> can cause the optimizer to bounce around local minima. Even with <code>lr=0.001</code>, this problem can occur sometimes. When the optimizer moves too far in steep areas of the loss surface, the updates bounce back and forth, making the training oscillate and become unstable.</p> <p>In this chapter, I use the training loop code many times. Let's build a unified training loop:</p> <pre><code>def train_model(\n    model: Module,\n    loss_f: Module,\n    optimizer,\n    n_epochs: int = 500\n):\n    for epoch in range(n_epochs):\n        # Forward\n        y_pred = model(x)\n        loss = loss_f(y_pred, y_target)\n\n        model.zero_grad() \n\n        # Backward\n        grad = loss_f.backward(y_pred, y_target)\n        model.backward(grad)\n\n        optimizer.step(model)\n\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n</code></pre> <p>Example:</p> <pre><code># Model architecture\nmodel = Sequential([\n    Linear(x.shape[1], 128, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(128, 64, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(64, 1, init_method=\"xavier\"),\n    Sigmoid()\n])\n\nbce = BCELoss()\noptimizer = SGD(lr=0.01, momentum=0.9)\n\n# Training: SGD Epic Fail!\ntrain_model(model, bce, optimizer, n_epochs=100)\n</code></pre> <p>Output:</p> <pre><code># Bouncing\nEpoch 0, Loss: 0.6892\nEpoch 1, Loss: 1.9551\nEpoch 2, Loss: 4.4117\nEpoch 3, Loss: 3.7495\nEpoch 4, Loss: 1.0243\nEpoch 5, Loss: 0.7010\nEpoch 6, Loss: 2.5385\nEpoch 7, Loss: 3.0514\nEpoch 8, Loss: 3.6277\nEpoch 9, Loss: 2.2218\nEpoch 10, Loss: 8.0590\n...\n# Overflow!\nEpoch 77, Loss: 8.0590\nEpoch 78, Loss: 8.0590\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\nC:\\Users\\oaiw\\AppData\\Local\\Temp\\ipykernel_4280\\1697699738.py:124: RuntimeWarning: overflow encountered in exp\n  self.output = 1 / (1 + np.exp(-x))\n# No further progress...\nEpoch 94, Loss: 8.0590\nEpoch 95, Loss: 8.0590\n...\nEpoch 498, Loss: 8.0590\nEpoch 499, Loss: 8.0590\n</code></pre>","tags":["Deep Learning","Gradient Descent","SGD","Neural Networks","Optimization","Machine Learning","AI Training"]},{"location":"2025/02/01/sgd-momentum--exploding-gradient/#sgd-and-momentum-again","title":"SGD and Momentum, again!","text":"<p>In my previous post, I used separate terms for the momentum and gradient directions inside <code>SGD</code> for demonstration purposes. This is not the standard way of applying momentum, and it doesn't seem quite right. I used this for experimentation \u2014 you can amplify the direction for the velocity and the current gradient step separately.</p> \\[v_{t+1} = \\mu \\cdot v_{t} - \\alpha \\nabla f(x_t)\\] <p>The update rule for our position becomes:</p> \\[x_{t+1} = x_t + v_{t+1}\\] <p>And the implementation is here:</p> <pre><code># Update with momentum\nself.velocity[param_id] = self.momentum * self.velocity[param_id] - self.lr * grad\n\n# Update parameters\nparam.data += self.velocity[param_id]\n</code></pre> <p>Now let's use the correct, standard form where \\(\\mu\\) controls the influence of previous gradients, and \\(1 - \\mu\\) scales the current gradient like this:</p> \\[v_{t+1} = \\mu \\cdot v_{t} + (1 - \\mu) \\nabla f(x_t)\\] <p>The update rule for our position, where \\(\\alpha\\) is the step size, is:</p> \\[x_{t+1} = x_t - \\alpha \\cdot v_{t+1}\\] <p>In the correct implementation, we use both terms in the negative direction.</p> <p>Implementation:</p> <pre><code>class SGD:\n    def __init__(\n        self,\n        lr: float = 0.01,\n        momentum: float = 0.9,\n    ):\n        self.lr = lr\n        self.momentum = momentum\n        self.velocity = {}\n\n    def step(self, module: Module):\n        for param in module.parameters():\n            param_id = param.name\n\n            # Init velocity if not exists\n            if param_id not in self.velocity:\n                self.velocity[param_id] = np.zeros_like(param.data)\n\n            grad = param.grad.copy()\n\n            # Update momentum\n            self.velocity[param_id] = (\n                self.momentum * self.velocity[param_id] +\n                (1 - self.momentum) * grad\n            )\n\n            # Update parameters in the *negative* direction!\n            param.data -= self.lr * self.velocity[param_id]\n</code></pre> <p>Let's re-run our training loop with the same parameters:</p> <pre><code># Recreate Model, BCE, optimizer\nmodel = Sequential([\n    Linear(x.shape[1], 128, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(128, 64, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(64, 1, init_method=\"xavier\"),\n    Sigmoid()\n])\n\nbce = BCELoss()\noptimizer = SGD(lr=0.01, momentum=0.9)\n\ntrain_model(model, bce, optimizer)\n</code></pre> <p>Output:</p> <pre><code>Epoch 0, Loss: 0.7023\nEpoch 1, Loss: 0.6343\nEpoch 2, Loss: 0.6479\nEpoch 3, Loss: 0.6560\nEpoch 4, Loss: 0.6377\nEpoch 5, Loss: 0.6282\nEpoch 6, Loss: 0.6342\nEpoch 7, Loss: 0.6373\nEpoch 8, Loss: 0.6352\nEpoch 9, Loss: 0.6288\n# ...\nEpoch 497, Loss: 0.0027\nEpoch 498, Loss: 0.0027\nEpoch 499, Loss: 0.0027\n</code></pre> <p>Stable movement towards the global minimum! The <code>SGD</code> optimization algorithm with vanilla <code>Momentum</code> now works stably!</p>","tags":["Deep Learning","Gradient Descent","SGD","Neural Networks","Optimization","Machine Learning","AI Training"]},{"location":"2025/02/01/sgd-momentum--exploding-gradient/#gradient-clipping","title":"Gradient Clipping","text":"<p>The spiral pattern is highly non-linear which makes <code>SGD</code> struggles. Momentum helps speed up convergence in consistent gradient directions, but it can amplifies the problem. Momentum accumulates large, changing gradients, and the velocity term becomes too large. Large gradient updates cause oscillations.</p> <p>In <code>SGD</code>, weights are updated with \\(\\mu = 0.9\\) (the momentum term), can which causes large accumulated gradients. Momentum can lead to gradient explosion because it accumulates past gradients and amplifies them by multiplying with the \\(\\mu\\) term:</p> <p>The velocity term: \\(v_{t+1} = \\mu \\cdot v_{t} + (1 - \\mu) \\nabla f(x_t)\\) and the update rule: \\(x_{t+1} = x_t - \\alpha \\cdot v_{t+1}\\)</p> <p>Let's use the \\(\\alpha=0.1\\) and the same \\(\\mu=0.9\\):</p> \\[x_{t+1} = x_t - 0.1 \\cdot (0.9 \\cdot v_t - 0.1 \\cdot \\nabla f(x_t))\\] <p>If gradients (\\(\\nabla f(x_t)\\)) are large (which is common in deep neural networks), the velocity term can build up, leading to exploding updates. This causes SGD to fail to adapt, bouncing around sharp ridges in the loss landscape instead of converging smoothly.</p> <p>Let's run the training loop with the \\(\\alpha=0.1\\):</p> <pre><code># Recreate Model, BCE, optimizer\nmodel = Sequential([\n    Linear(x.shape[1], 128, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(128, 64, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(64, 1, init_method=\"xavier\"),\n    Sigmoid()\n])\nbce = BCELoss()\n# lr=0.1\noptimizer = SGD(lr=0.1, momentum=0.9)\n\ntrain_model(model, bce, optimizer)\n</code></pre> <p>Output:</p> <pre><code>Epoch 0, Loss: 0.6846\nEpoch 1, Loss: 2.3613\nEpoch 2, Loss: 4.6792\nEpoch 3, Loss: 2.0382\nEpoch 4, Loss: 1.3863\nEpoch 5, Loss: 1.6400\nEpoch 6, Loss: 6.8190\nEpoch 7, Loss: 2.9502\n# ...\n# Overflow!\nEpoch 87, Loss: 6.6729\nEpoch 88, Loss: 6.6729\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\nC:\\Users\\oaiw\\AppData\\Local\\Temp\\ipykernel_5404\\1697699738.py:124: RuntimeWarning: overflow encountered in exp\n  self.output = 1 / (1 + np.exp(-x))\nEpoch 89, Loss: 6.6729\nEpoch 90, Loss: 6.6729\n# No further progress...\nEpoch 496, Loss: 6.6729\nEpoch 497, Loss: 6.6729\nEpoch 498, Loss: 6.6729\nEpoch 499, Loss: 6.6729\n</code></pre> <p>You might proudly tell me, \"The learning rate is simply too high! Reduce the learning rate, and the result will stabilize.\" But I know a better way to fix this - one that works even with <code>lr=0.1</code>!</p> <p>To fix the exploding gradient problem, I use gradient clipping. It limits the size of the gradients to a min/max range:</p> \\[\\nabla \\mathcal{L} \\leftarrow \\text{clip}(\\nabla \\mathcal{L}, -\\text{clip_value}, \\text{clip_value})\\] <p>This ensures that the gradients won't explode during backpropagation. Also, let's use the standard form where \\(\\mu\\) controls the influence of previous gradients and \\(1 - \\mu\\) scales the current gradient</p> <p>Implementation:</p> <pre><code>class SGD:\n    def __init__(\n        self,\n        lr: float = 0.01,\n        momentum: float = 0.9,\n        clip_value: float = 1.0\n    ):\n        r\"\"\"\n        Initializes the Stochastic Gradient Descent (SGD) optimizer.\n\n        Args:\n            lr (float): Learning rate for updating the model's parameters.\n            momentum (float): Momentum for accelerating gradient descent.\n            clip_value (float): Value to clip gradients to avoid exploding gradients.\n        \"\"\"\n\n        self.lr = lr\n        self.momentum = momentum\n        # Clipping value to avoid exploding gradients\n        self.clip_value = clip_value\n        # Store momentum for each parameter\n        self.velocity = {}\n\n    def step(self, module: Module):\n        r\"\"\"\n        Performs a single update step on the module parameters using the gradients.\n\n        Args:\n            module (Module): The module (e.g., layer) whose parameters are being updated.\n        \"\"\"\n\n        for param in module.parameters():\n            param_id = param.name\n\n            # Initialize velocity if not exists\n            if param_id not in self.velocity:\n                self.velocity[param_id] = np.zeros_like(param.data)\n\n            # Make a copy to avoid modifying original\n            grad = param.grad.copy()  \n\n            # Gradient clipping!\n            if self.clip_value is not None:\n                np.clip(grad, -self.clip_value, self.clip_value, out=grad)\n\n            # Update momentum\n            self.velocity[param_id] = (\n                self.momentum * self.velocity[param_id] +\n                (1 - self.momentum) * grad\n            )\n\n            # Update parameters in the *negative* direction!\n            param.data -= self.lr * self.velocity[param_id]\n</code></pre> <p>This simple fix:</p> <pre><code>if self.clip_value is not None:\n  np.clip(grad, -self.clip_value, self.clip_value, out=grad)\n</code></pre> <p>Makes the training much more stable! Let's build the training loop and try to solve the spiral dataset.</p> <pre><code># Model architecture\nmodel = Sequential([\n    Linear(x.shape[1], 128, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(128, 64, init_method=\"he_leaky\"),\n    LeakyReLU(alpha=0.01),\n    Linear(64, 1, init_method=\"xavier\"),\n    Sigmoid()\n])\nbce = BCELoss()\n\n# Use lr=0.1 for stable convergence!\noptimizer = SGD(lr=0.1, momentum=0.9)\n\n# 200 epochs are enoght!\ntrain_model(model, bce, optimizer, n_epoch=200)\n</code></pre> <p>Output:</p> <pre><code>Epoch 0, Loss: 0.6648\nEpoch 1, Loss: 0.6432\nEpoch 2, Loss: 0.6413\nEpoch 3, Loss: 0.6532\nEpoch 4, Loss: 0.6473\nEpoch 5, Loss: 0.6478\n...\nEpoch 196, Loss: 0.0234\nEpoch 197, Loss: 0.0205\nEpoch 198, Loss: 0.0223\nEpoch 199, Loss: 0.0195\n</code></pre> <p>Stable movement towards the solution! We prevent the exploding gradient problem with gradient clipping!</p> <p></p> <p>Plot of <code>SGD</code> decision boundaries with gradient clipping</p> <p>Also, we reduced the training epochs by more than half from <code>500</code> to <code>200</code>, and got approximatelly the same result!</p>","tags":["Deep Learning","Gradient Descent","SGD","Neural Networks","Optimization","Machine Learning","AI Training"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/","title":"Gradient Descent - Downhill to the Minima","text":"<p>The gradient, \\( \\nabla f(\\textbf{x}) \\), is a vector of partial derivatives of a function. Each component tells us how fast our function is changing. If you want to optimize a function, you head in the negative gradient direction because the gradient points towards the steepest ascent.</p> <p></p> <p>Tangent Line of a function at a given point</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#optional-check-my-blog-post-if-you-want-to-know-why-the-gradient-points-to-the-steepest-ascent","title":"Optional: check my blog post if you want to know why the gradient points to the steepest ascent","text":"","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#simple-way-to-understand-the-gradient-descent-tangent-line-of-a-function-at-a-given-point","title":"Simple way to understand the gradient descent: Tangent Line of a function at a given point","text":"<p>To grasp gradient descent, let's start simple: with the tangent line at a point on a curve. This line touches the curve at one point, showing the function's rate of change at that spot because the slope of the curve matches the tangent's.</p> <p>Consider the point-slope form of a line where \\(x_1\\) and \\(y_1\\) are where the tangent touches:</p> \\[y - y_1 = m \\cdot (x - x_1)\\] <p>Here, \\( m \\) is the slope, \\( \\frac{dy}{dx} \\), or rise over run. We're looking for the function's rate of change at \\(x\\). Since the derivative gives us this rate, we rewrite our equation using the function's derivative:</p> \\[y = f(x) + f'(x) \\cdot (x - x_1)\\] <p>We can compute this derivative numerically using the central difference method, which approximates the derivative as:</p> \\[ f'(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} \\] <p>We know how to program derivatives using the central difference, with \\(h\\) set to <code>0.001</code> for accuracy, similar to calculators like TI-83. You can check Mastering Derivatives From Math to Code with Python Numerical Differentiation for more details.</p> <pre><code># Central difference for derivative approximation\ndef cdiff(f, x, h=1e-3):\n    return (f(x + h) - f(x - h)) / (2 * h)\n</code></pre>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#visualizing-the-tangent-line-with-interactive-plots","title":"Visualizing the Tangent Line with Interactive Plots","text":"<p>Now, let's, we build interactive plots to dynamically visualize how the slope of the tangent changes with respect to different points on the curve. This isn't just about seeing lines on a screen; it's about understanding the movement towards optimization through interaction.</p> <p>By following the derivative's direction, we can find the function's critical points\u2014either maxima or minima. This idea is fundamental to methods like gradient descent, where we move in the direction of the negative gradient to optimize a function.</p> <p>The interactive plot is created using Python's <code>ipywidgets</code> and <code>matplotlib</code>. Here's how it works:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact\n\n\n# Our function for demonstration\ndef f(x):\n    return -x**2\n\n\n# Central difference for derivative approximation\ndef cdiff(f, x, h=1e-3):\n    return (f(x + h) - f(x - h)) / (2 * h)\n\n\n# Equation of the tangent line\ndef tangent_line(f, x, x0):\n    fx0 = f(x0)\n    dfx0 = cdiff(f, x0)\n    return fx0 + dfx0 * (x - x0)\n\n\ndef interactive_plot(f, x):\n    def _interactive_plot(x0):\n        y = f(x)\n        # Slope at the point x0\n        slope = cdiff(f, x0)\n        # Y values for the tangent line\n        y_tangent = tangent_line(f, x, x0)\n\n        plt.figure(figsize=(12, 6))\n        plt.plot(x, y, label=\"$f(x) = -x^2$\")\n        plt.plot(x, y_tangent, label=f\"Tangent at x={x0}, Slope: {slope}\")\n        # Point of tangency\n        plt.plot(x0, f(x0), 'ro', markersize=8)\n\n        plt.xlim(-10, 10)\n        plt.ylim(-10, 1)\n        plt.xlabel('x'); plt.ylabel('y')\n        plt.title(f'Function: $f(x) = -x^2$ with Tangent at x={x0}, Slope: {slope}')\n        plt.legend(); plt.grid(True)\n        plt.show()\n\n    return _interactive_plot\n\nx = np.linspace(-10, 10, 200)\ninteract(interactive_plot(f, x), x0=(-10, 10, 0.1))\n</code></pre>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#how-the-code-works","title":"How the Code Works","text":"<ol> <li> <p>Function Definition: We define a function <code>f(x) = -x\u00b2</code>, which is an inverted parabola.</p> </li> <li> <p>Central Difference: The <code>cdiff</code> function calculates the derivative using the central difference method, which approximates the slope of the function at any point.</p> </li> <li> <p>Tangent Line Calculation: The <code>tangent_line</code> function calculates the tangent at a given point <code>x0</code> using the formula: \\(y=f(x_0) + f'(x_0) \\cdot (x - x_0)\\)</p> </li> <li> <p>Interactive Plot: The <code>interactive_plot</code> function binds the <code>x0</code> value to the plot using the <code>interact</code> function from <code>ipywidgets</code>. This allows us to dynamically adjust <code>x0</code> and visualize the tangent line and slope.</p> </li> </ol>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#understanding-extrema","title":"Understanding Extrema","text":"<p>When the slope (or derivative) of a function at a given point equals zero, it means we have an extremum point, either a local minimum or maximum.</p> <p></p> <p>An extremum point is characterized by a horizontal tangent line. A zero slope at a point indicates that we are at an extremum.</p> <p>To determine which one, we analyze the sign of the slope before and after the point:</p> <ul> <li>Local Minimum: If the slope changes from negative to positive as we approach the point, it indicates a minimum.</li> <li>Local Maximum: If the slope changes from positive to negative, it indicates a maximum.</li> </ul> <p>We can see this in action with a function like \\( f(x) = -x^2 \\). For example:</p> <ol> <li>Before the extremum at \\( x = 0 \\), the slope is positive (e.g., at \\( x = -3 \\), the slope is 6). As we approach \\( x = 0 \\), the slope becomes less steep and eventually reaches zero (horizontal tangent).</li> </ol> <p></p> <p>The slope at \\( x = -3 \\) is 6, but with the floating point error 5.999999</p> <ol> <li>After \\( x = 0 \\), the slope becomes positive and steeper (e.g., at \\( x = 3 \\), the slope is -6). This change in slope from negative to positive shows that \\( x = 0 \\) is a maxima.</li> </ol> <p></p> <p>The slope at \\( x = 3 \\) is -6, but with the floating point error -5.999999</p> <p>If we check the positive parabola function \\( f(x) = x^2 \\), we see the opposite effect. Now, the slope changes from negagtive to positive as we approach the extremum, indicating a minimum.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#visualizing-extrema-with-sinx","title":"Visualizing Extrema with <code>sin(x)</code>","text":"<p>Next, let's visualize another example using \\( f(x) = \\sin(x) \\), where we analyze the extrema points.</p> <pre><code># Simply change the f(x) function and leave the rest as it is\ndef f(x):\n    return np.sin(x)\n</code></pre>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#analyzing-extrema-in-sinx","title":"Analyzing Extrema in <code>sin(x)</code>","text":"<p>For \\( f(x) = \\sin(x) \\), we can identify two extremum points in this range:</p> <ul> <li>Right Extremum (Maxima): Around \\( x = 1.57 \\), the slope changes from positive to negative, indicating a local maximum.</li> <li>Left Extremum (Minima): Around \\( x = -1.57 \\), the slope changes from negative to positive, indicating a local minimum.</li> </ul> <p>We can also follow the derivative to find the maxima or minima of the function:</p> <ul> <li>Starting from \\( x = -2 \\), the derivative is positive (4), and we move towards the maximum.</li> <li>Similarly, starting from a positive side like \\( x = 1 \\), we move towards the maximum since the slope is negative initially.</li> </ul> <p>The direction of the derivative acts like a compass, showing us the steepest ascent or descent of the function. Following the slope, we can find the maximum or minimum points.</p> <p>This idea is key to optimization methods, such as gradient descent, where we move in the direction of the negative gradient (steepest descent) to minimize a function, or in the direction of the positive gradient to maximize it.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#programming-the-ascent-and-descent","title":"Programming the Ascent and Descent","text":"","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#step-1-gradient-ascent","title":"Step 1: Gradient Ascent","text":"<p>The gradient ascent algorithm moves in the direction of the steepest ascent to find a function's maximum. Here's the process:</p> <ol> <li>Starting Point: Begin with an initial value \\( x_0 \\).</li> <li>Compute the Derivative: Use the central difference method to estimate the derivative at \\( x_0 \\).</li> <li>Update \\( x \\): Move in the direction of the gradient by adding the derivative scaled by the learning rate.</li> <li>Repeat: Perform the above steps multiple times to move closer to the maximum.</li> </ol>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#code-for-gradient-ascent","title":"Code for Gradient Ascent","text":"<pre><code>def ascent(f, x0, steps=5, lr=0.3):\n    result = [x0]  # Store all steps for visualization\n\n    for _ in range(steps):\n        dfx0 = cdiff(f, x0)  # Compute the derivative\n        x1 = x0 + dfx0 * lr  # Update x using the learning rate\n        result.append(x1)    # Add the new value to the result\n        x0 = x1              # Update x0 for the next iteration\n\n    return result\n</code></pre>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#step-2-gradient-descent","title":"Step 2: Gradient Descent","text":"<p>The gradient descent algorithm works similarly but moves in the opposite direction\u2014towards the steepest descent\u2014to find the minimum of a function.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#code-for-gradient-descent","title":"Code for Gradient Descent","text":"<pre><code>def descent(f, x0, steps=5, lr=0.3):\n    result = [x0]  # Store all steps for visualization\n\n    for _ in range(steps):\n        dfx0 = cdiff(f, x0)  # Compute the derivative\n        x1 = x0 - dfx0 * lr  # Subtract the scaled derivative\n        result.append(x1)    # Add the new value to the result\n        x0 = x1              # Update x0 for the next iteration\n\n    return result\n</code></pre> <p>Let's join everything together. We're about to create functions that follow the path of the derivative to either climb or descend our mathematical hills. Here's how we're going to do it:</p> <pre><code>from ipywidgets import interact, FloatSlider, IntSlider\n\n\ndef ascent(f, x0, steps=5, lr=0.3):\n    # 'result' will store each step of our journey\n    result = [x0]\n\n    for _ in range(steps):\n        # Compute the derivative at current x0\n        dfx0 = cdiff(f, x0)\n        # Move up in the direction of the derivative, scaled by learning rate\n        x1 = x0 + dfx0 * lr\n        result.append(x1)\n        # Update x0 for next iteration\n        x0 = x1\n\n    return result\n\n\ndef descent(f, x0, steps=5, lr=0.3):\n    # Similar to ascent, but we go down\n    result = [x0]\n\n    for _ in range(steps):\n        dfx0 = cdiff(f, x0)\n        # Move down by subtracting the derivative scaled by learning rate\n        x1 = x0 - dfx0 * lr\n        result.append(x1)\n        x0 = x1\n\n    return result\n\n\n# Simple parabola for demonstration\ndef f(x):\n    return x**2\n\n\ndef plot_path(f, path_builder):\n    def _plot_path(x0, steps, lr):\n        # Generate x values for plotting\n        x = np.linspace(-10, 10, 200)\n        y = f(x)\n\n        # Calculate path using the ascent or descent function\n        x_path = path_builder(f, x0, steps, lr)\n        x_path = np.array(x_path)\n        y_path = f(x_path)\n\n        # Set up the plot\n        _, ax = plt.subplots(figsize=(12, 6))\n\n        # Plot the path taken\n        ax.plot(x_path, y_path, 'g.-', markersize=10, label='Path')\n\n        # Add arrows to show direction of movement\n        for i in range(len(x_path) - 1):\n            ax.annotate(\n                '',\n                xy=(x_path[i+1], y_path[i+1]),\n                xytext=(x_path[i], y_path[i]),\n                arrowprops=dict(facecolor='red', shrink=0.01, width=2),\n            )\n\n        # Plot the original function\n        ax.plot(x, y, 'b-', label='$f(x) = x^2$')\n\n        ax.set_xlabel('x')\n        ax.set_ylabel('f(x)')\n        ax.set_title('Path')\n        ax.legend()\n        plt.show()\n\n    return _plot_path\n\n# Interactive plot setup with descent\ninteract(plot_path(f, descent), \n    x0=FloatSlider(min=-10, max=10, step=1, value=9, description='Starting x'),\n    steps=IntSlider(min=1, max=20, step=1, value=3, description='Steps'),\n    lr=FloatSlider(min=0.01, max=1, step=0.01, value=0.1, description='Learning Rate'),\n)\n</code></pre>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#the-learning-rate-experiment","title":"The Learning Rate Experiment","text":"<ul> <li>A High Learning Rate: When we set the learning rate too high, like 0.5, we reach the maximum in just a couple of steps, but we might overshoot or oscillate, for example for the learning rate equals 1 we have just the horyzontal line.</li> <li>A Balanced Learning Rate: Setting it to around 0.3 helps us smoothly approach the maximum or minimum without bouncing around, finding a sweet spot where we can see progress without instability.</li> <li>The Descent: Flip the sign, and you're on your way down. The descent function uses the same logic but subtracts the derivative, steering us towards the function's minimum.</li> </ul> <p>This approach mimics gradient descent, a fundamental technique in optimization, where we adjust parameters to minimize a loss function. Here, we're not just riding the slope; we're controlling our speed with the learning rate, making sure we don't miss the mark or overshoot the target.</p> <p>Experiment with different starting points, number of steps, and learning rates. You'll get a feel for how these parameters affect the path and convergence to the extrema. This isn't just math; it's the backbone of learning algorithms in AI, showing us how small steps in the right direction can lead to significant optimization outcomes. </p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/05/gradient-descent---downhill-to-the-minima/#final-words","title":"Final words","text":"<p>Gradient descent isn't just about equations; it's a hands-on journey into optimization. With a well-chosen learning rate, we can guide algorithms toward an optimal solution.</p> <ul> <li>Learning Rate Matters: A low rate converges slowly, while a high rate risks overshooting or oscillation. For instance:         Too High: A rate of 1 leads to erratic movement.         Just Right: Rates around 0.3 ensure smooth convergence.</li> <li>Interactive Exploration: Use the interactive plots to experiment with:</li> <li>Starting Points: Test different x0x0\u200b values.</li> <li>Steps: Observe how step count affects convergence.</li> <li>Learning Rate: Fine-tune to balance speed and stability.</li> </ul> <p>Gradient descent is a cornerstone of machine learning, turning abstract functions into real-world solutions. Experimenting with these concepts lays the foundation for understanding more advanced optimization techniques in AI and data science.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/","title":"Gradient Descent Ninja with Momentum","text":"<p>Today, we'll build the gradient descent for a complex function. It's not as easy as it was for the 2D parabola; we need to construct a more complicated method! Momentum - a powerful method to help us solve this challenge!</p> <p></p> <p>Gradient descent got stuck in a local minimum!</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#chapter1-3d-gradient-descent-the-rollercoaster-of-optimization","title":"Chapter1: 3D Gradient Descent: The Rollercoaster of Optimization","text":"","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#gradient-recap","title":"Gradient Recap","text":"<p>The gradient is a vector of partial derivatives of a function. Essentially, it's the derivative of a function with respect to all its variables, represented as a vector.</p> <p>Let's revisit the concept of the gradient for a moment. For example, if we have a function \\(f(x, y)\\), then the gradient of this function is a vector of its partial derivatives with respect to \\(x\\) and \\(y\\):</p> \\[\\nabla f(x, y) = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right)\\] <p>The gradient is a multi-dimensional version of the derivative. We know the derivative represents the rate of change of a function or the steepest ascent. You can check why is the gradient the steepest ascent post to know much more about the derivative and the gradient! The best way to see this in action is to start with the single-variable version of the gradient: the derivative. To understand derivatives better, it's useful to express them numerically. Check this out to understand the underlying concepts</p> <p>However, the main problem with numerical differentiation is numerical instability. Function optimization requires precision; we can't afford methods that introduce instability or unpredictability. I've illustrated a specific case of this issue here</p> <p>I won't use numerical differentiation; I'll only use the exact gradient of the chosen function.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#glance-at-the-gradient-descent-again","title":"Glance at the Gradient Descent... Again!","text":"<p>On a gradient field plot, you see the gradient vector at each point of a 3D function surface. The gradient vectors point in the direction of the steepest ascent. If we start at some point in the field and move in the opposite direction of the gradient vector, we move downhill. This is the idea of gradient descent - follow the negative gradient\u2019s direction to reach the function's minimum.</p> <p>The most intuitive way to understand this is by reducing everything to the 2D case\u2014input \\(x\\) and output \\(y\\). In the previous post, I talked about gradient descent using the derivative.</p> <p></p> <p>Gradient descent in 2D</p> <p>For a 3D gradient, the concept is similar but scaled to multi-dimensional space. The gradient represents the direction of the steepest ascent of a multivariable function in every dimension. If we follow the negative direction of the gradient, we create a gradient descent in multiple dimensions.</p> <p>First, I want to use a hyperparabola; it's the easiest way to start our gradient descent journey. The slope of the function is smooth, and the minimum of the function is pretty clear for the descent - just follow the negative gradient to reach the bottom of the bowl.</p> <p>3D Paraboloid Function</p> \\[ f(x, y) = x^2 + y^2 \\] <p>This function represents a simple 3D paraboloid, which is essentially a bowl-shaped surface where the height of the function increases quadratically in both the x and y directions from the origin.</p> <p>Gradient of the 3D Paraboloid</p> \\[ \\nabla f(x, y) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix} \\] <p>The gradient of this function gives the direction of the steepest ascent at any point on the surface. For \\(f(x, y) = x^2 + y^2\\), the gradient points radially outward from the origin since the function increases in all directions from the center.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider, IntSlider\n\n\ndef f2d(vector):\n    r\"\"\"\n    3d Paraboloid function $f(x, y) = x^2 + y^2$.\n\n    Args:\n    vector: A 2D numpy array or list with shape (2,) representing (x, y) coordinates.\n\n    Returns:\n    Value of the function at point (x, y)\n    \"\"\"\n    x, y = vector\n    return x**2 + y**2\n\n\ndef grad_f2d(vector):\n    \"\"\"\n    Gradient of the function $f(x, y) = x^2 + y^2$.\n\n    Args:\n    vector: A 2D numpy array or list with shape (2,) representing (x, y) coordinates.\n\n    Returns:\n    Gradient of the function at point (x, y) as a numpy array.\n    \"\"\"\n    x, y = vector\n    return np.array([2*x, 2*y])\n\n\ndef gradient_descent3d(start_vector, learning_rate, num_iterations):\n    path = [start_vector]\n    for _ in range(num_iterations):\n        grad = grad_f2d(start_vector)\n        start_vector = start_vector - learning_rate * grad\n        path.append(start_vector)\n    return np.array(path)\n\n\ndef plot_gradient_descent(start_x, start_y, learning_rate, num_iterations):\n    # Create the surface\n    x = np.linspace(-10, 10, 100)\n    y = np.linspace(-10, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = f2d(np.array([X, Y]))\n\n    # Perform gradient descent\n    start_vector = np.array([start_x, start_y])\n    path = gradient_descent3d(start_vector, learning_rate, num_iterations)\n\n    # Create the figure and subplots\n    fig = plt.figure(figsize=(20, 10))\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax2 = fig.add_subplot(122)\n\n    # 3D plot\n    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n    ax1.plot(path[:, 0], path[:, 1], f2d(path.T), 'r.-', linewidth=2, markersize=20)\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('Z')\n    ax1.set_title('3D Gradient Descent Visualization')\n\n    # 2D contour plot with filled contours for depth\n    contour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\n    ax2.plot(path[:, 0], path[:, 1], 'r&gt;-', linewidth=2, markersize=10)\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_title('2D Contour Plot with Gradient Descent Path')\n\n    # Add color bars\n    fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n    fig.colorbar(contour, ax=ax2)\n\n    plt.tight_layout()\n    plt.close(fig)\n    return fig\n\n\n# Create interactive plot\ninteract(plot_gradient_descent, \n         start_x=FloatSlider(min=-10, max=10, step=0.1, value=8, description='Start X'),\n         start_y=FloatSlider(min=-10, max=10, step=0.1, value=8, description='Start Y'),\n         learning_rate=FloatSlider(min=0.01, max=0.95, step=0.01, value=0.1, description='Learning Rate'),\n         num_iterations=IntSlider(min=1, max=50, step=1, value=20, description='Iterations'))\n</code></pre> <p></p> <p>Gradient descent in 3D</p> <p>It works very fast and finds the minimum without any problems - just a half-parabola trajectory straight to the minimum! But this isn't always the case. For complicated function shapes, there's no easy and straight way to find the location of the global minimum, and in many cases, gradient descent can fall into a local minimum, giving us a suboptimal solution rather than the real optimum of the function. I can show you an example.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#complexity-of-the-challenge","title":"Complexity of the Challenge","text":"<p>For real-world challenges you need to find solutions for multi-dimensional data with non-linear dependencies that are impossible to fit in a short amount of time. All these problems narrow down to the optimization problems one way or another, because the training process is when you need to minimize the errors of your model. Every model is just a fairly complicated approximator that we call Neural Networks, or we can call it Non-Linear Functional Composition Approximator to be more precise and clear.</p> <p>To build an intuition behind the training process, we can examine gradient descent as applied to a complex function. The optimization challenge remains the same, and in Deep Learning or Machine Learning, this is referred to as the Training process.</p> <p>The training input is always data, which can be thought of as just a collection of numbers. Under the hood, all algorithms are manipulating these numbers, adjusting the weights to produce the expected results. The training algorithms optimize these weights by moving in the direction of the steepest descent. If you truly master gradient descent, you grasp all these concepts at once. There is the key to unlocking the full potential of this knowledge.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#falling-into-the-local-minimum","title":"Falling into the Local Minimum","text":"<p>Let's apply our basic approach to much complex landscape. Before any strategic changes, we need to understand how we can fail. </p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#function","title":"Function:","text":"<p>The function we are working with is a 3D quadratic function combined with sinusoidal terms it can be described as a quadratic function with sinusoidal perturbations or QFSP function.</p> \\[ f(x, y) = x^2 + y^2 + 10 \\sin(x) \\sin(y) \\]","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#gradient","title":"Gradient:","text":"<p>The gradient of the function is a vector of partial derivatives with respect to \\(x\\) and \\(y\\):</p> \\[ \\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) \\] <p>The partial derivatives are calculated as:</p> \\[\\frac{\\partial f}{\\partial x} = 2x + 10 \\cos(x) \\sin(y)\\] \\[\\frac{\\partial f}{\\partial y} = 2y + 10 \\sin(x) \\cos(y)\\] <p>Thus, the gradient is:</p> \\[ \\nabla f(x, y) = \\left( 2x + 10 \\cos(x) \\sin(y), 2y + 10 \\sin(x) \\cos(y) \\right) \\] <p>The QFSP function features a paraboloid shape disrupted by sinusoidal waves, creating a landscape with numerous local minima and maxima, challenging for gradient descent due to potential entrapment in local optima.</p> <pre><code>import numpy as np\n\n\ndef f3d(vector):\n    \"\"\"\n    Modified 3D function: f(x, y) = x^2 + y^2 + 10*sin(x)*sin(y).\n\n    Args:\n    vector: A 2D numpy array or list with shape (2,) representing (x, y) coordinates.\n\n    Returns:\n    Value of the function at point (x, y)\n    \"\"\"\n    x, y = vector\n    return x**2 + y**2 + 10 * np.sin(x) * np.sin(y)\n\n\ndef grad_f3d(vector):\n    \"\"\"\n    Gradient of the function f(x, y) = x^2 + y^2 + 10*sin(x)*sin(y).\n\n    Args:\n    vector: A 2D numpy array or list with shape (2,) representing (x, y) coordinates.\n\n    Returns:\n    Gradient of the function at point (x, y) as a numpy array.\n    \"\"\"\n    x, y = vector\n    dx = 2*x + 10 * np.cos(x) * np.sin(y)\n    dy = 2*y + 10 * np.sin(x) * np.cos(y)\n    return np.array([dx, dy])\n\n\n\n# Generate data\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nX, Y = np.meshgrid(x, y)\nZ = f3d([X, Y])\n\n# Create new figure for 3D plot\nfig = plt.figure(figsize=(20, 10))\n\n# 3D plot\nax1 = fig.add_subplot(121, projection='3d')\nsurf = ax1.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\nax1.set_title('3D Surface Plot of f(x, y)')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('Z')\nfig.colorbar(surf, shrink=0.5, aspect=5, ax=ax1)\n\n# 2D contour plot\nax2 = fig.add_subplot(122)\ncontour = ax2.contourf(X, Y, Z, levels=50, cmap='viridis')\nax2.set_title('2D Contour Plot of f(x, y)')\nax2.set_xlabel('X')\nax2.set_ylabel('Y')\nfig.colorbar(contour, ax=ax2)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Quadratic function with sinusoidal perturbations in 3D</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#python-optimizer","title":"Python Optimizer","text":"<p>First, I want to build an <code>Optimizer</code> class, which will help us to add functionality to our optimization process. Using this class, we can now proceed to apply our standard gradient descent to a function known for its local minima, such as the QFSP function, and observe how the optimization fails without advanced techniques.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Callable, Tuple\n\nfrom ipywidgets import interact, FloatSlider, IntSlider\n\n\nclass Optimizer:\n    \"\"\"\n    A class for performing gradient-based optimization techniques.\n\n    Attributes:\n        f (Callable[[np.ndarray], float]): The objective function to minimize.\n        grad_f (Callable[[np.ndarray], np.ndarray]): The gradient of the objective function.\n    \"\"\"\n\n    f: Callable[[np.ndarray], float]\n    grad_f: Callable[[np.ndarray], np.ndarray]\n\n    def __init__(self, f: Callable[[np.ndarray], float], grad_f: Callable[[np.ndarray], np.ndarray]):\n        \"\"\"\n        Initialize the OptimizationSolver with the objective function and its gradient.\n\n        Args:\n            f (Callable[[np.ndarray], float]): The function to minimize.\n            grad_f (Callable[[np.ndarray], np.ndarray]): The gradient of the function.\n        \"\"\"\n        self.f = f\n        self.grad_f = grad_f\n\n\n    def gradient_descent(\n        self,\n        start_point: np.ndarray,\n        learning_rate: float = 0.1,\n        num_iterations: int = 10,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Perform gradient descent optimization with optional momentum and regularization.\n\n        Args:\n            start_point (np.ndarray): Initial guess for the optimization.\n            learning_rate (float): Step size for each iteration.\n            num_iterations (int): Maximum number of iterations to perform.\n\n        Returns:\n            np.ndarray: Path of points visited during optimization.\n        \"\"\"\n\n        path = [start_point]\n\n        # Init the momentum velocity\n        velocity = np.zeros_like(start_point)\n\n        for _ in range(num_iterations):\n            grad = self.grad_f(start_point)\n\n            new_point = start_point - learning_rate * grad\n\n            start_point = new_point\n            path.append(new_point)\n\n        return np.array(path)\n\n\n    def plot_gradient_descent(self, path: np.ndarray, min_max: Tuple[int, int] = (-15, 15)) -&gt; plt.Figure:\n        \"\"\"\n        Visualize the gradient descent path on the Ackley function in both 3D and 2D.\n\n        Args:\n            path (np.ndarray): Path of points visited during optimization.\n            min_max (Tuple[int, int]): Min and max values on the grid.\n\n        Returns:\n            plt.Figure: Matplotlib figure object containing the plots.\n        \"\"\"\n\n        # Create the surface\n        x = y = np.linspace(min_max[0], min_max[1], 100)\n        X, Y = np.meshgrid(x, y)\n\n        last_point = path[-1]\n        first_point = path[0]\n\n        Z = np.zeros_like(X)\n        for i in range(X.shape[0]):\n            for j in range(X.shape[1]):\n                Z[i, j] = self.f(np.array([X[i, j], Y[i, j]]))\n\n        # Create the figure and subplots\n        fig = plt.figure(figsize=(20, 10))\n        ax1 = fig.add_subplot(121, projection='3d')\n        ax2 = fig.add_subplot(122)\n\n        # 3D plot\n        surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n        # Plot the path from the user-defined start\n        Z_path = [self.f(point) for point in path]\n        ax1.plot(path[:, 0], path[:, 1], Z_path, 'r.-', linewidth=2, markersize=5, label='User-defined Start')\n\n        point_size = 300\n\n        # Plot the best point found by multi-start (Here we use the last point as we've only done one descent)\n        ax1.scatter(first_point[0], first_point[1], self.f(first_point), s=point_size, c='b', marker='*', label='First Point')\n        ax1.scatter(last_point[0], last_point[1], self.f(last_point), s=point_size, c='g', marker='*', label='Last Point')\n\n        ax1.set_xlabel('X')\n        ax1.set_ylabel('Y')\n        ax1.set_zlabel('Z')\n        ax1.set_title('3D Gradient Descent Visualization on Ackley Function')\n        ax1.legend()\n\n        # 2D contour plot with filled contours for depth\n        contour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\n        ax2.plot(path[:, 0], path[:, 1], 'r&gt;-', linewidth=2, markersize=5, label='User-defined Start')\n        ax2.scatter(first_point[0], first_point[1], s=point_size, c='b', marker='*', label='First Point')\n        ax2.scatter(last_point[0], last_point[1], s=point_size, c='g', marker='*', label='Last Point')\n\n        ax2.set_xlabel('X')\n        ax2.set_ylabel('Y')\n        ax2.set_title('2D Contour Plot with Gradient Descent Path on Ackley Function')\n        ax2.legend()\n\n        # Add color bars\n        fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n        fig.colorbar(contour, ax=ax2)\n\n        plt.tight_layout()\n        plt.close(fig)\n        return fig\n\n\ndef build_grad_and_plot(start_x, start_y, learning_rate, num_iterations):\n    start_point = np.array([start_x, start_y])\n    optimizer = Optimizer(f3d, grad_f3d)\n\n    path = optimizer.gradient_descent(\n        start_point,\n        learning_rate,\n        num_iterations\n    )\n    return optimizer.plot_gradient_descent(path)\n\n\n# Create interactive plot\ninteract(build_grad_and_plot, \n         start_x=FloatSlider(min=-10, max=10, step=0.1, value=7.11, description='Start X'),\n         start_y=FloatSlider(min=-10, max=10, step=0.1, value=6.97, description='Start Y'),\n         learning_rate=FloatSlider(min=0.01, max=0.95, step=0.01, value=0.1, description='Learning Rate'),\n         num_iterations=IntSlider(min=1, max=100, step=1, value=10, description='Iterations'))\n</code></pre> <p>The Challenge of Escaping Local Minima. We're stuck in a local minimum where, no matter how many steps we take, we remain trapped due to the nature of gradient descent</p> <p></p> <p>Gradient descent trapped</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#momentum-pull","title":"Momentum pull","text":"<p>Gradient Descent does not perform well when tackling a complicated function space. With each step of gradient descent, we compute the gradient and take a single step in the direction of that gradient. However, the gradient indicates the direction of the steepest ascent at that exact point, and if we find ourselves in a local minimum area, moving in the negative direction of the gradient might just plunge us deeper into that local minimum. This is problematic because our algorithm only \"looks\" one step ahead without any heuristic or corrective measures. How can we address this?</p> <p>Momentum is a technique that introduces a velocity term to the gradient updates. Instead of simply adjusting our position based on the current gradient, we now also consider the direction and magnitude of previous steps. Here's how it works:</p> <ul> <li> <p>Velocity Accumulation: We maintain a running average of the gradients from previous steps. This average is what we call the \"velocity\". The velocity helps us to keep moving in a consistent direction even if the gradient in the current step might suggest otherwise.</p> </li> <li> <p>Update Formula: First we compute the Momentum:</p> </li> </ul> \\[v_{t+1} = \\mu \\cdot v_{t} - \\alpha \\nabla f(x_t)\\] <ul> <li>The update rule for our position becomes:</li> </ul> \\[x_{t+1} = x_t + v_{t+1}\\] <p>Where:</p> <ul> <li> <p>\\(\\mu\\) is the momentum coefficient (typically between 0 and 1), where higher values make the algorithm \"remember\" previous gradients more strongly.</p> </li> <li> <p>\\(\\alpha\\) is the learning rate of the gradient</p> </li> </ul> <p>The Momentum leads to faster convergence - it helps to accelerate gradient descent in the relevant direction, potentially leading to faster convergence to the global minimum. By accumulating velocity, momentum allows the optimization to bypass small local minima that would otherwise trap the straightforward gradient descent. The choice of the momentum parameter \\(\\mu\\) is crucial. Too high, and the algorithm might overshoot; too low, and momentum might not provide significant benefits. Adding momentum increases the number of hyperparameters to tune, which can complicate the optimization process.</p> <p>By incorporating momentum, we essentially give our gradient descent a \"memory\" of past movements, which can smooth out the descent path over complex landscapes, making it more effective for functions with many local minima or flat regions.</p> <pre><code>class Optimizer:\n    # ... (previous methods and initialization remain unchanged)\n    def gradient_descent(\n        self,\n        start_point: np.ndarray,\n        learning_rate: float = 0.1,\n        num_iterations: int = 10,\n        momentum: float = 0\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Perform gradient descent optimization with optional momentum and regularization.\n\n        Args:\n            start_point (np.ndarray): Initial guess for the optimization.\n            learning_rate (float): Step size for each iteration.\n            num_iterations (int): Maximum number of iterations to perform.\n            momentum (float): Momentum factor if momentum is used.\n\n        Returns:\n            np.ndarray: Path of points visited during optimization.\n        \"\"\"\n\n        path = [start_point]\n\n        # Init the momentum velocity\n        velocity = np.zeros_like(start_point)\n\n        for _ in range(num_iterations):\n            grad = self.grad_f(start_point)\n\n            if momentum &gt; 0:\n                velocity = momentum * velocity - learning_rate * grad\n                new_point = start_point + velocity\n            else:\n                new_point = start_point - learning_rate * grad\n\n            start_point = new_point\n            path.append(new_point)\n\n        return np.array(path)\n\n\n    def compare_momentum_paths(\n        self,\n        start_point: np.ndarray,\n        learning_rate: float = 0.1,\n        num_iterations: int = 10,\n        momentum: float = 0.9,\n        min_max: Tuple[int, int] = (-15, 15)\n    ) -&gt; plt.Figure:\n        \"\"\"\n        Compare gradient descent paths with and without momentum.\n\n        Args:\n            start_point (np.ndarray): Initial guess for the optimization.\n            learning_rate (float): Step size for each iteration.\n            num_iterations (int): Maximum number of iterations to perform.\n            momentum (float): Momentum factor to use.\n            min_max (Tuple[int, int]): Min and max values on the grid.\n\n        Returns:\n            plt.Figure: Matplotlib figure object comparing paths with and without momentum.\n        \"\"\"\n\n        # Path without momentum\n        path_no_momentum = self.gradient_descent(\n            start_point,\n            learning_rate=learning_rate,\n            num_iterations=num_iterations,\n            momentum=0\n        )\n\n        # Path with momentum\n        path_with_momentum = self.gradient_descent(\n            start_point,\n            learning_rate=learning_rate,\n            num_iterations=num_iterations,\n            momentum=momentum\n        )\n\n        # Create the surface\n        x = y = np.linspace(min_max[0], min_max[1], 100)\n        X, Y = np.meshgrid(x, y)\n\n        Z = np.zeros_like(X)\n        for i in range(X.shape[0]):\n            for j in range(X.shape[1]):\n                Z[i, j] = self.f(np.array([X[i, j], Y[i, j]]))\n\n        # Create the figure and subplots\n        fig = plt.figure(figsize=(20, 10))\n        ax1 = fig.add_subplot(121, projection='3d')\n        ax2 = fig.add_subplot(122)\n\n        # 3D plot\n        surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n\n        # Compute function values for paths\n        Z_path_no_momentum = [self.f(point) for point in path_no_momentum]\n        Z_path_with_momentum = [self.f(point) for point in path_with_momentum]\n\n        # Plot paths\n        ax1.plot(path_no_momentum[:, 0], path_no_momentum[:, 1], Z_path_no_momentum, \n                'r.-', linewidth=2, markersize=5, label='No Momentum')\n        ax1.plot(path_with_momentum[:, 0], path_with_momentum[:, 1], Z_path_with_momentum, \n                'b.-', linewidth=2, markersize=5, label='With Momentum')\n\n        # Plot start and end points\n        point_size = 300\n        ax1.scatter(start_point[0], start_point[1], self.f(start_point), \n                    s=point_size, c='g', marker='*', label='Start Point')\n\n        # Last points for each path\n        ax1.scatter(path_no_momentum[-1, 0], path_no_momentum[-1, 1], self.f(path_no_momentum[-1]), \n                    s=point_size, c='r', marker='o', label='End (No Momentum)')\n        ax1.scatter(path_with_momentum[-1, 0], path_with_momentum[-1, 1], self.f(path_with_momentum[-1]), \n                    s=point_size, c='b', marker='o', label='End (With Momentum)')\n\n        ax1.set_xlabel('X')\n        ax1.set_ylabel('Y')\n        ax1.set_zlabel('Z')\n        ax1.set_title('3D Gradient Descent: Momentum Comparison')\n        ax1.legend()\n\n        # 2D Contour Plot\n        contour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\n\n        # Plot 2D paths\n        ax2.plot(path_no_momentum[:, 0], path_no_momentum[:, 1], 'r&gt;-', \n                linewidth=2, markersize=5, label='No Momentum')\n        ax2.plot(path_with_momentum[:, 0], path_with_momentum[:, 1], 'b&gt;-', \n                linewidth=2, markersize=5, label='With Momentum')\n\n        # Plot start and end points\n        ax2.scatter(start_point[0], start_point[1], \n                    s=point_size, c='g', marker='*', label='Start Point')\n        ax2.scatter(path_no_momentum[-1, 0], path_no_momentum[-1, 1], \n                    s=point_size, c='r', marker='o', label='End (No Momentum)')\n        ax2.scatter(path_with_momentum[-1, 0], path_with_momentum[-1, 1], \n                    s=point_size, c='b', marker='o', label='End (With Momentum)')\n\n        ax2.set_xlabel('X')\n        ax2.set_ylabel('Y')\n        ax2.set_title('2D Contour: Momentum Comparison')\n        ax2.legend()\n\n        # Add color bars\n        fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n        fig.colorbar(contour, ax=ax2)\n\n        plt.tight_layout()\n        plt.close(fig)\n        return fig\n\n\ndef build_grad_and_plot(start_x, start_y, learning_rate, num_iterations, momentum):\n    start_point = np.array([start_x, start_y])\n    optimizer = Optimizer(f3d, grad_f3d)\n    # Compare paths with different momentum values\n    return optimizer.compare_momentum_paths(\n        start_point, \n        learning_rate=learning_rate, \n        num_iterations=num_iterations, \n        momentum=momentum\n    )\n\n\n# Create interactive plot\ninteract(build_grad_and_plot, \n         start_x=FloatSlider(min=-10, max=10, step=0.1, value=7.11, description='Start X'),\n         start_y=FloatSlider(min=-10, max=10, step=0.1, value=6.97, description='Start Y'),\n         learning_rate=FloatSlider(min=0.01, max=0.95, step=0.01, value=0.1, description='Learning Rate'),\n         num_iterations=IntSlider(min=1, max=100, step=1, value=10, description='Iterations'),\n         momentum=FloatSlider(min=0, max=0.95, step=0.01, value=0, description='Momentum'))\n</code></pre> <p>You can see how momentum helps in finding the global minimum, performing significantly better than pure gradient descent.</p> <p></p> <p>Gradient Descent VS Gradient Descent with Momentum</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2024/12/12/gradient-descent-ninja-with-momentum/#sign-off","title":"Sign off","text":"<p>Gradient descent is a powerful tool for optimization, but it's not without challenges, especially in complex landscapes riddled with local minima. While basic gradient descent might suffice for simple functions, real-world applications demand more sophisticated approaches. Momentum is key, acting like a memory of past directions to help navigate through local traps towards global minima. Remember, the choice of hyperparameters like learning rate and momentum can make or break your optimization journey. Keep experimenting, keep learning, and always aim for that global minimum!</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning","Deep Learning","Python Visualization","Matplotlib"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/","title":"Mastering Neural Network - Linear Layer and SGD","text":"<p>The human brain remains one of the greatest mysteries, far more complex than anything else we know. It is the most complicated object in the universe that we know of. The underlying processes and the source of consciousness, as well as consciousness itself, remain unknown. Neural Nets are good for popularizing Deep Learning algorithms, but we can't say for sure what mechanism behind biological Neural Networks enables intelligence to arise.</p> <p></p> <p>Visualized Boundaries</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#what-well-build","title":"What We'll Build","text":"<p>In this chapter, we'll construct a Neural Network from the ground up using only Python and NumPy. By building everything from scratch, we'll demonstrate that Deep Learning isn't a black box - it's a system we can fully deconstruct and understand. This hands-on approach will show how the fundamental principles of neural networks can be implemented with minimal tools, demystifying the core concepts that power modern machine learning.</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#prerequisites","title":"Prerequisites","text":"<p>To understand this chapter, I recommend that you to check my previous post about gradient descent: Gradient Descent - Downhill to the Minima. We will use gradient descent to train our first neural network.</p> <p>The challenge our network is going to solve is the classical linearly separable pattern, which we use as a classification problem. You can check my previous post for more details about the classification problem and the training process: Dive into Learning from Data - MNIST Video Adventure</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#training-in-a-nutshell","title":"Training in a Nutshell","text":"<p>To train our network, we compute the gradient and update parameters in the direction of the steepest descent, following the negative gradient. If you want to know more about the gradient, you can check my post on why the gradient points upwards and downhill to the minima</p> <p>In short, the gradient is the most important operation in the training process. We need to follow the negative gradient direction to train our model. Backpropagation is the process of propagating the error backward through the network to update the model's parameters.</p> <p>The training consists of two main phases:</p> <ul> <li> <p><code>forward</code>: computes the model's output for a given input, storing intermediate values required for gradient computation.</p> </li> <li> <p><code>backward</code>: calculates the derivatives with respect to each parameter using the Chain Rule of Calculus. Gradient are then used to adjust the parameters in the direction that minimizes the loss.</p> </li> </ul>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#module","title":"Module","text":"<p>To maintain consistency across different components of our neural network, we'll use a base class called <code>Module</code>. This class provides a structure for implementing layers, activations, and other elements. The <code>Parameter</code> class makes it easy to manage and identify model parameters during optimization. It allows the <code>Optimizer</code> to access and modify parameter data and gradients in a structured way after the <code>forward</code> and <code>backward</code> steps.</p> <p>Here's the implementation:</p> <pre><code>from dataclasses import dataclass\n\n\n@dataclass\nclass Parameter:\n    r\"\"\"\n    Represents a trainable parameter in a neural network model.\n\n    Attributes:\n        name (str): The name of the parameter, typically identifying its purpose (e.g., \"weights\", \"biases\").\n        data (np.ndarray): The current value of the parameter, stored as a NumPy array.\n        grad (np.ndarray): The gradient of the parameter, calculated during the backward pass.\n    \"\"\"\n\n    name: str\n    data: np.ndarray\n    grad: np.ndarray\n\n\nclass Module:\n    r\"\"\"\n    A base class for all neural network components.\n    Provides a consistent interface for forward and backward passes,\n    and allows for the implementation of common operations like\n    parameter updates.\n    \"\"\"\n\n    def __call__(self, *args, **kwargs):\n        r\"\"\"\n        Enables the object to be called like a function. Internally,\n        this redirects the call to the `forward` method of the module,\n        which computes the model's output for the given input.\n\n        Args:\n            *args: Positional arguments passed to the `forward` method.\n            **kwargs: Keyword arguments passed to the `forward` method.\n\n        Returns:\n            The output of the `forward` method.\n        \"\"\"\n        return self.forward(*args, **kwargs)\n\n    def forward(self, *args, **kwargs):\n        r\"\"\"\n        Forward Pass: Compute the model's output for a given input,\n        accumulating the values required for gradient computation.\n        \"\"\"\n        raise NotImplementedError\n\n    def backward(self, *args, **kwargs):\n        r\"\"\"\n        Backward Pass: Calculate the derivative of the loss function\n        with respect to each parameter using the chain rule of calculus.\n        These derivatives (gradients) are then used to adjust the\n        parameters in the direction that minimizes the loss.\n        \"\"\"\n        raise NotImplementedError\n\n    def parameters(self) -&gt; List[Parameter]:\n        r\"\"\"Returns all trainable parameters of the module.\"\"\"\n        return []\n\n    def zero_grad(self):\n        r\"\"\"\n        Resets the gradients of all parameters in the module to zero.\n        This is typically done at the start of a new optimization step\n        to prevent accumulation of gradients from previous steps.\n        \"\"\"\n        for param in self.parameters():\n            param.grad.fill(0)\n</code></pre>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#weights-initialization","title":"Weights initialization","text":"<p>Before implementing the Linear Layer, it's essential to consider the importance of weight initialization! Proper weight initialization can significantly affect the performance of your neural network. For more details on this topic, you can refer to Weight Initialization Methods in Neural Networks.</p> <p>In short, the choice of initialization depends on the activation function used in your network:</p> <ul> <li>LeakyReLU: The best weight initialization for this activation function is the Leaky He method.</li> <li>Sigmoid: For Sigmoid activation, the Xavier-Glorot method works best.</li> </ul> <p>To simplify the implementation of the Dense Layer, we can leverage the parameter, which handles different initialization methods effectively.</p> <pre><code>import numpy as np\nfrom typing import Literal\n\n# Define a custom type alias for initialization methods\nInitMethod = Literal[\"xavier\", \"he\", \"he_leaky\", \"normal\", \"uniform\"]\n\ndef parameter(\n    input_size: int,\n    output_size: int,\n    init_method: InitMethod = \"xavier\",\n    gain: float = 1,\n    alpha: float = 0.01\n) -&gt; np.ndarray:\n    weights = np.random.randn(input_size, output_size)\n\n    if init_method == \"xavier\":\n        std = gain * np.sqrt(1.0 / input_size)\n        return std * weights\n    if init_method == \"he\":\n        std = gain * np.sqrt(2.0 / input_size)\n        return std * weights\n    if init_method == \"he_leaky\":\n        std = gain * np.sqrt(2.0 / (1 + alpha**2) * (1 / input_size))\n        return std * weights\n    if init_method == \"normal\":\n        return gain * weights\n    if init_method == \"uniform\":\n        return gain * np.random.uniform(-1, 1, size=(input_size, output_size))\n\n    raise ValueError(f\"Unknown initialization method: {init_method}\")\n</code></pre> <p>For a comprehensive overview of weight initialization techniques, visit Weight Initialization Methods in Neural Networks.</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#forward-mode-for-linear-layer","title":"Forward Mode for Linear Layer","text":"<p>At layer \\(i\\), the linear transformation is defined as:</p> \\[\\tag{linear step} \\label{eq:linear_step} A_i(\\mathbf{x}) = \\mathbf{x}\\mathbf{w}_i + b_i\\] <p>Here, \\(\\mathbf{x}\\) is the input to layer \\(i\\), \\(\\mathbf{w}_i\\) represents the weights, and \\(\\mathbf{b}_i\\) denotes the biases of the layer. The activation function, \\(\\sigma\\), is then applied to introduce non-linearity after the linear transformation.</p> <p>The single step \\(i\\) of the network can be written explicitly as:</p> \\[f_i(\\mathbf{x}) = \\sigma(A_i(\\mathbf{x}))\\] <p>where \\(A_i(\\mathbf{x})\\) denotes the linear transformation at layer \\(i\\).</p> <p>In a neural network, the input data undergoes a series of transformations layer by layer, resulting in the final output:</p> \\[f(\\mathbf{x}) = \\sigma(A_L(\\sigma(A_{L-1}( \\dots \\sigma(A_1(\\mathbf{x})) \\dots ))).\\] <p>Using functional composition, the deep neural network is compactly expressed as:</p> \\[\\tag{deep neural net} \\label{eq:deep_nn} f(\\mathbf{x}) = A_L \\circ \\sigma \\circ A_{L-1} \\circ \\dots \\circ \\sigma \\circ A_1 (\\mathbf{x})\\] <p>The forward pass computes these transformations sequentially, storing intermediate values for use during the backward pass.</p> <p>We can implement the backbone of the <code>Linear</code> layer and its <code>forward</code> method based on the equations.</p> <pre><code>class Linear(Module):\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        init_method: Literal[\"xavier\", \"he\", \"he_leaky\", \"normal\", \"uniform\"] = \"xavier\"\n    ):\n        self.input: np.ndarray = None\n\n        self.weights: np.ndarray = parameter(input_size, output_size, init_method)\n        self.d_weights: np.ndarray = np.zeros_like(self.weights)\n\n        self.biases: np.ndarray = np.zeros((1, output_size))\n        self.d_biases: np.ndarray = np.zeros_like(self.biases)\n\n    def forward(self, x: np.ndarray) -&gt; np.ndarray:\n        self.input = x\n        x1 = x @ self.weights + self.biases\n        return x1\n</code></pre>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#backward-mode-for-linear-layer","title":"Backward Mode for Linear Layer","text":"<p>In backpropagation, the goal is to compute the gradient of the loss function \\(\\mathcal{L}\\) with respect to every parameter in the network, so that the model parameters can be updated to minimize the loss.</p> <p>This is achieved by applying the chain rule of calculus, which allows the gradients to be propagated backward from the output layer to the input layer.</p> <p>In calculus, the chain rule describes how to compute the derivative of a composite function. For two functions \\(f(g(x))\\), the chain rule states:</p> \\[\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x)\\] <p>In neural networks, the loss function \\(\\mathcal{L}\\) depends on multiple layers of transformations applied to the input, forming a composition of functions:</p> \\[\\mathcal{L} = f(A_L(\\sigma(A_{L-1}(\\dots \\sigma(A_1(x)) \\dots))))\\] <p>To compute the gradient \\(\\nabla \\mathcal{L}\\) with respect to each parameter, we apply the chain rule recursively for this composition.</p> <p>We can express the gradient flow through the network as:</p> \\[\\tag{backprop} \\label{eq:backprop} \\nabla f = \\nabla A_1 \\circ \\sigma' \\circ \\nabla A_2 \\circ \\dots \\circ \\sigma' \\circ \\nabla A_L (\\mathcal{L})\\] <p>The chain rule propagates gradients backward: starting with the gradient of the loss (\\(\\nabla \\mathcal{L}\\)) with respect to the output (\\(x_{\\text{out}}\\)), gradients are successively multiplied by \\(\\nabla A_i\\) and \\(\\sigma'\\) for each layer. This backpropagates information through the network.</p> <p>This structured approach ensures that gradients are correctly computed and used to update all parameters.</p> <p>In the <code>backward</code> method, we compute the gradients of the loss with respect to the network's parameters. This is done by using the gradients of the output (denoted as <code>d_out</code>) and backpropagating them through the network using the chain rule. Essentially, the backward method applies the chain of gradients to adjust the weights and biases, improving the network's performance over time.</p> <p>The gradient vector with respect to the parameters (weights \\(w_i\\)\u200b and biases \\(b_i\\)\u200b) is expressed as:</p> \\[\\nabla A_i(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial A_i(\\mathbf{x})}{\\partial \\mathbf{w_i}} \\\\ \\frac{\\partial A_i(\\mathbf{x})}{\\partial \\mathbf{b_i}} \\end{bmatrix} \\] <p>Partial Derivative with Respect to Weights</p> <p>The partial derivative of the affine transformation \\(A_i\u200b(x)\\) with respect to the weights \\(\\mathbf{w}_i\\)\u200b is:</p> \\[\\frac{\\partial (\\mathbf{w}_i \\mathbf{x} + b_i)}{\\partial \\mathbf{w_i}}=\\mathbf{x}^T\\] <p>Why Do We Have \\(\\mathbf{x}^T\\)?</p> <p>\\(\\mathbf{w}_i\\) is a \\(m \\times n\\) matrix and \\(\\mathbf{x}\\) is a \\(n \\times 1\\) column vector, resulting in \\(A_i(\\mathbf{x})\\) being a \\(m \\times 1\\) vector.</p> <p>The derivative \\(\\frac{\\partial A_i(\\mathbf{x})}{\\partial \\mathbf{w}_i}\\) represents the sensitivity of each element of \\(A_i(\\mathbf{x})\\) to changes in each element of \\(\\mathbf{w}_i\\). Specifically, \\(\\mathbf{w}_i\\) contains \\(m \\times n\\) elements. For each scalar output in \\(A_i(\\mathbf{x})\\), its derivative with respect to each element in \\(\\mathbf{w}_i\\) involves \\(\\mathbf{x}\\), as:</p> \\[ \\frac{\\partial (\\mathbf{w}_i \\mathbf{x})}{\\partial \\mathbf{w}_i} = \\mathbf{x}^T \\] <ul> <li>Each row of \\(\\mathbf{x}^T\\) corresponds to the gradient contribution for one output in \\(A_i(\\mathbf{x})\\).</li> <li>Mathematically, \\(\\mathbf{x}^T\\) reshapes the gradient into a form that matches the structure of \\(\\mathbf{w}_i\\).</li> </ul> <p>Intuition Behind \\(\\mathbf{x}^T\\):</p> <ul> <li>Transpose Role: \\(\\mathbf{x}^T\\) rearranges the input \\(\\mathbf{x}\\) so that its dimensions align correctly with the gradient \\(d_{\\text{out}}\\) in the backward pass.</li> <li>Gradient Flow: Each column of \\(\\mathbf{w}_i\\) corresponds to how strongly a specific input feature contributes to each output. The transpose ensures that we sum over the batch dimension (if present) and correctly propagate the gradient back to the weights.</li> </ul> <p>By the chain rule we need to use the gradient of the loss with respect to the output of the current layer \\(d_{\\text{out}}\\):</p> \\[\\frac{\\partial L}{\\partial \\mathbf{w}_i} = \\mathbf{x}^T \\cdot d_{\\text{out}}\\] <p>This is efficiently computed in matrix form as:</p> <pre><code>dw = self.input.T @ d_out\n</code></pre> <p><code>self.input.T</code> is the transpose of the input matrix \\(\\mathbf{x}\\), which aligns the dimensions for the matrix multiplication. <code>d_out</code> represents the gradient of the loss with respect to the output of the current layer, \\(\\frac{\\partial L}{\\partial A_i}\\).</p> <p>Partial Derivative with Respect to Biases</p> <p>The partial derivative of the affine transformation \\(A_i(\\mathbf{x})\\) with respect to the biases \\(b_i\\) is:</p> \\[\\frac{\\partial (\\mathbf{w}_i \\mathbf{x} + b_i)}{\\partial \\mathbf{b_i}}=1\\] <p>In backpropagation, the chain rule is used to propagate gradients from the loss function \\(L\\) through the layers of the network.</p> <p>Start with the total derivative of the loss with respect to the bias:</p> \\[\\frac{\\partial L}{\\partial b_i} = \\frac{\\partial L}{\\partial A_i} \\cdot \\frac{\\partial A_i}{\\partial b_i}. \\] <p>Substituting, we get:</p> \\[\\frac{\\partial L}{\\partial b_i} = \\frac{\\partial L}{\\partial A_i} \\cdot 1 = \\frac{\\partial L}{\\partial A_i}. \\] <p>In the case of a batch of data, the bias gradient accumulates contributions from all samples in the batch. This is implemented as:</p> <pre><code>db = np.sum(d_out, axis=0, keepdims=True)\n</code></pre> <p><code>np.sum(d_out, axis=0, keepdims=True)</code> computes the sum of gradients across all samples in the batch, resulting in a single gradient vector for the biases. <code>keepdims=True</code> ensures that the resulting array maintains the correct dimensionality shape <code>(1, output_size)</code> for compatibility with subsequent computations.</p> <p>Finally, the gradient of affine transformation is defined as:</p> \\[\\tag{layer gradient} \\label{eq:layer_gradient} \\nabla A_i(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial A_i(\\mathbf{x})}{\\partial \\mathbf{w_i}} \\\\ \\frac{\\partial A_i(\\mathbf{x})}{\\partial \\mathbf{b_i}} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}^T \\\\ 1 \\end{bmatrix}\\] <p>Here's the full implementation of the <code>Linear</code> layer:</p> <pre><code>class Linear(Module):\n    r\"\"\"\n    A linear layer in a neural network that performs an affine transformation \n    followed by the addition of a bias term. It is the core building block \n    for many neural network architectures.\n\n    Attributes:\n        input (np.ndarray): The input to the linear layer, used during the forward and backward passes.\n        weights (np.ndarray): The weights of the linear transformation, initialized using the specified method.\n        d_weights (np.ndarray): The gradients of the weights with respect to the loss, computed during backpropagation.\n        biases (np.ndarray): The biases added during the linear transformation.\n        d_biases (np.ndarray): The gradients of the biases with respect to the loss, computed during backpropagation.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        init_method: InitMethod = \"xavier\"\n    ):\n        r\"\"\"\n        Initializes a Linear layer with the given input size and output size.\n        The weights are initialized using the specified method (e.g., Xavier, He, etc.), \n        and biases are initialized to zeros.\n\n        Args:\n            input_size (int): The number of input features to the layer.\n            output_size (int): The number of output features from the layer.\n            init_method (InitMethod): The initialization method for the weights (default is \"xavier\").\n\n        Attributes:\n            input (np.ndarray): Stores the input to the layer during the forward pass.\n            weights (np.ndarray): The weights for the linear transformation.\n            d_weights (np.ndarray): The gradients for the weights, calculated during backpropagation.\n            biases (np.ndarray): The biases for the linear transformation.\n            d_biases (np.ndarray): The gradients for the biases, calculated during backpropagation.\n        \"\"\"\n\n        self.input: np.ndarray = None\n\n        self.weights: np.ndarray = parameter(input_size, output_size, init_method)\n        self.d_weights: np.ndarray = np.zeros_like(self.weights)\n\n        self.biases: np.ndarray = np.zeros((1, output_size))\n        self.d_biases: np.ndarray = np.zeros_like(self.biases)\n\n    def forward(self, x: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"\n        Computes the forward pass of the linear layer. The input is multiplied by \n        the weights and biases are added, producing the output.\n\n        Args:\n            x (np.ndarray): The input to the layer, typically the output of the previous layer.\n\n        Returns:\n            np.ndarray: The output of the linear transformation (after applying weights and biases).\n        \"\"\"\n\n        self.input = x\n        return x @ self.weights + self.biases\n\n    def backward(self, d_out: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"\n        Computes the backward pass for the linear layer, calculating the gradients \n        with respect to the weights and biases, and the gradient of the input \n        to be passed to the next layer.\n\n        Args:\n            d_out (np.ndarray): The gradient of the loss with respect to the output of this layer. \n\n        Returns:\n            np.ndarray: The gradient of the loss with respect to the input of this layer, \n                        which will be passed to the next layer.\n        \"\"\"\n\n        # Compute gradients for weights and biases\n        self.d_weights = self.input.T @ d_out\n        self.d_biases = np.sum(d_out, axis=0, keepdims=True)\n\n        # Chain rule!\n        layer_out = d_out @ self.weights.T\n        return layer_out\n\n    def parameters(self):\n        r\"\"\"\n        Retrieves the parameters of the linear layer, including weights and biases, \n        along with their corresponding gradients. This method is typically used for \n        optimization purposes during training.\n\n        Returns:\n            list[Parameter]: A list of `Parameter` objects, where each object contains:\n                - `name` (str): The name of the parameter (e.g., \"weights\", \"biases\").\n                - `data` (np.ndarray): The parameter values (e.g., weights or biases).\n                - `grad` (np.ndarray): The gradients of the parameter with respect to the loss.\n        \"\"\"\n\n        return [\n            Parameter(\n                name=\"weights\",\n                data=self.weights,\n                grad=self.d_weights\n            ),\n            Parameter(\n                name=\"biases\",\n                data=self.biases,\n                grad=self.d_biases\n            ),\n        ]\n</code></pre>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#loss","title":"Loss","text":"<p>To check the accuracy of the model, we compare the predicted output, \\(y_{\\text{pred}}\\), with the true labels, \\(y_{\\text{target}}\\). The measure of this discrepancy is what we call the loss function or \\(\\mathcal{L}\\).</p> <p>For a classification problem with 2 classes, we deal with a Binary Classification Task, and the most effective loss function in this case is the Binary Cross-Entropy Loss. </p> <p>The binary cross-entropy formula for two classes (0 and 1) is:</p> \\[\\tag{BCE} \\label{eq:bce} \\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\] <p>Binary Cross-Entropy derivative</p> \\[\\tag{BCE derivative} \\label{eq:bce_derivative} \\frac{\\partial \\mathcal{L}}{\\partial p_i} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{p_i - y_i}{p_i (1 - p_i)}\\] <p>For more details, check out my post on Cross-Entropy.</p> <p>The complete implementation of the BCE module:</p> <pre><code>import numpy as np\n\n\nclass BCELoss(Module):\n    def forward(\n        self, pred: np.ndarray, target: np.ndarray, epsilon: float = 1e-7\n    ) -&gt; np.ndarray:\n        loss = -(\n            target * np.log(pred + epsilon) + \n            (1 - target) * np.log(1 - pred + epsilon)\n        )\n\n        # Average the loss over the batch size\n        return np.mean(loss)\n\n    def backward(\n        self, pred: np.ndarray, target: np.ndarray, epsilon: float = 1e-7\n    ) -&gt; np.ndarray:\n        grad = (pred - target) / (pred * (1 - pred) + epsilon)\n\n        # you should not average the gradients!\n        # instead, you should return the gradient for each example,\n        # as gradients represent how much the loss changes with\n        # respect to each individual prediction.\n\n        return grad\n</code></pre> <p>For more details, check out my post on Cross-Entropy</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#sigmoid","title":"Sigmoid","text":"<p>The sigmoid activation function is defined as:</p> \\[\\tag{sigmoid function} \\label{eq:sigmoid_function} \\sigma(\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{x}}}\\] <p>The derivative of the sigmoid activation function is:</p> \\[\\tag{sigmoid derivative} \\label{eq:sigmoid_derivative} \\sigma'(\\mathbf{x}) = \\sigma(\\mathbf{x}) \\cdot (1 - \\sigma(\\mathbf{x})) \\] <p>Check out this post to learn more about sigmoid, logits, and probabilities.</p> <p>The <code>Sigmoid</code> class inherits from <code>Module</code> and implements both the <code>forward</code> and <code>backward</code> methods for the sigmoid activation function. The <code>forward</code> method applies the sigmoid function to the input data, and the <code>backward</code> method computes the gradient required for the backward pass during model optimization (training).</p> <pre><code>class Sigmoid(Module):\n    r\"\"\"Sigmoid activation function and its derivative for backpropagation.\"\"\"\n\n    def forward(self, x: np.ndarray):\n        # Apply the Sigmoid function element-wise\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n\n    def backward(self, d_out: np.ndarray):\n        # Derivative of the Sigmoid function: sigmoid * (1 - sigmoid)\n        ds = self.output * (1 - self.output)\n        return d_out * ds\n</code></pre> <p><code>d_out</code> represents the gradient from the output of the current layer, to propagate errors backward through the network.</p> <p>Here's how the <code>Sigmoid</code> class can be used in practice:</p> <pre><code># Example inputs\ninput_x = np.array([0.5, 1.0, -1.5])\n\n# Initialize the Sigmoid activation\nactivation = Sigmoid()\n\n# Forward pass\noutput = activation(input_x)\nprint(\"Output of Sigmoid:\", output)\n\n# Backward pass\nd_out = np.array([0.1, 0.2, 0.3])  # Example gradient from next layer\ngradient = activation.backward(d_out)\nprint(\"Gradient of Sigmoid:\", gradient)\n</code></pre>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>In training neural networks, Stochastic Gradient Descent (SGD) is one of the popular optimization algorithms. It aims to minimize the loss function \\(\\mathcal{L}\\) by updating model parameters in the direction that reduces the loss.</p> <p>The key idea behind SGD is that, rather than using the entire dataset to compute gradients (as in batch gradient descent), we use only a single data point or a mini-batch to compute the gradients and update parameters. This makes the optimization process faster and more scalable.</p> <p>In SGD, the parameter update rule is given by:</p> \\[\\mathbf{w}_i = \\mathbf{w}_i - \\eta \\nabla \\mathcal{L}(\\mathbf{w}_i)\\] <p>Where \\(\\eta\\) is the learning rate and \\(\\nabla \\mathcal{L}(\\mathbf{w}_i)\\) is the gradient of the loss with respect to the parameter \\(\\mathbf{w}_i\\).</p> <p>To help accelerate convergence, momentum is introduced. It helps smooth the updates and avoid oscillations by incorporating the previous gradient into the current update. First we compute the Momentum:</p> \\[v_{t+1} = \\mu \\cdot v_{t} - \\alpha \\nabla f(x_t)\\] <p>The update rule for our position becomes:</p> \\[x_{t+1} = x_t + v_{t+1}\\] <p>For more datils check my post: Gradient Descent Ninja with Momentum</p> <p>Implementation:</p> <pre><code>class SGD:\n    def __init__(\n        self,\n        lr: float = 0.01,\n        momentum: float = 0.0,\n    ):\n        \"\"\"\n        Initializes the Stochastic Gradient Descent (SGD) optimizer.\n\n        - **Learning Rate (`lr`)**: Controls how big the updates are. A larger learning rate might result in faster convergence but could also cause instability.\n        - **Momentum (`momentum`)**: Helps accelerate the gradient descent process by adding inertia to the parameter updates. This makes the algorithm more efficient by using past gradients to update parameters in a more \"smooth\" way.\n\n        The optimizer aims to update the model's parameters in a way that reduces the loss function, allowing the model to improve its performance over time.\n\n        Args:\n            lr (float): Learning rate for updating the model's parameters.\n            momentum (float): Momentum for accelerating gradient descent.\n        \"\"\"\n        self.lr = lr\n        self.momentum = momentum\n        self.velocity = {}  # Store momentum for each parameter\n\n    def step(self, module: Module):\n        \"\"\"\n        Performs a single update step on the parameters using the gradients.\n\n        Args:\n            module (Module): The module (e.g., layer) whose parameters are being updated.\n        \"\"\"\n        for param in module.parameters():\n            param_id = param.name\n\n            # Initialize velocity if not exists\n            if param_id not in self.velocity:\n                self.velocity[param_id] = np.zeros_like(param.data)\n\n            grad = param.grad.copy()  # Make a copy to avoid modifying original\n\n            # Update with momentum\n            self.velocity[param_id] = self.momentum * self.velocity[param_id] - self.lr * grad\n\n            # Update parameters\n            param.data += self.velocity[param_id]\n</code></pre> <p>Example:</p> <pre><code># Initialize the SGD optimizer with custom settings\noptimizer = SGD(lr=0.01, momentum=0.9)\n\n# Perform a parameter update for each layer/module in the network\noptimizer.step(module)\n</code></pre> <p>This method updates the model parameters by using the gradients calculated during the backward pass, applying momentum, weight decay, and gradient clipping where needed.</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#putting-it-all-together-training-a-model-with-sgd","title":"Putting It All Together: Training a Model with SGD","text":"<p>Let's use the previously defined <code>Linear</code> layer, <code>BCELoss</code> function, and <code>SGD</code> optimizer to train a binary classification model on some random input data. The process involves a forward pass, a backward pass, and parameter updates using Stochastic Gradient Descent (SGD).</p> <p>We are going to use <code>make_classification</code> from <code>sklearn</code>. Let's import it and visualize the data.</p> <pre><code>from sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n\n\n# Default params\nn_samples = 100\nfeatures = 2\n\n# Create random input data\nx, y_target = make_classification(\n    n_samples=n_samples,\n    n_features=features,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    flip_y=0,\n    random_state=1\n)\n\ny_target = y_target.reshape(-1, 1)\n\n# Plot the data\nplt.figure(figsize=(8, 6))\n\n# Create scatter plot\nscatter = plt.scatter(\n    x[:, 0],\n    x[:, 1],\n    c=y_target.flatten(), \n    cmap=plt.cm.Paired, \n    edgecolor='k', \n    s=100,\n)\n\n# Title and labels\nplt.title(\"Classification Data\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\n\n# Add color legend using scatter's 'c' values\nhandles, labels = scatter.legend_elements()\n\n# Modify labels to add custom class names\ncustom_labels = ['Class 0', 'Class 1']  # Custom labels for each class\n\n# Use the custom labels in the legend\nplt.legend(handles, custom_labels, title=\"Classes\", loc=\"lower left\")\n\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>This dataset represents a linearly separable pattern with two classes, which the network must classify. Since data generation can vary slightly, you can experiment by creating and visualizing your own datasets. In this example, we use <code>random_state=1</code> to fix the data generation for reproducibility, but feel free to change it to explore different patterns.</p> <p>Also, let's define function for plotting decision boundaries</p> <pre><code>def plot_decision_boundaries(model, X, y, bins=500):\n    # Set the limits of the plot\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    # Generate a grid of points\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, bins), np.linspace(y_min, y_max, bins))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n\n    # Get the predicted class for each point in the grid\n    Z = model.forward(grid)  # Only get the predicted output\n    Z = (Z &gt; 0.5).astype(int)  # Assuming binary classification\n\n    # Reshape the predictions back to the grid shape\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundary\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), edgecolors='k', marker='o', s=30, label='Data Points')\n    plt.title('Decision Boundary')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.colorbar()\n    plt.tight_layout()\n    plt.grid(True)\n    plt.show()\n</code></pre> <p>Create the model, activation, loss function, and optimizer:</p> <pre><code># Linear layer with 3 input features and 1 output.\nmodel = Linear(input_size=features, output_size=1, init_method=\"xavier\")\n\n# Sigmoid activation\nactivation = Sigmoid()\n\n# BCE calculate the loss between predicted and actual values\nbce = BCELoss()\n\n# optimizer with a learning rate of 0.01, and momentum of 0.9\noptimizer = SGD(lr=0.01, momentum=0.9)\n</code></pre> <p>Training Loop:</p> <p>We'll train the model over 10 epochs. At each epoch, we:</p> <ul> <li> <p>Perform a forward pass: Compute the predicted output using the model.</p> </li> <li> <p>Calculate the loss using <code>BCELoss</code>.</p> </li> <li> <p>zero_grad before the <code>backward</code> step</p> </li> <li> <p>Perform a backward pass: Compute the gradients of the loss with respect to the parameters.</p> </li> <li> <p>Update parameters: Use the optimizer to update the model weights.</p> </li> </ul> <p>Here's the training loop:</p> <pre><code>n_epoch = 10\n\nfor epoch in range(n_epoch):\n    # Forward\n    output = model(x)\n    y_pred = activation(output)\n    loss = bce(y_pred, y_target)\n\n    model.zero_grad()\n\n    # Backward\n    grad = bce.backward(y_pred, y_target)\n    grad = activation.backward(grad)\n    model.backward(grad)\n\n    optimizer.step(model)\n\n    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Epoch 0, Loss: 0.2021\nEpoch 1, Loss: 0.1668\nEpoch 2, Loss: 0.1235\nEpoch 3, Loss: 0.0893\nEpoch 4, Loss: 0.0655\nEpoch 5, Loss: 0.0485\nEpoch 6, Loss: 0.0361\nEpoch 7, Loss: 0.0268\nEpoch 8, Loss: 0.0200\nEpoch 9, Loss: 0.0151\n</code></pre> <p>Almost perfect score! Let's plot the decision boundaries.</p> <pre><code>plot_decision_boundaries(model, x, y_target)\n</code></pre> <p>Output:</p> <p></p> <p>Visualized Boundaries</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/22/mastering-neural-network---linear-layer-and-sgd/#sign-off","title":"Sign off","text":"<p>We've broken down the fundamental building blocks of training a neural network. The journey starts with forward and backward passes, leveraging gradients to adjust model parameters and minimize loss. By implementing layers like <code>Linear</code>, activation functions like <code>Sigmoid</code>, and optimization algorithms like <code>SGD</code>, we create a system capable of learning from data.</p> <p>Each component - forward computation, backpropagation, and optimization plays a vital role in ensuring the network trains effectively. The concepts of the chain rule, gradient clipping, and momentum enhance stability and convergence during training.</p> <p>Together, these elements form the backbone of modern deep learning, providing the tools needed to solve complex problems with elegance and precision. Whether you're experimenting with binary classification or scaling to larger networks, the same principles apply, making this modular approach robust and versatile.</p>","tags":["Gradient Descent","Backpropagation","Stochastic Gradient Descent","Linear Layer","Sigmoid Activation","Binary Cross-Entropy","Training Loop","Optimization","Neural Networks","Loss Function","Machine Learning"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/","title":"The journey from Logits to Probabilities","text":"<p>Logits are the raw outputs generated by a model before applying any activation function, such as the sigmoid or softmax functions. These values are unbounded, meaning they can be positive, negative, or zero. Logits represent the model's unnormalized confidence scores for assigning an input to different classes.</p> <p></p> <p>Logits vs Probabilities</p> <p>In a classification problem, especially in multi-class classification, the model produces a set of logits\u2014one for each class. These logits indicate the model's relative confidence in each class without being normalized into probabilities.</p>","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"<p>Let's consider a 5-class classification problem. The model outputs the following logits, which are generated using <code>np.random.randn(5)</code>. To ensure reproducibility, we set the random seed using <code>np.random.seed(42)</code>:</p> <pre><code>import numpy as np\nnp.random.seed(42)\n\nlogits = np.random.randn(5)\nlogits\n</code></pre> <p>Output:</p> <pre><code>array([ 0.49671415, -0.1382643,  0.64768854,  1.52302986, -0.23415337])\n</code></pre> <p>These are the raw outputs. To convert them into probabilities, you can apply the sigmoid function.</p> <p>Softmax for Multi-Class classification</p> <p>For multi-class classification, we typically use the softmax function rather than the sigmoid function to convert logits to probabilities. The softmax function ensures all probabilities sum to 1, which is essential for multi-class problems.</p> <p>These logits represent the raw, unnormalized scores assigned by the model to each of the 5 classes, they can take any value - positive, negative, or zero. </p>","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#inside-the-linear-layer","title":"Inside the Linear Layer","text":"<p>The Linear Layer performs a key operation: a linear transformation. This is represented as:</p> \\[\\hat{x}_{i+1} = \\sigma(\\mathbf{w}_i^T \\mathbf{x} + b_i)\\] <p>Here \\(\\mathbf{w}_i^T\\) are the layer's weights and \\(b_i\\) represents the biases. The term \\(\\mathbf{w}_i^T \\mathbf{x} + b_i\\) applies a matrix multiplication (weights times the input) and adds the biases. We can simulate this step with the following code:</p> <pre><code>import numpy as np\n\nW = np.random.randn(5, 5)  # Random weights\nx = np.random.randn(5)     # Input vector\nb = np.ones(5)             # Biases\n\nx_1 = W @ x + b            # Linear transformation\nx_1\n</code></pre> <p>Output:</p> <pre><code>array([ 4.99851316,  1.01778965, -0.74321057,  0.75818997, -0.80840313])\n</code></pre> <p>In this example, we simulate the computation inside a model layer to create the logits-raw scores that are unbounded and not yet normalized.</p> <p>To make these logits interpretable as probabilities for each class, we use an activation function like the sigmoid function. The sigmoid maps these raw values into a range between 0 and 1, making them easier to interpret as class probabilities. For multi-class classification, we use the softmax function, which converts these raw values into a probability distribution that sums to 1.</p>","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#sigmoid","title":"Sigmoid","text":"<p>The sigmoid function is defined as:</p> \\[\\tag{sigmoid function} \\label{eq:sigmoid_function} \\sigma(\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{x}}}\\] <p>This function maps any input value \\(\\mathbf{x}\\) to a range between 0 and 1, making it particularly useful for binary classification tasks or probabilistic outputs.</p> <p>To compute the derivative of \\(\\sigma(\\mathbf{x})\\), we begin with the definition:</p> \\[\\frac{d}{dx} \\left( \\sigma(\\mathbf{x}) \\right) = \\frac{d}{dx} \\left( \\frac{1}{1 + e^{-\\mathbf{x}}} \\right) =\\] \\[= \\frac{e^{-\\mathbf{x}}}{(1 + e^{-\\mathbf{x}})^2}\\] <p>Next, we rewrite this derivative using the original sigmoid function \\(\\sigma(\\mathbf{x})\\):</p> \\[\\frac{e^{-\\mathbf{x}}}{(1 + e^{-\\mathbf{x}})^2} = \\frac{1}{1 + e^{-\\mathbf{x}}}\\frac{e^{-\\mathbf{x}}}{1 + e^{-\\mathbf{x}}} =\\] \\[= \\sigma(\\mathbf{x}) \\cdot \\frac{e^{-\\mathbf{x}}}{1 + e^{-\\mathbf{x}}}\\] <p>From the sigmoid definition, we know:</p> \\[\\frac{e^{-x}}{1 + e^{-x}} = \\frac{1+e^{-\\mathbf{x}}}{1+e^{-\\mathbf{x}}} - \\frac{1}{1+e^{-\\mathbf{x}}} = 1 - \\sigma(\\mathbf{x})\\] <p>Substituting this back, we get:</p> \\[\\frac{d(\\sigma(\\mathbf{x}))}{dx} = \\sigma(\\mathbf{x}) \\cdot (1 - \\sigma(\\mathbf{x}))\\] <p>Finally, the derivative of the sigmoid activation function is:</p> \\[\\tag{sigmoid derivative} \\label{eq:sigmoid_derivative} \\sigma'(\\mathbf{x}) = \\sigma(\\mathbf{x}) \\cdot (1 - \\sigma(\\mathbf{x})) \\] <p>This elegant result highlights a key property of the sigmoid function: its derivative can be expressed directly in terms of the function itself.</p>","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#implementation","title":"Implementation","text":"<pre><code>import numpy as np\n\n\nclass Sigmoid:\n    r\"\"\"Sigmoid function and its derivative for backpropagation.\"\"\"\n\n    def forward(self, x: np.ndarray):\n        r\"\"\"\n        Compute the Sigmoid of the input.\n        $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n\n        Args:\n            x (np.ndarray): Input array, typically the weighted sum of inputs to a neuron.\n\n        Returns:\n            np.ndarray: Sigmoid-transformed values.\n        \"\"\"\n\n        return 1 / (1 + np.exp(-x))\n\n    def backward(self, x: np.ndarray):\n        r\"\"\"\n        Compute the derivative of the sigmoid.\n\n        $$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$\n\n        Args:\n            x (np.ndarray): Input array, typically the weighted sum of inputs to a neuron.\n\n        Returns:\n            np.ndarray: Derivative of the sigmoid with respect to the input.\n        \"\"\"\n\n        sig = self.forward(x)\n\n        # Derivative of the Sigmoid function: sigmoid * (1 - sigmoid)\n        ds = sig * (1 - sig)\n        return ds\n</code></pre>","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#sigmoid-and-derivative-plot","title":"Sigmoid and derivative plot","text":"<p>The Sigmoid Function maps input values to an output range between 0 and 1. For large negative inputs (\\(x \\to -\\infty\\)), the sigmoid output approaches 0. For large positive inputs (\\(x \\to +\\infty\\)), the sigmoid output approaches 1. At \\(x = 0\\), the sigmoid output is 0.5, providing symmetry around the origin.</p> <p></p> <p>Sigmoid and derivative</p> <p>The left graph shows the sigmoid curve. Notice its smooth S-shape transitioning from 0 to 1 as \\(x\\) increases. This smoothness is crucial for backpropagation in neural networks, as it allows gradients to flow efficiently during optimization.</p> <p>The right graph shows the derivative of the sigmoid function. The peak at \\(x = 0\\) highlights the point of maximum sensitivity. This is the region where the sigmoid responds most strongly to changes in input. At the tails (\\(x \\to \\pm\\infty\\)), the derivative diminishes, leading to the vanishing gradient problem in deep networks when using sigmoid in multiple layers.</p> <p>This derivative shows how the sigmoid output changes with respect to the input \\(x\\).</p>","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#logits-to-probabilities","title":"Logits to Probabilities","text":"<p>Now, let's use the sigmoid function to transform the simulated logits into probabilities:</p> <pre><code>W = np.random.randn(5, 5)\nx = np.random.randn(5)\nb = np.ones(5)\n\nx_1 = W @ x + b\nsigmoid.forward(x_1)\n</code></pre> <p>Output:</p> <pre><code>array([0.99577714, 0.47530024, 0.17042292, 0.85459768, 0.80969836])\n</code></pre> <p>The output shows the normalized probabilities for each class. These probabilities are easy to interpret and directly usable for classification tasks.</p>","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#interpreting-the-sigmoid-output","title":"Interpreting the Sigmoid Output","text":"<ul> <li>As \\(x\\) decreases toward negative infinity, the sigmoid output approaches 0.</li> <li>As \\(x\\) increases toward positive infinity, the sigmoid output approaches 1.</li> </ul> <p>A threshold of 0.5 is commonly used:</p> <ul> <li>Outputs below 0.5 are typically classified as class 0.</li> <li>Outputs above 0.5 are classified as class 1.</li> </ul> <p>This transformation helps us map the model's raw outputs (logits) into meaningful probabilities, ready for decision-making.</p> <p>You can explore the Logits vs. Probabilities simulation on your own, code is here</p> <p></p> <p>Logits vs Probabilities 5 classes</p>","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#check-this-out","title":"Check this out","text":"","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#sigmoid-in-action","title":"Sigmoid in action","text":"","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2025/01/03/the-journey-from-logits-to-probabilities/#sigmoid-and-logistic-regression","title":"Sigmoid and Logistic Regression","text":"","tags":["Classification","Deep Learning","Logits","Sigmoid","Probability"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/","title":"Matrix Multiplication and Broadcasting","text":"<p>Understanding the transformation of input through weights and biases is fundamental in machine learning. Let's dive into how matrix multiplication plays a crucial role. It's all about matrix multiplication, the bread and butter of transforming inputs into meaningful outputs. Let's dive into how weights, biases, and inputs dance together in this mathematical ballet.</p> <p></p> <p>For matrix multiplication, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The result matrix has the number of rows of the first and the number of columns of the second matrix.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#exploring-matrix-multiplication-in-detail","title":"Exploring Matrix Multiplication in Detail","text":"<p>Matrix multiplication is not just about numbers; it's about understanding how data flows through layers of computation in machine learning models.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#setting-up-our-matrices","title":"Setting Up Our Matrices:","text":"<ul> <li>Input Matrix \\(\\mathbf{X}\\): This represents our data, with each row being a sample and each column a feature. For 100 samples and 3 features, we have:</li> </ul> \\[ \\mathbf{X} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ x_{101} &amp; x_{102} &amp; x_{103} \\end{bmatrix} \\] <ul> <li>Weights Vector \\(\\mathbf{W}\\): Our weights are like the tuning knobs for our model; they adjust how much each feature influences the outcome:</li> </ul> \\[ \\mathbf{W} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{bmatrix} \\quad \\text{(3 weights)} \\] <ul> <li>Bias \\(b\\): A little nudge to shift our predictions, just a single number:</li> </ul> \\[ b = 1 \\quad \\text{(Scalar bias term)} \\]","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#the-process","title":"The Process","text":"<ul> <li>Linear Combination \\(Z\\): The first step in our logistic regression model is to compute \\(z\\), the linear combination of our features with their respective weights plus the bias:</li> </ul> \\[ z = \\mathbf{X} \\cdot \\mathbf{W} + b = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_{100} \\end{bmatrix} \\quad \\text{(100 linear combinations)} \\] <ul> <li>Predicted Probabilities \\(\\hat{y}\\): We then pass our linear combinations through the sigmoid function to squash the results between 0 and 1, turning them into probabilities:</li> </ul> \\[ \\hat{y} = \\sigma(Z) = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_{100} \\end{bmatrix} \\quad \\text{(100 predicted probabilities)} \\] <p>But why do we end up with 100 results? Let's examine this more closely!</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#the-dot-product-vector-multiplication","title":"The Dot Product - Vector Multiplication","text":"<p>This simple operation is the heartbeat of matrix multiplication, allowing our model to compute how much each feature contributes to the prediction. The dot product is the sum of the element-wise products of two vectors.</p> \\[ \\mathbf{a} \\cdot \\mathbf{b} = \\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix} \\cdot \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} = a_1 \\cdot b_1 + a_2 \\cdot b_2 \\]","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Matrix multiplication is not just about numbers; it's about understanding how data flows through layers of computation in machine learning models. Consider Matrix A with dimensions 2x2 and Matrix B also with dimensions 2x2:</p> \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 1 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; 0 \\end{bmatrix} \\] \\[ \\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B} = \\begin{bmatrix} 1 \\cdot 2 + 2 \\cdot 3 &amp; 1 \\cdot 1 + 2 \\cdot 0 \\\\ 3 \\cdot 2 + 1 \\cdot 3 &amp; 3 \\cdot 1 + 1 \\cdot 0 \\end{bmatrix} = \\begin{bmatrix} 8 &amp; 1 \\\\ 9 &amp; 3 \\end{bmatrix} \\]","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#step-by-step-matrix-multiplication","title":"Step-by-Step Matrix Multiplication","text":"<p>Here's the color-coded step-by-step representation of matrix multiplication. To compute \\(\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}\\):</p> <p>Step #1:</p> \\[ \\mathbf{A} = \\begin{bmatrix} \\colorbox{blue}{1} &amp; \\colorbox{blue}{2} \\\\ 3 &amp; 1 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} \\colorbox{blue}{2} &amp; 1 \\\\ \\colorbox{blue}{3} &amp; 0 \\end{bmatrix} \\] <ul> <li>First Element of \\( \\mathbf{C} \\):</li> </ul> \\[ C_{11} = \\colorbox{blue}{1} \\cdot \\colorbox{blue}{2} + \\colorbox{blue}{2} \\cdot \\colorbox{blue}{3} = 2 + 6 = 8 \\] <p>Step #2:</p> \\[ \\mathbf{A} = \\begin{bmatrix} \\colorbox{blue}{1} &amp; \\colorbox{blue}{2} \\\\ 3 &amp; 1 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 2 &amp; \\colorbox{blue}{1} \\\\ 3 &amp; \\colorbox{blue}{0} \\end{bmatrix} \\] <ul> <li>Second Element of the First Row:</li> </ul> \\[ C_{12} = \\colorbox{blue}{1} \\cdot \\colorbox{blue}{1} + \\colorbox{blue}{2} \\cdot \\colorbox{blue}{0} = 1 + 0 = 1 \\] <p>Step #3:</p> \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ \\colorbox{blue}{3} &amp; \\colorbox{blue}{1} \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} \\colorbox{blue}{2} &amp; 1 \\\\ \\colorbox{blue}{3} &amp; 0 \\end{bmatrix} \\] <ul> <li>First Element of the Second Row:</li> </ul> \\[ C_{21} = \\colorbox{blue}{3} \\cdot \\colorbox{blue}{2} + \\colorbox{blue}{1} \\cdot \\colorbox{blue}{3} = 6 + 3 = 9 \\] <p>Step #4:</p> \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ \\colorbox{blue}{3} &amp; \\colorbox{blue}{1} \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 2 &amp; \\colorbox{blue}{1} \\\\ 3 &amp; \\colorbox{blue}{0} \\end{bmatrix} \\] <ul> <li>Second Element of the Second Row:</li> </ul> \\[ C_{22} = \\colorbox{blue}{3} \\cdot \\colorbox{blue}{1} + \\colorbox{blue}{1} \\cdot \\colorbox{blue}{0} = 3 + 0 = 3 \\] <p>After completing these steps, the resulting matrix \\( \\mathbf{C} \\) would be:</p> \\[ \\mathbf{C} = \\begin{bmatrix} \\colorbox{blue}{8} &amp; \\colorbox{blue}{1} \\\\ \\colorbox{blue}{9} &amp; \\colorbox{blue}{3} \\end{bmatrix} \\] <p>Each step highlights the corresponding elements from \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) used in the calculation, with the color blue indicating the active elements in the multiplication.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#dimensionality-in-matrix-multiplication","title":"Dimensionality in Matrix Multiplication","text":"<p>Matrix multiplication is not only about the arithmetic of numbers but also about how dimensions play a crucial role in the operation. Here's a detailed look:</p> <ul> <li>Let's revisit Matrix A and Matrix B:</li> </ul> \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 1 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; 0 \\end{bmatrix} \\] <ul> <li>The Requirement for Multiplication: The number of columns in \\(\\mathbf{A}\\) must equal the number of rows in \\(\\mathbf{B}\\). Both \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are 2x2, fitting this requirement perfectly. But, if we take a column from \\(\\mathbf{B}\\), say \\( \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} \\), we can multiply \\(\\mathbf{A}\\) by this vector!</li> </ul> <p>Step #1 - Matrix A by Column of B:</p> \\[ \\mathbf{A} = \\begin{bmatrix} \\colorbox{blue}{1} &amp; \\colorbox{blue}{2} \\\\ \\colorbox{blue}{3} &amp; \\colorbox{blue}{1} \\end{bmatrix}, \\quad \\mathbf{B1} = \\begin{bmatrix} \\colorbox{blue}{2} \\\\ \\colorbox{blue}{3} \\end{bmatrix} \\] <ul> <li>First Row Multiplication:</li> </ul> \\[ 1 \\cdot 2 + 2 \\cdot 3 = 2 + 6 = 8 \\] <ul> <li>Second Row Multiplication:</li> </ul> \\[ 3 \\cdot 2 + 1 \\cdot 3 = 6 + 3 = 9 \\] <p>Resulting in:</p> \\[ \\begin{bmatrix} \\colorbox{blue}{8} \\\\ \\colorbox{blue}{9} \\end{bmatrix} \\] <p>This operation indeed matches the first column of the full matrix multiplication result! Now, take only the first row of \\(\\mathbf{A}\\) and multiply it by \\(\\mathbf{B}\\):</p> <p>Step #1 - Row of A by Matrix B:</p> \\[ \\mathbf{A1} = \\begin{bmatrix} \\colorbox{blue}{1} &amp; \\colorbox{blue}{2} \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} \\colorbox{blue}{2} &amp; \\colorbox{blue}{1} \\\\ 3 &amp; 0 \\end{bmatrix} \\] <ul> <li>First Column Multiplication:</li> </ul> \\[ 1 \\cdot 2 + 2 \\cdot 3 = 2 + 6 = 8 \\] <ul> <li>Second Column Multiplication:</li> </ul> \\[ 1 \\cdot 1 + 2 \\cdot 0 = 1 + 0 = 1 \\] <p>Resulting in:</p> \\[ \\begin{bmatrix} \\colorbox{blue}{8} &amp; \\colorbox{blue}{1} \\end{bmatrix} \\] <p>This matches the first row of our previous matrix multiplication result.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#generalizing-matrix-multiplication","title":"Generalizing Matrix Multiplication","text":"<ul> <li>When we multiply a matrix \\(\\mathbf{A}\\) of size \\(M \\times N\\) by a matrix \\(\\mathbf{B}\\) of size \\(N \\times P\\), the result is a matrix \\(\\mathbf{C}\\) of size \\(M \\times P\\):</li> </ul> \\[ \\mathbf{C}_{m \\times p} = \\mathbf{A}_{m \\times n} \\cdot \\mathbf{B}_{n \\times p} \\] <ul> <li>Dimensionality Rules:</li> <li>The dimensions of \\(\\mathbf{A}\\)'s columns must match the dimensions of \\(\\mathbf{B}\\)'s rows (<code>N</code>).</li> <li>The resulting matrix \\(\\mathbf{C}\\) has the row dimension of \\(\\mathbf{A}\\) (<code>M</code>) and the column dimension of \\(\\mathbf{B}\\) (<code>P</code>).</li> </ul> <p>This process helps in understanding how matrix multiplication transforms data across neural network layers or any other application requiring feature transformation. It's about ensuring that the 'middle' dimensions align, allowing for a fusion of input data with weights to produce an output that can be interpreted in the context of our model's needs.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#exploring-different-approaches-to-matrix-multiplication","title":"Exploring Different Approaches to Matrix Multiplication","text":"<p>Matrix multiplication isn't limited to one technique; there are various ways to conceptualize and compute it. Here's how we can think about this operation both horizontally (column by column) and vertically (row by row):</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#horizontal-stacking-multiply-a-by-columns-of-b","title":"Horizontal Stacking - Multiply <code>A</code> by Columns of <code>B</code>:","text":"<p>Let's split matrix \\(\\mathbf{B}\\) into columns \\(b_1\\) and \\(b_2\\):</p> \\[ \\mathbf{B} = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; 0 \\end{bmatrix} \\rightarrow \\mathbf{b_1} = \\begin{bmatrix}     2 \\\\     3 \\end{bmatrix}, \\mathbf{b_2} = \\begin{bmatrix}     1 \\\\     0 \\end{bmatrix} \\] <p>Then, we can multiply matrix \\(\\mathbf{A}\\) by each of these vectors and stack the results horizontally:</p> \\[ \\mathbf{C} = \\begin{bmatrix}     \\mathbf{A}\\mathbf{b_1} &amp; \\mathbf{A}\\mathbf{b_2} \\end{bmatrix} = \\begin{bmatrix}     \\begin{bmatrix}         1 &amp; 2 \\\\         3 &amp; 1     \\end{bmatrix}     \\begin{bmatrix}         2 \\\\         3     \\end{bmatrix}     &amp;     \\begin{bmatrix}         1 &amp; 2 \\\\         3 &amp; 1     \\end{bmatrix}     \\begin{bmatrix}         1 \\\\         0     \\end{bmatrix} \\end{bmatrix} \\] <ul> <li>First Column of \\(\\mathbf{C}\\):</li> </ul> \\[ \\mathbf{A}\\mathbf{b_1} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 2 + 2 \\cdot 3 \\\\ 3 \\cdot 2 + 1 \\cdot 3 \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 9 \\end{bmatrix} \\] <ul> <li>Second Column of \\(\\mathbf{C}\\):</li> </ul> \\[ \\mathbf{A}\\mathbf{b_2} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1 + 2 \\cdot 0 \\\\ 3 \\cdot 1 + 1 \\cdot 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\] <p>Thus, the resulting matrix \\(\\mathbf{C}\\) is:</p> \\[ \\mathbf{C} = \\begin{bmatrix} 8 &amp; 1 \\\\ 9 &amp; 3 \\end{bmatrix} \\]","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#vertical-stacking-multiply-rows-of-a-by-b","title":"Vertical Stacking - Multiply Rows of <code>A</code> by <code>B</code>:","text":"<p>Now, let's split matrix \\(\\mathbf{A}\\) into rows \\(a_1\\) and \\(a_2\\):</p> \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 1 \\end{bmatrix} \\rightarrow \\mathbf{a_1} = \\begin{bmatrix}     1 &amp; 2 \\end{bmatrix}, \\mathbf{a_2} = \\begin{bmatrix}     3 &amp; 1 \\end{bmatrix} \\] <p>Then, we multiply each row of \\(\\mathbf{A}\\) by \\(\\mathbf{B}\\) and stack the results vertically:</p> \\[ \\mathbf{C} = \\begin{bmatrix}     \\mathbf{a_1}\\mathbf{B} \\\\     \\mathbf{a_2}\\mathbf{B} \\end{bmatrix} = \\begin{bmatrix}     \\begin{bmatrix}         1 &amp; 2     \\end{bmatrix}     \\begin{bmatrix}         2 &amp; 1 \\\\         3 &amp; 0     \\end{bmatrix} \\\\     \\begin{bmatrix}         3 &amp; 1     \\end{bmatrix}     \\begin{bmatrix}         2 &amp; 1 \\\\         3 &amp; 0     \\end{bmatrix} \\end{bmatrix} \\] <ul> <li>First Row of \\(\\mathbf{C}\\):</li> </ul> \\[ \\mathbf{a_1}\\mathbf{B} = \\begin{bmatrix} 1 &amp; 2  \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 2 + 2 \\cdot 3 &amp; 1 \\cdot 1 + 2 \\cdot 0  \\end{bmatrix} = \\begin{bmatrix} 8 &amp; 1  \\end{bmatrix} \\] <ul> <li>Second Row of \\(\\mathbf{C}\\):</li> </ul> \\[ \\mathbf{a_2}\\mathbf{B} = \\begin{bmatrix} 3 &amp; 1  \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 3 \\cdot 2 + 1 \\cdot 3 &amp; 3 \\cdot 1 + 1 \\cdot 0  \\end{bmatrix} = \\begin{bmatrix} 9 &amp; 3  \\end{bmatrix} \\] <p>Thus, the resulting matrix \\(\\mathbf{C}\\) is:</p> \\[ \\mathbf{C} = \\begin{bmatrix} 8 &amp; 1 \\\\ 9 &amp; 3 \\end{bmatrix} \\] <p>Both horizontal and vertical stacking methods yield the same result, highlighting the flexibility in matrix multiplication techniques while emphasizing the importance of understanding how dimensions interact.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#matrix-multiplication-an-interesting-dimensionality-example","title":"Matrix Multiplication: An Interesting Dimensionality Example","text":"<p>In the world of matrix operations, sometimes the dimensionality rules allow for some intriguing transformations. Let's dive into an example where the dimensions seem unconventional:</p> <ul> <li>Matrix \\(\\mathbf{A}\\): 3 rows and 1 column,</li> <li>Matrix \\(\\mathbf{B}\\): 1 row and 2 columns.</li> </ul> <p>Here, \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) have matching dimensions for multiplication, specifically the column dimentions of \\(\\mathbf{A}\\) matches the row dimentions of \\(\\mathbf{B}\\):</p> \\[ \\mathbf{A} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 1 &amp; 2 \\end{bmatrix} \\] <ul> <li>Computing the Output:</li> </ul> \\[ \\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1 &amp; 1 \\cdot 2 \\\\ 2 \\cdot 1 &amp; 2 \\cdot 2 \\\\ 3 \\cdot 1 &amp; 3 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 4 \\\\ 3 &amp; 6 \\end{bmatrix} \\] <p>The resulting matrix \\(\\mathbf{C}\\) has dimensions \\(3 \\times 2\\), showing how the multiplication can indeed change the shape of our data.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#matrix-multiplication-column-row-way","title":"Matrix Multiplication: Column-Row Way","text":"<p>Let's use this method for Matrix \\(\\mathbf{A}\\) and Matrix \\(\\mathbf{B}\\):</p> \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 1 \\end{bmatrix}, \\quad \\mathbf{B} = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; 0 \\end{bmatrix} \\] <ul> <li>First Column of \\(\\mathbf{A}\\) by First Row of \\(\\mathbf{B}\\):</li> </ul> \\[ \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 2 &amp; 1 \\cdot 1 \\\\ 3 \\cdot 2 &amp; 3 \\cdot 1 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1 \\\\ 6 &amp; 3 \\end{bmatrix} \\] <ul> <li>Second Column of \\(\\mathbf{A}\\) by Second Row of \\(\\mathbf{B}\\):</li> </ul> \\[ \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\begin{bmatrix} 3 &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\cdot 3 &amp; 2 \\cdot 0 \\\\ 1 \\cdot 3 &amp; 1 \\cdot 0 \\end{bmatrix} = \\begin{bmatrix} 6 &amp; 0 \\\\ 3 &amp; 0 \\end{bmatrix} \\] <p>Now, we sum these resulting matrices:</p> \\[ \\mathbf{C} = \\begin{bmatrix} 2 &amp; 1 \\\\ 6 &amp; 3 \\end{bmatrix} + \\begin{bmatrix} 6 &amp; 0 \\\\ 3 &amp; 0 \\end{bmatrix} =  \\begin{bmatrix} 8 &amp; 1 \\\\ 9 &amp; 3 \\end{bmatrix} \\] <p>This method, where we multiply a column from one matrix by a row from another, effectively constructs the result matrix \\(\\mathbf{C}\\) by summing these intermediate results. Each element of \\(\\mathbf{C}\\) is formed by considering the dot product between corresponding columns of \\(\\mathbf{A}\\) and rows of \\(\\mathbf{B}\\), which is a fundamental way to understand how matrix elements interact in matrix multiplication.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#broadcasting-expanding-the-dimensions","title":"Broadcasting: Expanding the Dimensions","text":"<p>Now let's come back to our example. We know that our Matrix <code>X</code> is of 3 by 3 dimensions, and Matrix <code>W</code> is three rows with only one column. We then add a bias term <code>B</code>, which is just a scalar, equal to 1. The dimensionality of <code>X * W</code> results in a <code>3x1</code> matrix, and adding the scalar <code>B</code> involves an operation called broadcasting.</p> <p>Broadcasting is a method used by most scientific computing libraries like PyTorch or NumPy to handle operations between arrays of different shapes. Here\u2019s how it works:</p> <ul> <li>Broadcasting Rules: </li> <li>When performing an operation, compare the dimensions from right to left side.</li> <li>If the dimensions do not match, the shape with a size of 1 is stretched to match the other shape.</li> </ul> <p>In our example, if we were to broadcast:</p> <ul> <li>Matrix <code>A</code> with shape <code>(3, 1)</code> (our <code>X * W</code> result)</li> <li>Matrix <code>B</code> with shape <code>(1, 4)</code> (our bias <code>B</code> expanded to match <code>X * W</code> for broadcasting)</li> </ul> <p>For the addition, we:</p> <ol> <li>Stretch <code>A</code> to match <code>B</code> by duplicating the column four times.</li> <li>Stretch <code>B</code> to match <code>A</code> by duplicating the row three times.</li> </ol> <p>Thus, both matrices would be aligned to have dimensions of 3x4, allowing for element-wise addition.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#broadcasting-example-in-python","title":"Broadcasting Example in Python","text":"<p>Let's see this in code using NumPy:</p> <pre><code>import numpy as np\n\n# Define array A with shape (3, 1)\nA = np.array([\n    [1],\n    [2],\n    [3],\n])\nprint(f\"Array A shape: {A.shape}\")\n\n# Define array B with shape (1, 4)\nB = np.array([\n    [1, 2, 3, 4],\n])\nprint(f\"Array B shape: {B.shape}\")\n\n# Perform broadcasting addition\nresult = A + B\n\nprint(\"A + B result: \")\nprint(result)\n\nprint(f\"Result of A + B shape: {result.shape}\")\n\n# Broadcasting the same for the matrix multiplication\nmatmul = A @ B\nprint(f\"Matmul A @ B shape: {matmul.shape}\")\n\nprint(\"Matmul result: \")\nprint(matmul)\n</code></pre> <p>Output:</p> <pre><code>Array A shape: (3, 1)\nArray B shape: (1, 4)\nA + B result: \n[[2 3 4 5]\n [3 4 5 6]\n [4 5 6 7]]\nResult of A + B shape: (3, 4)\nMatmul A @ B shape: (3, 4)\nMatmul result: \n[[ 1  2  3  4]\n [ 2  4  6  8]\n [ 3  6  9 12]]\n</code></pre> <p>This code demonstrates:</p> <ul> <li>Broadcasting addition where <code>A</code> is extended across columns and <code>B</code> across rows to perform element-wise addition, yielding a <code>3x4</code> result.</li> <li>Matrix multiplication where <code>A</code> (as a column vector) is multiplied with <code>B</code> (as a row vector), also yielding a <code>3x4</code> matrix.</li> </ul> <p>Both operations use the broadcasting mechanism to align the arrays appropriately before performing the operation. The output shapes and values will confirm that the broadcasting worked as expected, providing insight into how matrix dimensions can be manipulated and aligned for operations in machine learning and data processing tasks.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#final-step-the-power-of-broadcasting","title":"Final Step: The Power of Broadcasting","text":"<p>Returning to our initial example, we have:</p> <ul> <li>Matrix <code>X</code> with 100 samples and 3 features, leading to a 100x3 matrix.</li> <li>Matrix <code>W</code> with 3 weights, forming a 3x1 matrix after transposition.</li> <li>Bias <code>B</code> as a scalar, which we can consider as having dimensions 1x1.</li> </ul> <p>When performing the linear combination of <code>X</code> with <code>W</code>, we get a result with dimensions 100x1, as each sample in <code>X</code> is mapped to a single value through the weights. Now, we add <code>B</code>:</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#broadcasting-in-action","title":"Broadcasting in Action","text":"<ul> <li> <p>Applying Broadcasting Rules:</p> </li> <li> <p>Step 1: The rightmost dimension of <code>B</code> (1) matches with the leftmost dimension of the result of <code>X * W</code> (1). This means <code>B</code> can be broadcasted along the rows of our result to match the 100 rows.</p> </li> <li> <p>Operation: We're adding a 1x1 matrix to a 100x1 matrix. Broadcasting stretches <code>B</code> to have the same number of rows as our result from the multiplication, effectively turning it into a 100x1 matrix where every element is <code>1</code>.</p> </li> </ul> <p>Thus, the addition operation looks like this:</p> \\[ \\mathbf{Z} = \\mathbf{X} \\cdot \\mathbf{W} + \\mathbf{B} = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_{100} \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} z_1 + 1 \\\\ z_2 + 1 \\\\ \\vdots \\\\ z_{100} + 1 \\end{bmatrix} \\] <p>The final output <code>Z</code> still has dimensions <code>100x1</code>, giving us 100 linear combinations with the bias term added to each.</p>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/20/matrix-multiplication-and-broadcasting/#takeaways","title":"Takeaways","text":"<p>Remember, in machine learning, the shape of your data often dictates the shape of your solution!</p> <ul> <li> <p>Matrix Shapes: The shape of your matrices tells you a lot about the nature of your data and what operations are possible or necessary. Here, the shape of <code>Z</code> after broadcasting indicates that we now have a vector of 100 predictions, each adjusted by our bias term.</p> </li> <li> <p>Dimension Management: Understanding and managing dimensions is crucial in machine learning. Broadcasting allows us to perform operations that would otherwise require reshaping or looping over arrays manually, simplifying our code and making operations more efficient.</p> </li> <li> <p>Predictive Power: With this setup, each element in <code>Z</code> represents a 'score' for each sample, which, when passed through an activation function like sigmoid, gives us probabilistic predictions for classification tasks.</p> </li> </ul>","tags":["Matrices","Broadcasting","Neural Networks","Pytorch","Numpy"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/","title":"Mastering Derivatives From Math to Code","text":"<p>Learn the mathematical concept, see how it translates into Python code, and discover three numerical differentiation methods - forward, backward, and central. Watch as we visualize their performance, helping you understand which method provides the most precise results for your calculations!</p> <p></p> <p>Absolute Error Numerical Differentiation Example</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#the-derivative-a-simple-example","title":"The Derivative: A Simple Example","text":"<p>Let's consider a basic function:</p> \\[ f(x) = x^2 \\] <p>The derivative of this function, \\( f'(x) \\), is:</p> \\[ f'(x) = 2x \\]","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#mathematical-definition-of-a-derivative","title":"Mathematical Definition of a Derivative","text":"<p>The derivative \\( f'(x) \\) is defined as:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} \\] <p>Here's what this means:</p> <ul> <li> <p>\\( f(x + h) \\): The function's value when we slightly change the input by adding \\( h \\).</p> </li> <li> <p>\\( f(x) \\): The function's original value.</p> </li> <li> <p>\\( \\frac{f(x + h) - f(x)}{h} \\): This fraction shows how much the function changes when its input changes by \\( h \\).</p> </li> </ul>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#applying-this-to-our-function","title":"Applying This to Our Function","text":"<p>For \\( f(x) = x^2 \\):</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{(x + h)^2 - x^2}{h} \\] <p>Simplifying:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{x^2 + 2xh + h^2 - x^2}{h} = \\lim_{h \\to 0} \\frac{2xh + h^2}{h} = \\lim_{h \\to 0} 2x + h \\] <p>As \\( h \\) approaches zero, we get:</p> \\[ f'(x) = 2x \\] <p>Yes, math checks out!</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#understanding-derivatives-through-coding","title":"Understanding Derivatives Through Coding","text":"<p>To really get how derivatives work, let's code them and see what's happening behind the scenes.</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#the-functions","title":"The Functions","text":"<p>Let's define \\( f(x) = x^2 \\) in Python:</p> <pre><code>def f(x):\n    return x**2\n</code></pre> <p>Next, the derivative \\( f'(x) = 2x \\):</p> <pre><code>def df(x):\n    return 2*x\n</code></pre>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#numerical-derivative","title":"Numerical Derivative","text":"<p>Now, how do we code the derivative's limit definition? We'll use a small value for \\( h \\):</p> \\[ f'(x) \\approx \\frac{f(x+h) - f(x)}{h} \\] <p>Here's our function to calculate this:</p> <pre><code>def ndiff(f, x, h=1e-2):\n    return (f(x + h) - f(x)) / h\n</code></pre>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#testing-our-functions","title":"Testing Our Functions","text":"<p>Let's test with \\( x = 3 \\):</p> <pre><code>x = 3\n\nfx = f(x)  # This should give us 9\ndfx = df(x)  # This should give us 6\nfndfx = ndiff(f, x, h=1e-4)  # Here we use a smaller h for better precision\n\nprint(fx, dfx, fndfx)\n</code></pre> <ul> <li>\\( f(3) = 9 \\)</li> <li>\\( df(3) = 6 \\)</li> <li>Our <code>ndiff</code> function gives us 6.0001, which is very close to the actual derivative.</li> </ul>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#exploring-step-size","title":"Exploring Step Size","text":"<p>As we reduce \\( h \\), the numerical derivative becomes more accurate:</p> <pre><code>import matplotlib.pyplot as plt\n\nx = 3\ndfx = df(x)\n\nhs = [10**-p for p in range(1, 15)]\nabs_errors = []\n\nfor h in hs:\n    ndfx = ndiff(f, x, h)\n    abs_errors.append(abs(ndfx - dfx))\n\nplt.plot(hs, abs_errors)\nplt.xscale('log')\nplt.yscale('log')\nplt.xlabel('Step Size h')\nplt.ylabel('Absolute Error')\nplt.title('Error vs Step Size for Numerical Derivative')\nplt.show()\n</code></pre> <p>This plot will show how the error decreases as we make \\( h \\) smaller, confirming our understanding of derivatives through practical implementation.</p> <p></p> <p>It's a dragon!</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#observations","title":"Observations","text":"<ul> <li>The plot shows error vs. step size.</li> <li>As we reduce \\( h \\), the error decreases initially, which is expected.</li> <li>Around \\( 10^{-8} \\), there's an inflection point where the error starts to increase.</li> <li>This indicates that beyond a certain small \\( h \\), our numerical method for differentiation might not be optimal due to precision issues in floating-point arithmetic.</li> </ul> <p>Our basic approach to estimate numerical derivatives works well until we hit very small step sizes where computational errors become significant.</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#three-methods-to-numerically-estimate-derivatives","title":"Three Methods to Numerically Estimate Derivatives","text":"<p>There are three common ways to approximate derivatives numerically. Let me explain each:</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#1-forward-difference","title":"1. Forward Difference","text":"\\[ \\frac{df}{dx} \\approx \\frac{f(x + h) - f(x)}{h} \\] <ul> <li>Why 'Forward'? Because we're looking at how the function changes when we move \\( x \\) in the positive direction by a small amount \\( h \\).</li> </ul>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#2-backward-difference","title":"2. Backward Difference","text":"\\[ \\frac{df}{dx} \\approx \\frac{f(x) - f(x - h)}{h} \\] <ul> <li>Why 'Backward'? This method measures the change in the function by moving \\( x \\) slightly in the negative direction.</li> </ul>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#3-central-difference","title":"3. Central Difference","text":"\\[ \\frac{df}{dx} \\approx \\frac{f(x + h) - f(x - h)}{2h} \\] <ul> <li>The Process:</li> <li>We take the average of the forward and backward differences to get this formula.</li> <li> <p>Simplification leads to dropping common terms and results in the formula above.</p> </li> <li> <p>Why 'Central'? This method balances both directions, often providing the most accurate approximation of the derivative.</p> </li> </ul> <p>The Central Difference is usually the most precise for estimating derivatives numerically due to its symmetry and error cancellation properties.</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#comparing-three-numerical-differentiation-methods","title":"Comparing Three Numerical Differentiation Methods","text":"<p>Let's program and compare the performance of three numerical differentiation methods: Forward, Backward, and Central differences.</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#the-functions_1","title":"The Functions","text":"<p>We already have <code>ndiff</code> for forward difference. Let's implement backward and central differences:</p> <pre><code>def fdiff(f, x, h=1e-2):\n    return (f(x + h) - f(x)) / h\n\ndef bdiff(f, x, h=1e-2):\n    return (f(x) - f(x - h)) / h\n\ndef cdiff(f, x, h=1e-2):\n    return (f(x + h) - f(x - h)) / (2 * h)\n</code></pre>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#testing-with-functions","title":"Testing with Functions","text":"","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#exponential-function","title":"Exponential Function","text":"\\[ \\frac{d}{dx}e^x = e^x \\] <pre><code>import numpy as np\n\ndef exp(x):\n    return np.exp(x)\n\ndef d_exp(x):\n    return np.exp(x)\n\nx = 3\nplot_errors(exp, d_exp, x)\n</code></pre> <p>Exponential Function</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#sine-function","title":"Sine Function","text":"\\[ \\frac{d}{dx}sin(x) = cos(x) \\] <pre><code>def sin(x):\n    return np.sin(x)\n\ndef d_sin(x):\n    return np.cos(x)\n\nx = 3\nplot_errors(sin, d_sin, x)\n</code></pre> <p>Sin Function</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#hyperbolic-tangent","title":"Hyperbolic Tangent","text":"\\[ \\frac{d}{dx}tanh(x) = 1 - tanh^2(x) \\] <pre><code>def tanh(x):\n    return np.tanh(x)\n\ndef d_tanh(x):\n    return 1 - np.tanh(x)**2\n\nx = 3\nplot_errors(tanh, d_tanh, x)\n</code></pre> <p>Hyperbolic Tangent Function</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#observations_1","title":"Observations","text":"<ul> <li>Central Difference is generally more precise, particularly at smaller step sizes.</li> <li>Forward and Backward methods show similar error behavior, but Central difference often results in lower errors, especially for \\( h \\approx 10^{-5} \\).</li> <li>The performance can vary slightly based on the function being differentiated, but Central difference consistently outperforms the others in accuracy.</li> </ul>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/11/30/mastering-derivatives-from-math-to-code/#fun-fact","title":"Fun Fact:","text":"<p>The symmetric difference quotient, also known as the central difference, is used as the method for approximating derivatives in various calculators. Specifically, TI-82, TI-83, TI-84, and TI-85 calculators all implement this technique with a standard step size of \\( h = 0.001 \\).</p>","tags":["Calculus","Numerical Differentiation","Python","Derivatives","Forward Difference","Backward Difference","Central Difference","Data Visualization","Machine Learning","Computational Science"]},{"location":"2024/12/10/instability-in-numerical-differentiation/","title":"Instability in Numerical Differentiation","text":"<p>Using the Centered Difference approximation of the derivative can lead to numerical instability. Function optimization is a precise task; we cannot afford methods that introduce instability and unpredictability into our results. I've discovered a specific case that illustrates this issue.</p> <p></p> <p>Oscillating Function VS Exact Derivative</p> <p>Let's compare the numerical derivative of the oscillating function \\(f(x) = \\sin(\\frac{1}{x})\\) with its exact derivative. First, we need to determine the exact derivative analytically.</p> <p>The exact derivative of \\(f(x) = \\sin(\\frac{1}{x})\\) can be computed using the chain rule. The derivative is:</p> \\[ f'(x) = \\cos(\\frac{1}{x}) \\cdot \\left(-\\frac{1}{x^2}\\right) = -\\frac{\\cos(\\frac{1}{x})}{x^2} \\] <p>This derivative also oscillates as \\(x \\rightarrow 0\\) because it leads to division by zero! Let's define a legitimate function that works everywhere.</p>","tags":["Gradient Descent","Numerical Differentiation","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib","Numerical Stability","Deep Learning","Backpropagation","Autodiff"]},{"location":"2024/12/10/instability-in-numerical-differentiation/#oscillating-function","title":"Oscillating Function","text":"<p>The function \\(f(x)\\) is defined as:</p> \\[ f(x) =  \\begin{cases}  \\sin\\left(\\frac{1}{x}\\right) &amp; \\text{if } x \\neq 0 \\\\ 0 &amp; \\text{if } x = 0 \\end{cases} \\] <p>This function \\(\\sin(\\frac{1}{x})\\) oscillates rapidly as \\(x \\to 0\\) and tends to explode to infinity. To prevent this behavior, I defined it as a piecewise function.</p>","tags":["Gradient Descent","Numerical Differentiation","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib","Numerical Stability","Deep Learning","Backpropagation","Autodiff"]},{"location":"2024/12/10/instability-in-numerical-differentiation/#derivative","title":"Derivative","text":"<p>The derivative \\(f'(x)\\) is:</p> \\[ f'(x) =  \\begin{cases}  -\\frac{\\cos\\left(\\frac{1}{x}\\right)}{x^2} &amp; \\text{if } x \\neq 0 \\\\ 0 &amp; \\text{if } x = 0 \\end{cases} \\]","tags":["Gradient Descent","Numerical Differentiation","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib","Numerical Stability","Deep Learning","Backpropagation","Autodiff"]},{"location":"2024/12/10/instability-in-numerical-differentiation/#visualize-the-function-and-derivative","title":"Visualize the Function and Derivative","text":"<p>Let's implement a Python script that computes both the numerical and exact derivatives for \\(f(x) = \\sin(\\frac{1}{x})\\) as \\(x\\) approaches 0, and then compare the results. We can plot this function along with its derivative.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider, IntSlider\n\n\n# Function to calculate the oscillating function sin(1/x) for an array of x values\ndef oscillating(x):\n    \"\"\"Vector-valued function that oscillates rapidly as x approaches 0.\"\"\"\n\n    # Create an array filled with zeros with the same shape as x\n    result = np.zeros_like(x)\n    # Create a boolean mask for x values not equal to zero\n    non_zero_mask = x != 0\n    # Apply sin(1/x) only where x is not zero to avoid division by zero\n    result[non_zero_mask] = np.sin(1 / x[non_zero_mask])\n    return result\n\n\n# Function to compute the derivative of the oscillating function\ndef d_oscillating(x):\n    \"\"\"Vector-valued exact derivative of the oscillating function.\"\"\"\n\n    # Similar to oscillating, initialize result with zeros\n    result = np.zeros_like(x)\n    # Mask for non-zero x values\n    non_zero_mask = x != 0\n    # Calculate the derivative where x is not zero\n    result[non_zero_mask] = -np.cos(1 / x[non_zero_mask]) / (x[non_zero_mask] ** 2)\n    return result\n\n\n# Function to plot both the oscillating function and its derivative\ndef plot_oscillating(min=-0.1, max=0.1, steps=500):\n    # Generate x values between min and max, avoiding zero to prevent division by zero\n    x_values = np.linspace(min, max, steps)\n\n    # Compute the function values and its derivative\n    y_values = oscillating(x_values)\n    dy_values = d_oscillating(x_values)\n\n    # Create plots\n    plt.figure(figsize=(14, 6))\n\n    # Plot oscillating function\n    plt.subplot(1, 2, 1)\n    plt.plot(x_values, y_values, label='Oscillating Function', color='blue')\n    plt.title('Oscillating Function $f(x) = sin(1/x)$')\n    plt.xlabel('x')\n    plt.ylabel('f(x)')\n    plt.grid(True)\n    plt.legend()\n\n    # Plot exact derivative\n    plt.subplot(1, 2, 2)\n    plt.plot(x_values, dy_values, label='Exact Derivative', color='orange')\n    plt.title(\"Exact Derivative $f'(x) = -\\\\frac{cos(1/x)}{x^2}$\")\n    plt.xlabel('x')\n    plt.ylabel(\"f'(x)\")\n    plt.grid(True)\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n# Use interact to create interactive sliders for adjusting plot parameters\ninteract(\n    plot_oscillating,\n    min=FloatSlider(min=-0.1, max=-0.01, step=0.01, value=-0.1, description='Min'),\n    max=FloatSlider(min=0.01, max=0.1, step=0.01, value=0.1, description='Max'),\n    steps=IntSlider(min=100, max=1000, step=100, value=500, description='Steps')\n)\n</code></pre>","tags":["Gradient Descent","Numerical Differentiation","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib","Numerical Stability","Deep Learning","Backpropagation","Autodiff"]},{"location":"2024/12/10/instability-in-numerical-differentiation/#plot","title":"Plot","text":"<p>Oscillating Function VS Exact Derivative</p> <p>The derivative of this function is not well-defined at \\(x = 0\\), and for practical purposes, we can consider it as \\(0\\).</p> <p>These expressions describe an oscillating function that becomes increasingly unpredictable as \\(x\\) approaches zero, with its derivative mirroring the complex behavior in that region.</p>","tags":["Gradient Descent","Numerical Differentiation","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib","Numerical Stability","Deep Learning","Backpropagation","Autodiff"]},{"location":"2024/12/10/instability-in-numerical-differentiation/#numerical-derivative-with-the-central-difference","title":"Numerical Derivative with the Central Difference","text":"<p>We can attempt to estimate the derivative of this oscillated function using the Central Difference method. This approach works well for larger values of \\(x\\) but as \\(x\\) gets smaller, the method begins to fail. Here is how we can demonstrate this:</p> <pre><code>import numpy as np\n\n\ndef cdiff(func, x, h=1e-3):\n    \"\"\"Centered difference approximation of the derivative.\"\"\"\n    return (func(x + h) - func(x - h)) / (2 * h)\n\n\ndef oscillating(x):\n    \"\"\"Function that oscillates rapidly as x approaches 0.\"\"\"\n    return np.sin(1/x) if x != 0 else 0\n\n\ndef d_oscillating(x):\n    \"\"\"Exact derivative of the oscillating function.\"\"\"\n    if x != 0:\n        return -np.cos(1/x) / (x**2)\n    else:\n        return 0  # Not defined, but can be treated as 0 for comparison\n\n\n# Test values close to 0\nx_values = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n\n# Compute and compare the derivatives\nprint(f\"{'x':&gt;12} | {'Numerical Derivative':&gt;20} | {'Exact Derivative':&gt;20} | {'Abs Error':&gt;20}\")\nprint(\"-\" * 80)\nfor x in x_values:\n    numerical_derivative = cdiff(oscillating, x)\n    exact_deriv = d_oscillating(x)\n\n    # Calculate the relative error\n    if exact_deriv != 0:  # Avoid division by zero for relative error\n        abs_error = np.abs((numerical_derivative - exact_deriv))\n    else:\n        abs_error = np.nan  # Not defined for comparison\n\n    print(f\"{x:&gt;12} | {numerical_derivative:&gt;20.6f} | {exact_deriv:&gt;20.6f} | {abs_error:&gt;20.6f}\")\n</code></pre>","tags":["Gradient Descent","Numerical Differentiation","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib","Numerical Stability","Deep Learning","Backpropagation","Autodiff"]},{"location":"2024/12/10/instability-in-numerical-differentiation/#output","title":"Output","text":"x Numerical Derivative Exact Derivative Abs Error 0.1 83.721363 83.907153 0.185790 0.01 555.383031 -8623.188723 9178.571754 0.001 -233.885903 -562379.076291 562145.190388 0.0001 -884.627816 95215536.825901 95216421.453717 1e-05 -736.979383 9993608074.376921 9993608811.356304 <p>Rapid Oscillation: The exact derivative \\(-\\frac{\\cos(\\frac{1}{x})}{x^2}\\) oscillates rapidly, especially as \\(x\\) approaches zero. The \\(\\cos(\\frac{1}{x})\\) term oscillates between <code>-1</code> and <code>1</code>, causing the derivative to fluctuate greatly in both positive and negative directions. The centered difference method can still provide accurate approximations for larger values of \\(x\\) like <code>0.1</code> when we have:</p> <ul> <li>Numerical Derivative: 83.721363 VS Exact Derivative 83.907153</li> </ul> <p>Numerical Stability issues start to appear for smaller values of \\(x\\) such as <code>0.01</code> where we have:</p> <ul> <li>Numerical Derivative: 555.383031 VS Exact Derivative -8623.188723</li> </ul> <p>As \\(x\\) approaches zero, the numerical method begins to exhibit instability due to the oscillatory nature of both the function and its derivative. This instability can be critical in deep learning applications, as we need to optimize functions that handle high-dimensional data.</p> <p>Recall the MNIST dataset challange from my first video? With 784 pixels for such simple data! Imagine now working with high-resolution images, video, or text data, which requires utmost precision.</p>","tags":["Gradient Descent","Numerical Differentiation","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib","Numerical Stability","Deep Learning","Backpropagation","Autodiff"]},{"location":"2024/12/10/instability-in-numerical-differentiation/#final-words","title":"Final words","text":"<p>Numerical differentiation is excellent for understanding the true nature of the derivative, but it falls short when facing real challenges in the Deep Learning field. We should use the exact derivative of the function; tools like <code>backpropagation</code> or <code>autodiff</code> algorithms can compute these derivatives efficiently. That's where the true power of frameworks like <code>PyTorch</code> lies - in <code>autodiff</code>!</p>","tags":["Gradient Descent","Numerical Differentiation","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib","Numerical Stability","Deep Learning","Backpropagation","Autodiff"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/","title":"Text-to-Speech (TTS) Models Overview, Little Theory and Math","text":"<p>Audio is a very complicated data structure, just take a look for a 1 sec waveform...</p> Audio exhibits patterns at multiple time scales. Source: Google DeepMind. <p>Developing a high-quality text-to-speech (TTS) system is a complex task that requires extensive training of machine learning models. While successful TTS models can revolutionize how we interact with technology, enabling natural-sounding speech synthesis for various applications, the path to achieving such models is often paved with challenges and setbacks.</p> <p>Training a TTS model from scratch is a challenging process that involves numerous steps, from data preparation and preprocessing to model architecture selection, hyperparameter tuning, and iterative refinement. Even with state-of-the-art techniques and powerful computational resources, it's not uncommon for initial training attempts to fall short of expectations, yielding suboptimal results or encountering convergence issues. By carefully analyzing the shortcomings of unsuccessful models and identifying the root causes of their underperformance, researchers and practitioners can gain invaluable insights into the intricacies of TTS model training. These lessons can then inform and refine the training process, leading to more robust and high-performing models. In this article, we embark on a comprehensive journey, exploring the intricate world of TTS models.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#how-it-all-started","title":"How it all started?","text":"<p>At Peech, we're dedicated to making Text to Speech accessible to everyone - individuals and publishers alike. Our innovative technology converts web articles, e-books, and any written content into engaging audiobooks. This is especially beneficial for individuals with dyslexia, ADHD, or vision impairments, as well as anyone who prefers listening to reading. The heart of our app lies in its text-to-speech technology. I've recently joined the impressive speech team with the ambition of advancing our capabilities in AI and Machine Learning. To achieve this, we've embarked on a research journey focused on training models in this domain. I'm excited to share with you the results I've achieved in our research and model training. We have done a good job, and I'd like to present the accomplishments to you.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#one-to-many-mapping-problem-in-tts","title":"One-to-Many Mapping Problem in TTS","text":"<p>In TTS, the goal is to generate a speech waveform \\(y\\) from a given input text \\(x\\). This can be represented as a mapping function \\(f\\) such that:</p> \\[y = f(x)\\] <p>However, the mapping from text to speech is not unique, as there can be multiple valid speech outputs \\(y\\) for the same input text \\(x\\). This is because speech is a complex signal that encodes various types of information beyond just the phonetic content, such as pitch, duration, speaker characteristics, prosody, emotion, and more.</p> <p>Let's denote these additional factors as a set of variation parameters \\(v\\). Then, the mapping function \\(f\\) can be rewritten as:</p> \\[y = f(x, v)\\] <p>This means that for a given input text \\(x\\), the speech output \\(y\\) can vary depending on the values of the variation parameters \\(v\\).</p> <p>For example, consider the input text <code>x = \"Hello, how are you?\"</code>. Depending on the variation parameters \\(v\\), we can have different speech outputs:</p> <ul> <li> <p>If \\(v\\) represents pitch contour, we can have a speech output with a rising pitch at the end (questioning tone) or a falling pitch (statement tone).</p> </li> <li> <p>If \\(v\\) represents speaker identity, we can have speech outputs from different speakers, each with their unique voice characteristics.</p> </li> <li> <p>If \\(v\\) represents emotion, we can have speech outputs conveying different emotions, such as happiness, sadness, or anger. Mathematically, we can represent the variation parameters \\(v\\) as a vector of different factors:</p> </li> </ul> \\[v = [v_{\\text{pitch}}, v_{\\text{duration}}, v_{\\text{speaker}}, v_{\\text{prosody}}, v_{\\text{emotion}}, \\dots]\\] <p>The challenge in TTS is to model the mapping function \\(f\\) in such a way that it can generate appropriate speech outputs \\(y\\) for a given input text \\(x\\) and variation parameters \\(v\\). This is known as the one-to-many mapping problem, as there can be multiple valid speech outputs for the same input text, depending on the variation factors.</p> <p>There are two main approaches to modeling the specific aspects of the variation parameters \\(v\\):</p> <ol> <li> <p>Modeling Individual Factors: In this approach, each variation factor is modeled independently, without considering the interdependencies and interactions between different factors. For example, a model might focus solely on predicting pitch contours or duration values, treating them as separate components. While this approach can capture specific aspects of the variation parameters, it fails to model the variation information in a comprehensive and systematic way. The interdependencies between different factors, such as the relationship between pitch and prosody or the influence of speaker characteristics on duration, are not accounted for.</p> </li> <li> <p>Unified Frameworks for Modeling Multiple Factors: Recent research has proposed unified frameworks that aim to model multiple variation factors simultaneously, capturing their complementary nature and enabling more expressive and faithful speech synthesis. In this approach, the TTS model is designed to jointly model and generate multiple variation factors, considering their interdependencies and interactions. For instance, a unified framework might incorporate modules for predicting pitch, duration, and speaker characteristics simultaneously, while also accounting for their mutual influences. Mathematically, this can be represented as a mapping function \\(f\\) that takes the input text \\(x\\) and generates the speech output \\(y\\) by considering the combined effect of multiple variation parameters \\(v\\):</p> </li> </ol> \\[y=f(x,v_{\\text{pitch}},v_{\\text{duration}},v_{\\text{speaker}},v_{\\text{prosody}},v_{\\text{emotion}}, \\dots)\\] <p>By modeling the variation parameters in a unified and comprehensive manner, these frameworks aim to capture the complex relationships between different factors, enabling more expressive and faithful speech synthesis that better reflects the nuances and variations present in natural speech. The unified approach to modeling variation parameters in TTS systems has gained traction in recent years, as it addresses the limitations of modeling individual factors independently and enables the generation of more natural and expressive speech outputs.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#two-sides-of-a-coin-ar-vs-nar","title":"Two sides of a coin: AR vs NAR","text":"<p>Any structured data can be transformed into a sequence. For instance, speech can be represented as a sequence of waveforms. Neural sequence generation methods can be used to generate such sequences, and there are two main categories of models for this purpose: autoregressive (AR) and non-autoregressive (non-AR) sequence generation methods. I believe, that any sequence can be generated using neural sequence generation methods (but not sure).</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#autoregressive-models","title":"Autoregressive models","text":"<p>Autoregressive (AR) sequence generation involves generating a sequence one token at a time in an autoregressive manner. In an AR model, the current value in the sequence is predicted based on the previous values and an error term (often referred to as white noise, which represents the unpredictable or random component).</p> <p>The definition from wikipedia: autoregressive model:</p> <p>In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation) which should not be confused with a differential equation.</p> <p>The AR model's ability to capture sequential dependencies allows it to generate speech with natural-sounding variations, for example - making it expressive. This is because the model can learn to predict the next value in the sequence based on the context provided by the previous values.</p> <p>Autoregressive model of order \\(n\\), denoted as \\(\\text{AR}(n)\\), can be defined as:</p> \\[x_t = b + \\varphi_1x_{t-1} + \\varphi_2x_{t-2} + \\dots + \\varphi_px_{t-n} + \\epsilon_t\\] <p>Using sum notation this can be written as:</p> \\[x_t = b + \\sum_{i=1}^n \\varphi_i x_{t-i} + \\varepsilon_t\\] <p>where \\(b\\) is the bias term, \\(x_t\\) is the current value, \\(x_{t-1}, x_{t-2}, \\ldots, x_{t-n}\\) or \\(x_{t-i}\\) are the \\(n\\) previous values, \\(\\varphi_1, \\varphi_2, \\dots , \\varphi_n\\) are the model parameters and \\(\\varepsilon_t\\) is the error term or the white noise.</p> <p>During training, the model learns the optimal parameters \\(\\varphi_1, \\varphi_2, \\ldots, \\varphi_n\\) by minimizing a loss function, such as the Mean Squared Error (MSE):</p> \\[L(\\varphi) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{x_i})^2\\] <p>where \\(x_i\\) is the true value, \\(\\hat{x_i}\\) is the predicted value, and \\(n\\) is the total number of time steps.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#probabilistic-perspective","title":"Probabilistic Perspective","text":"<p>To define AR Sequence Generation in a probabilistic manner, we can use the chain rule to decompose the joint probability of the sequence into a product of conditional probabilities.</p> <p>AR sequence generation can be formulated using the chain rule of probability. Let \\(x = (x_1, x_2, \\ldots, x_n)\\) be the true sequence of length \\(n\\).</p> \\[P(x) = P(x_1, x_2, ..., x_n)\\] <p>The joint probability of the sequence can be decomposed into a product of conditional probabilities:</p> \\[P(x) = P(x_1, x_2, ..., x_n) = P(x_1)P(x_2 | x_1)P(x_3 | x_1, x_2) \\dots P(x_n | x_1, x_2 \\dots, x_{n-1})\\] <p>Using prod notation this can be written as:</p> \\[P(x) = \\prod_{i=1}^nP(x_i|x_1,x_2,\\dots,x_{i-1})\\] <p>We can simplify the result to:</p> \\[P(x) = \\prod_{i=1}^nP(x_i|\\mathbf{x}_{&lt;i})\\]","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#inference","title":"Inference","text":"<p>During inference, the model generates the sequence one token at a time, using the previously predicted tokens as input:</p> \\[\\hat{x}_t = f(\\hat{x}_{t-1}, \\dots, \\hat{x}_{t-n})\\] <p>where \\(f\\) is the AR model, and \\(\\hat{x}_{t-1}, \\hat{x}_{t-2}, \\ldots, \\hat{x}_{t-n}\\) are the previously predicted tokens. </p> <p>Using sum notation and AR model definition this can be written as:</p> \\[\\hat{x}_t = b + \\sum_{i=1}^n \\varphi_i \\hat{x}_{t-i}\\]","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#ar-model-schema","title":"AR model schema","text":"<p>Note: oversimplified schema, shouldn't be considered seriously. SOS - start of the sequence, EOS - end of the sequence.</p> <p></p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#the-best-example-of-the-ar-world-transformer-llm","title":"The best example of the AR world: Transformer &amp; LLM","text":"<p>Transformers, which are the backbone of many state-of-the-art natural language processing (NLP) and speech models, including Large Language Models (LLMs), use a mechanism called self-attention to model relationships between all pairs of tokens in a sequence. Self-attention allows the model to capture long-range dependencies and weigh the importance of each token relative to others, enabling it to focus on the most relevant tokens when generating the next token. The attention mechanism in Transformers can be thought of as a weighted sum of the input tokens, where the weights represent the importance or relevance of each token for the current prediction. This allows the model to selectively focus on the most relevant tokens when generating the output.</p> <p>Mathematically, the attention mechanism can be represented as \\(QKV\\) attention:</p> \\[\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\] <p>where:</p> <ul> <li>\\(Q\\) (Query) represents the current token or context for which we want to find relevant information.</li> <li>\\(K\\) (Key) and \\(V\\) (Value) represent the input tokens and their corresponding values or representations.</li> <li>\\(d_k\\) is the dimensionality of the key vectors, and the scaling factor \\(\\sqrt{d_k}\\) is used for numerical stability.</li> <li>The softmax function is applied to the scaled dot-product of the query and key vectors, resulting in a set of weights that sum to 1. These weights determine the relative importance of each input token for the current prediction.</li> <li>The weighted sum of the value vectors, \\(V\\), is then computed using the attention weights, producing the final output of the attention mechanism.</li> </ul>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#schema","title":"Schema:","text":"<p>The self-attention mechanism in Transformers allows the model to capture dependencies between tokens in a sequence, regardless of their position or distance. This has made Transformers highly effective for various natural language processing tasks, including language modeling, machine translation, and text generation.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#cons-of-autoregressive-ar-models","title":"Cons of Autoregressive (AR) models","text":"<p>While the autoregressive nature of AR models allows them to capture sequential dependencies and generate expressive sequences, it also introduces a significant drawback: slow inference time, especially for long sequences.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#time-complexity-analysis","title":"Time Complexity Analysis","text":"<p>The time complexity of an algorithm or model refers to how the computational time required for execution scales with the input size. In the case of AR models, the input size is the sequence length, denoted as \\(n\\). During inference, AR models generate sequences one token at a time, using the previously generated tokens to predict the next token. This sequential generation process means that the model needs to process each token in the sequence, resulting in a time complexity of \\(O(n)\\). In other words, the inference time grows linearly with the sequence length. This linear time complexity can become a significant bottleneck in real-time scenarios or when dealing with long sequences, such as generating long texts, speech utterances, or high-resolution images. Even with modern hardware and parallelization techniques, the sequential nature of AR models can make them computationally expensive and slow for certain applications. The slow inference time of AR models can be particularly problematic in the following scenarios:</p> <ol> <li> <p>Real-time speech synthesis: In applications like virtual assistants or text-to-speech systems, the ability to generate speech in real-time is crucial for a seamless user experience. AR models may struggle to keep up with the required speed, leading to noticeable delays or latency.</p> </li> <li> <p>Long-form text generation: When generating long texts, such as articles, stories, or reports, the inference time of AR models can become prohibitively slow, making them impractical for certain use cases. While AR models excel at capturing sequential dependencies and generating expressive sequences, their slow inference time can be a significant limitation in certain applications, especially those requiring real-time or low-latency performance or involving long sequences.</p> </li> </ol> <p>One of the inherent challenges in autoregressive (AR) models is the unavoidable loss of quality for longer sequences. This issue is visually represented in the following plot:</p> <p></p> <p>As you can see, the dispersion or spread of the predicted values increases as the sequence length grows. This phenomenon occurs by design in AR models, and it's crucial to understand the underlying reasons behind it.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#harmful-biases-in-autoregressive-ar-generation","title":"Harmful Biases in Autoregressive (AR) Generation","text":"<p>Autoregressive (AR) models are prone to several harmful biases that can negatively impact their performance and the quality of the generated sequences. These biases can arise from various factors, including the training process, the model architecture, and the nature of the data itself. Let's explore the reasons behind these, why do we have such kind of troubles?</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#error-propagation","title":"Error Propagation","text":"<p>Error propagation occurs when errors in the predicted tokens accumulate and affect the subsequent tokens, leading to a compounding effect, where the errors in the early tokens propagate to the later tokens. This can cause the generated sequence to deviate significantly from the true sequence, especially for longer sequences.</p> <p>For example, the model predicts a sequence of length \\(n\\), denoted as</p> \\[\\mathbf{\\hat{x}_n} = (\\hat{x}_{t-1}, \\hat{x}_{t-2}, \\dots, \\hat{x}_{t-n})\\] <p>Each predicted token \\(\\hat{x}_t\\) is computed based on the previous predicted tokens:</p> \\[\\hat{x}_t = \\varphi_1\\hat{x}_{t-1} + \\varphi_2\\hat{x}_{t-2} + \\dots + \\varphi_p\\hat{x}_{t-n} + b = \\sum_{i=1}^n \\varphi_i\\hat{x}_{t-i} + b\\] <p>Suppose the model predicts a token \\(\\hat{x}_{t-i}\\) with an error \\(\\epsilon_{t-i}\\).  The next token will be predicted based on the erroneous token \\(\\hat{x}_{t-i}\\), leading to a new error. This process continues, and the errors accumulate, leading to a significant deviation from the true sequence.</p> <p>We can define this error propagation as:</p> \\[\\hat{x}_t = (\\varphi_1\\hat{x}_{t-1} + \\epsilon_{t-1}) + (\\varphi_2\\hat{x}_{t-2} + \\epsilon_{t-2}) + \\dots + (\\varphi_p\\hat{x}_{t-n} + \\epsilon_{t-n}) + b\\] <p>Adding the growing error as a separated sum helps to visualize the harmful impact of the error propagation term:</p> \\[\\hat{x}_t = \\sum_{i=1}^n \\varphi_i\\hat{x}_{t-i} + \\sum_{i=1}^n \\epsilon_{t-i} + b\\] <p>The second term, \\(\\sum_{i=1}^n \\epsilon_{t-i}\\), represents the accumulated error propagation term, which grows as the sequence length increases.</p> <p>Error-propagation term: </p> \\[\\text{err} = \\sum_{i=1}^n \\epsilon_{t-i}\\] <p>The worst-case scenario occurs when the error happens on the first token in the sequence, \\(\\hat{x}_1\\). In this case, the error propagates and accumulates through the entire sequence, leading to the maximum deviation from the true sequence.</p> <p>In the context of text-to-speech synthesis, error propagation can lead to the generation of random noise or artifacts in the synthesized audio, making it difficult to avoid and potentially resulting in an unnatural or distorted output.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#label-bias","title":"Label Bias","text":"<p>Label bias occurs when the normalization constraint over the vocabulary items at each decoding step in AR models leads to learning miscalibrated distributions over tokens and sequences. This can happen because the model is forced to assign probabilities that sum to 1 over the entire vocabulary, even if some tokens or sequences are highly unlikely or impossible in the given context.</p> <p>Mathematically, the probability distribution over the vocabulary items at time step \\(t\\) is typically computed using a softmax function:</p> \\[p(x_t | x_{t-1}, \\dots, x_{t-n}) = \\text{softmax}(W x_{t-1} + \\dots + W x_{t-n} + b)\\] <p>where \\(p(x_t | x_{t-1}, ..., x_{t-n})\\) is the probability distribution over the vocabulary items, \\(W\\) is the weight matrix, and \\(b\\) is the bias term.</p> <p>This normalization constraint can lead to a bias towards certain tokens or sequences, even if they are unlikely or irrelevant in the given context. For example, consider a language model trained on a corpus of news articles. If the model encounters a rare or unseen word during inference, it may still assign a non-zero probability to that word due to the normalization constraint, even though the word is highly unlikely in the context of news articles.</p> <p>The consequences of label bias can be significant, especially in cases where the data distribution is highly skewed or contains rare or unseen events. It can lead to the generation of nonsensical or irrelevant tokens, which can degrade the overall quality and coherence of the generated sequences. This bias can be particularly problematic in applications such as machine translation, text summarization, or dialogue systems, where accurate and context-appropriate generation is crucial.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#order-bias","title":"Order Bias","text":"<p>Order bias occurs when the left-to-right generation order imposed by AR models is not the most natural or optimal order for the given task or data. In some cases, the data may prefer a different generation order or require considering the entire context simultaneously, rather than generating tokens sequentially.</p> <p>For example, in text-to-speech synthesis, the natural flow of speech may not follow a strict left-to-right order. The pitch, intonation, or emphasis of a sentence can be determined by the context of the entire sentence, rather than the individual words. If the model is trained to generate the audio sequence in a left-to-right order, it may fail to capture these nuances, leading to an unnatural or strange-sounding output.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#non-autoregressive-models-nar","title":"Non-autoregressive models (NAR)","text":"<p>Non-autoregressive (NAR) sequence generation is an alternative approach to autoregressive (AR) models, where the entire output sequence is generated in parallel, without relying on previously generated tokens. Unlike AR models, which generate the sequence one token at a time in an autoregressive manner, NAR models do not have any sequential dependencies during the generation process. The key advantage of NAR models is their computational efficiency and speed. By generating the entire sequence in a single pass, NAR models can significantly reduce the inference time compared to AR models, which can be particularly beneficial for applications that require real-time or low-latency performance.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#time-complexity-analysis_1","title":"Time Complexity Analysis","text":"<p>The time complexity of NAR models is constant, denoted as \\(O(1)\\), as the generation of the entire sequence is performed in a single pass, regardless of the sequence length. This is in contrast to AR models, which have a linear time complexity or \\(O(n)\\).</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#definition","title":"Definition","text":"<p>Here is how we can define the non-autoregressive model:</p> \\[x_t = f(x, t)\\] <p>where \\(f\\) is the NAR model, \\(x\\) is the input sequence, and \\(t\\) is the time step.</p> <p>In a non-autoregressive model, the output sequence is generated in a single pass, without any sequential dependencies. This can be represented mathematically as:</p> \\[\\hat{x} = f(x)\\] <p>where \\(x\\) is the input sequence, \\(f\\) is the NAR model and \\(\\hat{x}\\) is the output sequence.</p> <p>To define NAR Sequence Generation in a probabilistic manner, we can use the product rule to decompose the joint probability of the sequence into a product of independent probabilities.</p> <p>NAR Sequence Generation as a Probabilistic Model: Let's denote the sequence of tokens as \\(x = (x_1, x_2, \\dots, x_n)\\), where \\(n\\) is the sequence length. We can model the joint probability of the sequence using the product rule:</p> \\[P(x) = P(x_1)P(x_2) \\dots P(x_n) = \\prod_{i=1}^nP(x_i)\\] <p>Note that in a non-autoregressive model, the probability of each token is independent of the previous tokens, in contrast to AR models, which model the conditional probabilities using the chain rule.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#nar-model-schema","title":"NAR model schema","text":"<p>Note: over-oversimplified schema, shouldn't be considered seriously at all. Shows that every token is independent.</p> <p></p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#cons-of-nar-models","title":"Cons of NAR models","text":"<p>NAR models generate each token independently, without considering the context of the previous tokens, models do not capture sequential dependencies between tokens, which can result in generated sequences that do not follow the natural flow of language. The independence assumption in the output space ignores the real dependency between target tokens. NAR models are not well-suited to capture hierarchical structures in language, such as syntax and semantics.</p> <p>Based on the nature NAR models may generate repetitive phonemes or sounds, leading to a lack of naturalness and coherence in the synthesized speech, models may generate shorter or broken utterances, failing to capture the full meaning or context of the input text. NAR models may generate speech that sounds unnatural or robotic, indicating a lack of understanding of the acoustic properties of speech, and also may struggle to capture long-range dependencies between tokens, leading to a lack of coherence and fluency in the generated sequence.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#text-to-speech-non-autoregressive-models","title":"Text-to-Speech Non-Autoregressive models","text":"<p>Despite their limitations, Non-Autoregressive (NAR) models are still widely used in Text-to-Speech (TTS) systems. One of the main advantages of NAR models is their ability to generate speech much faster than Autoregressive (AR) models, thanks to their reduced computational requirements. This makes them well-suited for real-time TTS applications where speed is crucial. While NAR models may not produce speech that is as natural-sounding as AR models, they can still generate high-quality speech that is sufficient for many use cases.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#side-by-side-ar-nar","title":"Side-by-side AR / NAR","text":"<p>To enhance the quality of the synthesized audio in non-autoregressive (NAR) text-to-speech models, researchers have explored the idea of incorporating additional modules or blocks to predict various speech parameters. By explicitly modeling speech parameters, such as pitch, energy, and other prosodic features, the models can generate more expressive and natural-sounding speech.</p> <p>For instance, in the FastPitch model, a dedicated pitch predictor module is introduced to generate pitch contours in parallel, which are then used to condition the mel-spectrogram generation.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#schema-of-fastspeech","title":"Schema of FastSpeech:","text":"<p>While introducing additional modules and conditioning the generation process on various speech parameters can improve the quality of the synthesized audio, it is important to strike a balance between model complexity and computational efficiency. The non-autoregressive nature of these models is designed to achieve faster inference speeds compared to autoregressive models, and adding too many complex modules or dependencies could potentially negate this advantage.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#mostly-adequate-tts-models-review","title":"Mostly adequate TTS models review","text":"<p>In the following sections, I will review the most interesting implementations and key ideas in the field of non-autoregressive (NAR) text-to-speech models. It's important to note that not all models have published research papers or open-source code available. However, I will endeavor to uncover and present the relevant information and insights from the available sources.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#ar-and-hybrid-models","title":"AR and hybrid models","text":"","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#sunoai-bark","title":"SunoAI Bark","text":"<p>My research has begun with the discovery of an incredible open-source solution that can produce incredible results: SunoAI Bark</p> <p>Bark is obviously AR model, but how it works? I found this great repo: audio-webui and author tried to found out how bark works, but:</p> <p>The author is researching voice cloning using the SunoAI Bark model and identified three different models used in the generation process: Semantics, Coarse, and Fine. The author notes that the current method of generating semantic tokens is problematic, as it generates tokens based on the input text rather than the actual audio.</p> <p>The author proposes three potential way for creation of speaker files for voice cloning:</p> <ol> <li> <p>Find out how Bark generates semantic tokens (may not be publicly available).</p> </li> <li> <p>Train a neural network to convert audio files to semantic tokens (time-consuming, but effective).</p> </li> <li> <p>Use a speech converter to change the input audio without editing semantics (not perfect, but better than current methods).</p> </li> </ol> <p>The author has implemented Method 3 using coqui-ai/TTS and achieved decent voice cloning results, but with limitations. </p> <p>Results: Decent voice cloning, not near perfect though. Better than previous methods, but struggles with some voices and accents. These issues lie in the transfer step.</p> <p>They also explored Method 1, but were unable to find a suitable model. </p> <p>Pre: It looks like bark uses AudioLM for the semantic tokens! I'm not sure if they use a different model though. I'll have to test that. But if they don't use a pre-trained model. I can do step 2. Results: No, it doesn't look like i can find a model, i did succeed in creating same-size vector embeddings, but the vectors use the wrong tokens.</p> <p>Method 2, which involves creating a quantizer based on data, has shown promising results, achieving convincing voice cloning with some limitations.</p> <p>Pre: What if instead of training a whole model, i only create a quantizer based on a bunch of data? Results: Successful voice cloning, can be very convincing. Still some limitations. But better than anything I've seen done with bark before.</p> <p>I can't tell you more, please, share any details if you find better explanations of the Bark. AR nature of the model is obvious, that's for sure. I faced with the limitations from the first touch - model can't generate audio more than 13-14 sec length. Several issues about this problem are here:</p> <ul> <li> <p>Limited to 13 seconds?</p> </li> <li> <p>Arbitrarily long text</p> </li> </ul> <p>Comment about it from issue #36 Inquiry on Maximum input character text prompt Length:</p> <p>right now the output is limited by the context window of the model (1024) which equates to about 14s. So text should be around that duration (meaning ~2-3 sentences). For longer texts you can either do them one at a time (and use the same history prompt to continue the same voice) or feed the first generation as the history for the second. i know that that's still a bit inconvenient. will try to add better support for that in the next couple of days</p> <p>You can implement the following approach, split sentences and synthesize the audio chunks:</p> <pre><code>from bark import generate_audio,preload_models\nfrom scipy.io.wavfile import write as write_wav\nimport numpy as np\nimport nltk\n\nnltk.download('punkt')\npreload_models()\n\nlong_string = \"\"\"\nBark is a transformer-based text-to-audio model created by [Suno](https://suno.ai/). Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. To support the research community, we are providing access to pretrained model checkpoints ready for inference.\n\"\"\"\n\nsentences = nltk.sent_tokenize(long_string)\n\n# Set up sample rate\nSAMPLE_RATE = 22050\nHISTORY_PROMPT = \"en_speaker_6\"\n\nchunks = ['']\ntoken_counter = 0\n\nfor sentence in sentences:\n    current_tokens = len(nltk.Text(sentence))\n    if token_counter + current_tokens &lt;= 250:\n        token_counter = token_counter + current_tokens\n        chunks[-1] = chunks[-1] + \" \" + sentence\n    else:\n        chunks.append(sentence)\n        token_counter = current_tokens\n\n# Generate audio for each prompt\naudio_arrays = []\nfor prompt in chunks:\n    audio_array = generate_audio(prompt,history_prompt=HISTORY_PROMPT)\n    audio_arrays.append(audio_array)\n\n# Combine the audio files\ncombined_audio = np.concatenate(audio_arrays)\n\n# Write the combined audio to a file\nwrite_wav(\"combined_audio.wav\", SAMPLE_RATE, combined_audio)\n</code></pre> <p>Also you can use batching: <pre><code>from IPython.display import Audio\n\nfrom transformers import AutoProcessor\nfrom transformers import BarkModel\n\nmodel = BarkModel.from_pretrained(\"suno/bark-small\")\nprocessor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n\ntext_prompt = [\n\u00a0 \u00a0 \"Let's try generating speech, with Bark, a text-to-speech model\",\n\u00a0 \u00a0 \"Wow, batching is so great!\",\n\u00a0 \u00a0 \"I love Hugging Face, it's so cool.\"\n]\n\ninputs = processor(text_prompt).to(device)\n\nwith torch.inference_mode():\n\u00a0 # samples are generated all at once\n\u00a0 speech_output = model.generate(**inputs, do_sample = True, fine_temperature = 0.4, coarse_temperature = 0.8)\n\n# let's listen to the output samples!\n\nAudio(speech_output[2].cpu().numpy(), rate=sampling_rate)\n</code></pre></p> <p>But the butch generation creates a new problem. The quality of the audio is dramatically reduced, and it generates strange noises, much more often than in the case of generation in order. I found a great description in the comment: output quality seems worse if you pack it like that One more problem that I found is empty sounds if your text chunks are different sizes. Sometimes it's a random noise like background voices etc. So, you need to split the text carefully, it's very important. Bark occasionally hallucinates, which can lead to unexpected and inaccurate results. Unfortunately, it is impossible to detect this issue during the generation process, making it challenging to identify and correct. Furthermore, even during the review process, hallucinations can be difficult to spot, as they may be subtle or blend in with the rest of the generated content. This can result in incorrect or misleading information being presented as factual, which can have serious consequences in certain applications.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#note","title":"\ud83d\udcd3 Note","text":"<p>I'm still unsure how bark works. I am trying to figure out how it works though. My current knowledge goes here.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#sunoai-summary","title":"SunoAI summary","text":"<p>From what I understand, the issue of long context is not a priority for the Bark team and is largely being overlooked in the issues discussion. The only solution they've proposed is to use batches, but this approach has its limitations and doesn't work well. Moreover, due to the text context, it can't be processed accurately. Specifically, a small context of 2-3 sentences tends to disregard what happened in the previous passage, which can significantly impact the intonation and tempo of a speaker. Bark occasionally hallucinates, which can lead to unexpected and inaccurate results.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#vall-e-neural-codec-language-models-are-zero-shot-text-to-speech-synthesizers","title":"VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers","text":"<p>Samples on the demo page.</p> <p>VALL-E is a novel language model approach for text-to-speech (TTS) synthesis that leverages large-scale, diverse, and multi-speaker speech data. Unlike traditional cascaded TTS systems, VALL-E treats TTS as a conditional language modeling task, using discrete audio codec codes as intermediate representations. This approach enables VALL-E to synthesize high-quality, personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt.</p> <p>Key Components:</p> <ol> <li> <p>Neural Codec Model: VALL-E uses a pre-trained neural audio codec model, EnCodec, to convert audio waveforms into discrete acoustic codes. This model is trained on a large dataset of speech and can reconstruct high-quality waveforms even for unseen speakers.</p> </li> <li> <p>Language Model: VALL-E is trained as a language model, using the discrete acoustic codes as input and output. This allows the model to learn the patterns and structures of speech, enabling it to generate coherent and natural-sounding speech.</p> </li> <li> <p>In-Context Learning: VALL-E has strong in-context learning capabilities, similar to GPT-3, which enables it to adapt to new speakers and acoustic environments with minimal additional training data.</p> </li> </ol>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#the-problem-formulation-is-valid-for-bark-as-well-except-dimensions","title":"The Problem Formulation is valid for Bark as well (except dimensions):","text":"<p>Problem Formulation: Regarding TTS as Conditional Codec Language Modeling</p> <p>Given a dataset \\(D = \\{x_i, y_i\\}\\), where \\(y\\) is an audio sample and \\(x = \\{x_0, x_1, \\dots, x_L\\}\\) is its corresponding phoneme transcription, we use a pre-trained neural codec model to encode each audio sample into discrete acoustic codes, denoted as \\(\\text{Encodec}(y) = C^{T\\times8}\\), where \\(C\\) represents the two-dimensional acoustic code matrix, and \\(T\\) is the downsampled utterance length. The row vector of each acoustic code matrix \\(c_{t,:}\\) represents the eight codes for frame \\(t\\) and the column vector of each acoustic code matrix \\(c_{:,j}\\) represents the code sequence from the \\(j\\)-th codebook, where \\(j \\in \\{1, \\dots, 8\\}\\). After quantization, the neural codec decoder is able to reconstruct the waveform, denoted as \\(\\text{Decodec}(C) \\approx \\hat{y}\\).</p> <p></p> <p>Based on the paper, it appears that the proposed model combines the principles of Autoregressive (AR) and Non-Linear Autoregressive (NAR) models:</p> <p>The combination of the AR model and the NAR model provides a good trade-off between speech quality and inference speed. On the one hand, the rate of the generated speech should be consistent with the enrolled recording, and it is hard to train a length predictor for different speakers since their speaking speed may be very diverse. In this case, the AR model is a more natural choice with its flexibility for acoustic sequence length prediction. On the other hand, for the consecutive stages, as the number of output slots follows the sequence length of the first stage, NAR can reduce the time complexity from \\(O(T)\\) to \\(O(1)\\). Overall, the prediction of \\(C\\) can be modeled as: \\(p(C|x, \\tilde{C}; \\theta) = p(c_{:,1}| \\tilde{C}_{:,1}, X; \\theta_{\\text{AR}}) \\prod_{x=2}^8 p(c_{:,j}|c_{:,&lt;j}, x, \\tilde{C}; \\theta_{\\text{NAR}})\\)</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#vall-e-implementations","title":"VALL-E implementations","text":"<p>Note that there is currently no official implementation of VALL-E available. Even ETA for the official release is unknown VALL-E ETA?. The official repository currently contains an empty space, awaiting further updates. However, for those interested in experimenting with VALL-E, an unofficial PyTorch implementation is available at unofficial PyTorch implementation of VALL-E. This implementation can be trained on a single GPU and is widely used in the community.</p> <p>After eight hours of training, the model can synthesize natural speech!</p> <p>code branch <code>position</code> PR #46 egs <code>https://github.com/lifeiteng/vall-e/tree/position/egs/ljspeech</code> synthesised speech https://github.com/lifeiteng/vall-e/tree/position/egs/ljspeech/demos The next step is to verify <code>in-context learning capabilities</code> of VALL-E on a large dataset.</p> <p>And I found one more training story: After 100 epochs training, the model can synthesize natural speech on LibriTTS</p> <p>I trained vall-e on LibriTTS about 100 epochs (took almost 4 days on 8 A100 GPUs) and I obtained plausible synthesized audio.</p> <p>Here is a demo.</p> <p>[1] prompt : prompt_link  synthesized audio : synt_link</p> <p>[2] prompt : prompt_link  ground truth : gt_link  synthesized audio : synt_link</p> <p>[3] prompt : prompt_link  synthesized audio : synt_link</p> <p>[4] prompt : prompt_link  ground truth : gt_link  synthesized audio : synt_link</p> <p>The model I trained has worse quality than the original vall-e because of the dataset amount. However, it has promising quality in clean audio. I'm not sure whether I can share my pre-trained LibriTTS model. If I can, I would like to share the pre-trained LibriTTS model.</p> <p>Model weights:</p> <p>Sorry for late reply. This is the model that I trained.  google drive link : link</p> <p>One more unofficial PyTorch implementation of VALL-E, based on the EnCodec tokenizer.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#vall-e-summary","title":"VALL-E summary","text":"<p>So, VALL-E is an AR or NAR model? VALL-E is a hybrid model that leverages the strengths of both AR and NAR architectures.</p> <p>In section 6 Conclusion, Limitations, and Future Work you can see one important note:</p> <p>Synthesis robustness: We observe that some words may be unclear, missed, or duplicated in speech synthesis. It is mainly because the phoneme-to-acoustic language part is an autoregressive model, in which disordered attention alignments exist and no constraints to solving the issue. The phenomenon is also observed in vanilla Transformer-based TTS, which was addressed by applying non-autoregressive models or modifying the attention mechanism in modeling. In the future, we would like to leverage these techniques to solve the issue.</p> <p>It's interesting to note that the authors acknowledge this limitation and plan to explore techniques from non-autoregressive models or modified attention mechanisms to address this issue in future work. Even state-of-the-art AR models struggle with limitations like disordered attention alignments, leading to synthesis robustness issues.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#styledtts-2-towards-human-level-text-to-speech-through-style-diffusion-and-adversarial-training-with-large-speech-language-models","title":"StyledTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models","text":"<p>Github repo StyleTTS 2 is a state-of-the-art text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. By modeling styles as a latent random variable through diffusion models, StyleTTS 2 is able to generate the most suitable style for the input text in diverse speech synthesis. The use of large pre-trained SLMs, such as WavLM, as discriminators with differentiable duration modeling enables end-to-end training and improves speech naturalness.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#style-diffusion","title":"Style Diffusion","text":"<p>StyleTTS 2 models speech \\(x\\) as a conditional distribution \\(p(x|t) = \\int p(x|t, s)p(s|t) \\mathrm{d}s\\), where \\(t\\) represents the phonetic content, and \\(s\\) is a latent variable representing the generalized speech style. This style variable \\(s\\) is sampled using Efficient Diffusion Models (EDM), which follow the combined probability flow and time-varying Langevin dynamics. By modeling styles as a latent random variable through diffusion models, StyleTTS 2 can generate the most suitable style for the input text, enabling diverse speech synthesis.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#adversarial-training-with-large-slms","title":"Adversarial Training with Large SLMs","text":"<p>StyleTTS 2 employs large pre-trained SLMs, such as WavLM, as discriminators during training. This adversarial training, combined with the proposed differentiable duration modeling, results in improved speech naturalness.</p> <p></p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Differentiable Upsampling and Fast Style Diffusion: StyleTTS 2 introduces a differentiable upsampling method that allows for generating speech samples during training in a fully differentiable manner, similar to inference. These generated samples are used to optimize the loss function involving the large pre-trained SLM, enabling end-to-end training of all components.</p> </li> <li> <p>Efficient Style Sampling: To enable fast style sampling during inference, StyleTTS 2 uses the ancestral DPM-2 solver instead of the 2nd-order Heun method, allowing for fast and diverse sampling without significantly impacting inference speed.</p> </li> <li> <p>Multispeaker Modeling: For multispeaker settings, StyleTTS 2 models \\(p(s|t, c)\\), where \\(c\\) is a speaker embedding obtained from a reference audio of the target speaker. This speaker embedding is injected into the transformer model using adaptive layer normalization.</p> </li> </ol>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#known-issues","title":"Known Issues","text":"<p>It's worth noting that StyleTTS 2 has a known issue where loud white noise can occur on short texts when the <code>embedding_scale</code> parameter is set to a value greater than 1 (see GitHub issue #46).</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#hybrid-ar-nar-approach-perspective","title":"Hybrid AR-NAR Approach perspective","text":"<p>StyleTTS 2 presents a novel and promising approach to text-to-speech synthesis by combining the strengths of both autoregressive (AR) and non-autoregressive (NAR) models. This hybrid architecture leverages the power of large pre-trained SLMs, which are typically autoregressive models, while incorporating the diversity and expressiveness offered by diffusion models, which are non-autoregressive. By combining the pre-trained SLM with the style diffusion process, StyleTTS 2 effectively creates a hybrid architecture that combines the strengths of both AR and NAR models. The SLM provides a strong foundation for capturing sequential dependencies and generating high-quality speech, while the diffusion process introduces diversity and expressiveness, enabling the generation of natural-sounding speech with appropriate styles and characteristics. The combination of style diffusion and adversarial training with large SLMs has proven to be a powerful approach for generating natural and expressive speech.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#non-ar-models","title":"Non AR models","text":"","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#fastpitch-parallel-text-to-speech-with-pitch-prediction","title":"FastPitch: Parallel Text-to-Speech with Pitch Prediction","text":"<p>Implementation NVIDIA/NeMo: FastPitch and Mixer-TTS Training</p> <p>Traditional autoregressive TTS models, which generate speech one step at a time, suffer from slow inference speeds, making them unsuitable for real-time applications. FastPitch, proposed by NVIDIA in 2020, is a fully-parallel text-to-speech model that addresses these challenges by introducing parallel pitch prediction and conditioning the mel-spectrogram generation on the predicted pitch contours. It is based on FastSpeech and composed mainly of two feed-forward Transformer (FFTr) stacks. The first one operates in the resolution of input tokens, the second one in the resolution of the output frames. Let \\(x = (x_1, \\dots, x_n)\\) be the sequence of input lexical units, and \\(y = (y_1, \\dots, y_t)\\) be the sequence of target mel-scale spectrogram frames. The first FFTr stack produces the hidden representation \\(h = \\text{FFTr}(x)\\). The hidden representation \\(h\\) is used to make predictions about the duration and average pitch of every character with a 1-D CNN</p> <p></p> \\[d = \\text{DurationPredictor}(h), \\hat{p} = \\text{PitchPredictor}(h)\\] <p>where \\(\\hat{d} \\in \\mathbb{N}^n\\) and \\(\\hat{p} \\in \\mathbb{R}^n\\). Next, the pitch is projected to match the dimensionality of the hidden representation \\(h \\in \\mathbb{R}^{n\u00d7d}\\) and added to \\(h\\). The resulting sum \\(g\\) is discretely up-sampled and passed to the output FFTr, which produces the output mel-spectrogram sequence:</p> <p>\\(g = h + \\text{PitchEmbedding}(p)\\) \\(\\hat{y} = \\text{FFTr}([\\underbrace{g_1, \\dots, g_1}_{d_1}, \\dots \\underbrace{g_n, \\dots, g_n}_{d_n} ])\\)</p> <p>Ground truth \\(p\\) and \\(d\\) are used during training, and predicted \\(\\hat{p}\\) and \\(\\hat{d}\\) are used during inference. The model optimizes mean-squared error (MSE) between the predicted and ground-truth modalities \\(\\mathcal{L} = ||\\hat{y} \u2212 y||^2_2 + \u03b1|| \\hat{p} \u2212 p ||^2_2 + \u03b3||\\hat{d} \u2212 d||^2_2\\)</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#key-contributions_1","title":"Key Contributions","text":"<p>Parallel Pitch Prediction: One of the main innovations of FastPitch is its ability to predict the fundamental frequency (F0) contours, which represent the pitch variations in speech, directly from the input text in a fully-parallel manner. Unlike previous TTS models that predicted pitch contours autoregressively, FastPitch employs a feed-forward architecture (FFTr) and a dedicated pitch predictor to generate the pitch contours in a single pass. This parallel pitch prediction eliminates the need for autoregressive processing, leading to faster inference times.</p> <p>Conditioning on Pitch Contours: FastPitch conditions the mel-spectrogram generation, which represents the spectral characteristics of speech, on the predicted pitch contours. By explicitly modeling and incorporating the pitch information, FastPitch can capture and reproduce the natural variations in pitch and intonation present in human speech, resulting in more natural-sounding synthesized audio.</p> <p>Feed-Forward Architecture: FastPitch is a feed-forward model, which means that it does not rely on recurrent neural networks (RNNs) or attention mechanisms. This architecture choice contributes to the model's efficiency and parallelizability.</p> <p>Improved Speech Quality: By explicitly modeling pitch contours and conditioning the mel-spectrogram generation on them, FastPitch achieves improved speech quality compared to its predecessor, FastSpeech. The synthesized speech exhibits more natural-sounding pitch variations and intonation patterns.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#limitations-and-future-work","title":"Limitations and Future Work","text":"<p>While FastPitch represents a significant advancement in parallel text-to-speech synthesis, the authors acknowledge some limitations and suggest future research directions:</p> <p>Prosody Modeling: Although FastPitch improves pitch prediction, further work is needed to model other aspects of prosody, such as duration and energy contours, in a parallel and efficient manner.</p> <p>Multi-Speaker Adaptation: The FastPitch model presented in the paper is trained on a single-speaker dataset. Extending the model to handle multiple speakers and enable efficient speaker adaptation remains an open challenge.</p> <p>Deployment and Inference Optimization: While FastPitch offers faster inference times compared to autoregressive models, further optimization and deployment strategies could be explored to enable real-time applications and on-device inference.</p> <p>Overall, FastPitch is a notable contribution to the field of text-to-speech synthesis, introducing parallel pitch prediction and demonstrating improved speech quality while maintaining computational efficiency. However, there is still room for further research and development to address the remaining limitations and expand the model's capabilities.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#delightfultts-the-microsoft-speech-synthesis-system-for-blizzard-challenge-2021","title":"DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021","text":"<p>DelightfulTTS is a text-to-speech (TTS) system developed by Microsoft for the Blizzard Challenge 2021, a prestigious annual evaluation of TTS systems. This system aims to generate high-quality and expressive speech while maintaining computational efficiency.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#architecture","title":"Architecture","text":"<p>DelightfulTTS is based on a non-autoregressive (NAR) architecture, which allows for parallel generation of mel-spectrograms, enabling faster inference compared to traditional autoregressive models. The acoustic model still faces the challenge of the one-to-many mapping problem, where multiple speech variations can correspond to the same input text. To address this issue, DelightfulTTS systematically models variation information from both explicit and implicit perspectives:</p> <ol> <li> <p>For explicit factors like speaker ID and language ID, lookup embeddings are used during training and inference.</p> </li> <li> <p>For explicit factors like pitch and duration, the values are extracted from paired text-speech data during training, and dedicated predictors are employed to estimate these values during inference.</p> </li> <li> <p>For implicit factors like utterance-level and phoneme-level prosody, reference encoders are used to extract the corresponding values during training, while separate predictors are employed to predict these values during inference.</p> </li> </ol> <p>The acoustic model in DelightfulTTS is built upon non-autoregressive generation models like FastSpeech, incorporating an improved Conformer module to better capture local and global dependencies in the mel-spectrogram representation. By explicitly modeling variation information from multiple perspectives and leveraging non-autoregressive architectures, DelightfulTTS aims to generate high-quality and expressive speech while addressing the one-to-many mapping challenge inherent in text-to-speech synthesis.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#schema_1","title":"Schema:","text":"","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#conformer-architecture","title":"Conformer Architecture","text":"<p>The Conformer architecture is a variant of the Transformer model that integrates both convolutional neural networks (CNNs) and self-attention mechanisms. It was originally proposed for end-to-end speech recognition tasks and later adopted for TTS systems due to its ability to capture both local and global dependencies in the input sequences. In the context of TTS, the Conformer architecture is typically used in the acoustic model, which is responsible for generating the mel-spectrogram representation of speech from the input text. Mathematically, the Conformer block can be represented as follows: Let \\(X\\) be the input sequence. The key components of the Conformer block used in the acoustic model are:</p> <ol> <li> <p>Convolutional Feed-Forward Module: This module applies a 1D convolutional layer to the input sequence, capturing local dependencies. \\(X' = \\text{Conv1D}(X) + X\\)</p> </li> <li> <p>Depthwise Convolution Module: This module applies a depthwise 1D convolutional layer, further enhancing the modeling of local correlations. \\(X'' = \\text{DepthwiseConv1D}(X') + X'\\)</p> </li> <li> <p>Self-Attention Module: This module employs multi-headed self-attention, allowing the model to capture global dependencies and long-range interactions within the input sequence. \\(X''' = \\text{MultiHeadAttention}(X'') + X''\\)</p> </li> <li> <p>Second Convolutional Feed-Forward Module: Another convolutional feed-forward module is applied after the self-attention module, further processing the output. \\(Y = \\text{Conv1D}(X''') + X'''\\)</p> </li> </ol> <p>Here, \\(\\text{Conv1D}\\), \\(\\text{DepthwiseConv1D}\\), and \\(\\text{MultiHeadAttention}\\) represent the convolutional, depthwise convolutional, and multi-headed self-attention operations, respectively. The Conformer architecture combines the strengths of CNNs and self-attention, allowing the model to capture both local and global dependencies in the input sequence. This is particularly important for TTS tasks, where the model needs to generate mel-spectrograms of varying lengths while capturing the prosodic and acoustic characteristics of speech. In the DelightfulTTS model, the authors likely employed the Conformer architecture in the acoustic model to benefit from its ability to model both local and global dependencies, leading to improved prosody and audio fidelity in the synthesized speech.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#variation-information-modeling","title":"Variation Information Modeling","text":"<p>Text-to-speech (TTS) is a typical one-to-many mapping problem where there could be multiple varying speech outputs (e.g., different pitch, duration, speaker, prosody, emotion, etc.) for a given text input. It is critical to model these variation information in speech to improve the expressiveness and fidelity of synthesized speech. While previous works have tried different methods to model the information, they focus on a specific aspect and cannot model in a comprehensive and systematic way. In this paper, considering that different variation information can be complementary to each other, the authors propose a unified way to model them in the proposed variance adaptor (as shown in Figure 1c).</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#categorization-of-variation-information","title":"Categorization of Variation Information","text":"<p>Observing that some variation information can be obtained implicitly (e.g., pitch can be extracted by some tools) or explicitly (e.g., utterance-level prosody can only be learned by the model), the authors categorize all the information they model as follows:</p> <ol> <li> <p>Explicit Modeling: Language ID, Speaker ID, Pitch, Duration</p> </li> <li> <p>Implicit Modeling: Utterance-level prosody, Phoneme-level prosody</p> </li> </ol> <p>For speaker and language ID, lookup embeddings are used in training and inference. For pitch and duration, the values are extracted from paired text-speech data in training, and two predictors are used to predict the values in inference. For utterance-level and phoneme-level prosody, two reference encoders are used to extract the values in training , and two separate predictors are used to predict the values in inference. The two reference encoders are both made up of convolution and RNN layers. Utterance-level prosody vector is obtained by the last RNN hidden and a style token layer. Phoneme-level prosody vectors are obtained by using the outputs of the phoneme encoder (phoneme-level) as a query to attend to the outputs of the mel-spectrogram reference encoder (frame-level). Different from, the authors do not use VAE but directly use the latent representation as the phoneme-level vector for training stability . The utterance-level prosody predictor contains a GRU layer followed by a bottleneck module to predict the prosody vector. The phoneme-level prosody predictor takes both the outputs of the text encoder and the utterance-level prosody vector as input. With the help of the utterance-level prosody vector, the authors do not need an autoregressive prosody predictor as in for faster inference. By unifying explicit and implicit information in different granularities (language-level, speaker-level, utterance-level, phoneme-level) in the variance adaptor, the authors aim to achieve better expressiveness in prosody and controllability in pitch and duration.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#key-contributions_2","title":"Key Contributions","text":"<ul> <li> <p>Non-Autoregressive Architecture: DelightfulTTS employs a non-autoregressive architecture, enabling faster inference compared to autoregressive models while maintaining high speech quality.</p> </li> <li> <p>Explicit Prosody Modeling: The system explicitly models pitch and energy contours, in addition to duration, to capture the prosodic variations in speech, leading to more natural and expressive synthesized speech.</p> </li> <li> <p>Multi-Speaker Capabilities: DelightfulTTS supports multi-speaker synthesis by conditioning the generation process on speaker embeddings, enabling efficient speaker adaptation.</p> </li> </ul>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#limitations-and-future-work_1","title":"Limitations and Future Work","text":"<p>While DelightfulTTS represents a significant advancement in non-autoregressive TTS, the authors acknowledge some limitations and suggest future research directions:</p> <ul> <li> <p>Robustness to Long Inputs: The system's performance may degrade for very long input sequences, and further work is needed to improve robustness and consistency for such cases.</p> </li> <li> <p>Fine-Grained Prosody Control: While DelightfulTTS models pitch, energy, and duration, additional work could explore more fine-grained control over prosodic aspects, such as emphasis and phrasing.</p> </li> <li> <p>Deployment and Optimization: Further optimization and deployment strategies could be explored to enable real-time applications and on-device inference, leveraging the non-autoregressive architecture's potential for efficiency.</p> </li> </ul> <p>Overall, DelightfulTTS is a notable contribution to the field of text-to-speech synthesis, demonstrating the potential of non-autoregressive architectures and explicit prosody modeling for generating high-quality and expressive speech. The system's performance in the Blizzard Challenge 2021 highlights its competitiveness and the progress made in this domain.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#tts-review-summary","title":"TTS Review summary","text":"<p>Based on my research, I haven't found a clear convergence between the areas of autoregressive (AR) and non-autoregressive (NAR) text-to-speech models in terms of combining or composing them into a single unified model. The key requirements for NAR models are sustainability, the ability to guarantee stability in the generated audio sequences, balanced quality, and fast generation speed, which is a significant advantage of the NAR architecture. Both the FastPitch and DelightfulTTS models meet these criteria. However, I prefer the DelightfulTTS model because it is more complex compared to FastPitch and can potentially provide more diverse results. The DelightfulTTS model's complexity and diversity could be further enhanced by incorporating ideas from the StyledTTS2 model, which uses diffusion as an AR variator to make the speech more unique and expressive based on the context. While the diffusion-based approach in StyledTTS2 can introduce expressiveness, the NAR nature of DelightfulTTS can contribute to the stability of the generated speech. Therefore, a potential direction could be to explore a hybrid approach that combines the strengths of DelightfulTTS's NAR architecture with the expressive capabilities of the diffusion-based variator from StyledTTS2. Such a hybrid model could leverage the stability and efficiency of the NAR framework while incorporating the contextual expressiveness and diversity offered by the diffusion-based variator. However, it's important to note that integrating these two approaches into a single unified model may pose challenges, and further research is needed to investigate the feasibility and effectiveness of such a hybrid approach.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#delightfultts-implementation-and-training","title":"DelightfulTTS implementation and training","text":"<p>DelightfulTTS Github repo</p> <p>After conducting research, I decided to implement the DelightfulTTS model. I didn't find many details in the paper about the implementation and hyperparameters, but I found the Comprehensive-Transformer-TTS repository and was heavily influenced by this implementation. For example, you can check the Conformer implementation: Comprehensive-Transformer-TTS/model/transformers/conformer.py</p> <p>I also found another great implementation on GitHub: dunky11/voicesmith</p> <p>VoiceSmith makes it possible to train and infer on both single and multispeaker models without any coding experience. It fine-tunes a pretty solid text to speech pipeline based on a modified version of DelightfulTTS and UnivNet on your dataset.</p> <p>And there's also a coqui-ai PR Add Delightful-TTS model implemetation.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#pulp-friction","title":"Pulp Friction","text":"<p>I chose to use my own training code due to the flexibility it offers in the training process. While PyTorch Lightning is an excellent training framework that simplifies multi-GPU training and incorporates many best practices, custom implementations come with their own set of advantages and disadvantages.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#how-to-train-our-model","title":"How to Train Our Model","text":"<p>Using PyTorch Lightning to train a model is very straightforward. My training script allows you to train on a single machine with either one or multiple GPUs.</p> <p>I tried to support the hyperparameters and configurations in one place. You can find the configuration in the <code>models/config</code> directory. <code>PreprocessingConfig</code> is the basic piece of configuration where you can set the configuration for preprocessing audio used for training.</p> <p><code>PreprocessingConfigUnivNet</code> and <code>PreprocessingConfigHifiGAN</code> have different <code>n_mel_channels</code>, and these two configurations are not compatible because this parameter fundamentally defines the architecture of the <code>AcousticModel</code>.</p> <p>For the <code>PreprocessingConfigHifiGAN</code> , we have sampling rates of 22050 or 44100. You can see the post-initialization action that sets the parameters of <code>stft</code> based on this parameter:</p> <pre><code>def __post_init__(self):\n    r\"\"\"It modifies the 'stft' attribute based on the 'sampling_rate' attribute.\n    If 'sampling_rate' is 44100, 'stft' is set with specific values for this rate.\n    If 'sampling_rate' is not 22050 or 44100, a ValueError is raised.\n\n    Raises:\n        ValueError: If 'sampling_rate' is not 22050 or 44100.\n    \"\"\"\n    if self.sampling_rate == 44100:\n        self.stft = STFTConfig(\n            filter_length=2048,\n            hop_length=512,\n            win_length=2048,\n            n_mel_channels=80,\n            mel_fmin=20,\n            mel_fmax=11025,\n        )\n    if self.sampling_rate not in [22050, 44100]:\n        raise ValueError(\"Sampling rate must be 22050 or 44100\")\n</code></pre> <p>To set up the cluster, you need to define the following environment variables:</p> <pre><code># Node runk in the cluster\nnode_rank = 0\n# Num nodes in the cluster\nnum_nodes = 1\n\nos.environ[\"WORLD_SIZE\"] = f\"{num_nodes}\"\nos.environ[\"NODE_RANK\"] = f\"{node_rank}\"\n\n# Setup of the training cluster port\nos.environ[\"MASTER_PORT\"] = \"12355\"\n\n# Change the IP address to the IP address of the master node\nos.environ[\"MASTER_ADDR\"] = \"10.148.0.6\"\n</code></pre> <p>You can also choose the logs directory and checkpoints paths:</p> <pre><code>default_root_dir = \"logs\"\nckpt_acoustic = \"./checkpoints/epoch=301-step=124630.ckpt\"\nckpt_vocoder = \"./checkpoints/vocoder.ckpt\"\n</code></pre> <p>Here's my Trainer code. I use the <code>DDPStrategy</code> because it's the only strategy that works in my case. Other useful parameters are:</p> <pre><code>trainer = Trainer(\n    accelerator=\"cuda\",  # Use GPU acceleration\n    devices=-1,  # Use all available GPUs\n    num_nodes=num_nodes,\n    strategy=DDPStrategy(\n        gradient_as_bucket_view=True,  # Optimize gradient communication\n        find_unused_parameters=True,  # Ignore unused parameters\n    ),\n    default_root_dir=default_root_dir,  # Directory for logs\n    enable_checkpointing=True,  # Enable checkpoint saving\n    accumulate_grad_batches=5,  # Accumulate gradients for 5 batches\n    max_epochs=-1,  # Train indefinitely (until manually stopped)\n    log_every_n_steps=10,  # Log every 10 steps\n    gradient_clip_val=0.5,  # Clip gradients to a maximum value of 0.5\n)\n</code></pre> <p>Import the preprocessing configuration that you want to train and choose the type of training. For example:</p> <pre><code>from models.config import PreprocessingConfigUnivNet as PreprocessingConfig\n\n# ...\n\npreprocessing_config = PreprocessingConfig(\"multilingual\")\nmodel = DelightfulTTS(preprocessing_config)\n\ntuner = Tuner(trainer)\n\ntrain_dataloader = model.train_dataloader(\n    # NOTE: Preload the cached dataset into the RAM\n    # Use only if you have enough RAM to cache your entire dataset\n    cache_dir=\"/dev/shm/\",\n    cache=True,\n)\n\ntrainer.fit(\n    model=model,\n    train_dataloaders=train_dataloader,\n    # Resume training states from the checkpoint file\n    ckpt_path=ckpt_acoustic,\n)\n</code></pre> <p>The entry point to the project is inside the train.py file, which serves as the foundation of the training process.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#inference-code","title":"Inference code","text":"<p>For inference, you can use the code examples from <code>demo/demo.py</code>. You have three possible options for combining the acoustic model and vocoder:</p> <ul> <li>DelightfulTTS + UnivNet, with a sample rate of 22050 Hz</li> <li>DelightfulTTS + HiFi-GAN, with a sample rate of 44100 Hz</li> <li>FastPitch + HiFi-GAN, with a sample rate of 44100 Hz</li> </ul> <p>You can experiment with these TTS models and find the best fit for your use case.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#dataset-code","title":"Dataset code","text":"<p>For managing datasets, I utilize the lhotse library. This library allows you to wrap your audio dataset, prepare cutset files, and then filter or select specific subsets of the data. For example:</p> <pre><code># Duration lambda\nself.dur_filter = (\n    lambda duration: duration &gt;= self.min_seconds\n    and duration &lt;= self.max_seconds\n)\n\n# Filter the HiFiTTS cutset to only include the selected speakers\nself.cutset_hifi = self.cutset_hifi.filter(\n    lambda cut: isinstance(cut, MonoCut)\n    and str(cut.supervisions[0].speaker) in self.selected_speakers_hi_fi_ids_\n    and self.dur_filter(cut.duration),\n).to_eager()\n</code></pre> <p>With lhotse, you can filter your dataset based on various criteria, such as speaker identities or audio duration. Notably, you don't need to read the audio metadata for this step, as the metadata is already present in the cutset file, which can be prepared beforehand. The lhotse library provides a wide range of functions and utilities for managing and processing speech datasets efficiently. I highly recommend exploring its capabilities to streamline your dataset preparation and filtering tasks.</p> <p>I have prepared the following ready-to-use datasets:</p> <ol> <li> <p>HifiLibriDataset: This dataset code combines the Hi-Fi Multi-Speaker English TTS Dataset and LibriTTS-R. You can choose specific speakers from these datasets or filter the audio based on various parameters. The dataset code can cache data to the filesystem or RAM if needed. This dataset can be used to train the DelightfulTTS model.</p> </li> <li> <p>HifiGanDataset: This dataset code is designed for the HiFi-GAN model. The data is prepared in a specific way, as you can see in the <code>__getitem__</code> method. With this dataset implementation, you can train the HiFi-GAN vocoder.</p> </li> <li> <p>LIBRITTS_R: This code is a modified version of the Pytorch LIBRITTS dataset.</p> </li> </ol> <p>Additionally, you can explore the dataset code I prepared for various experiments within this project.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#docs","title":"Docs","text":"<p>I firmly believe that a good project starts with comprehensive documentation, and good code is built upon a solid foundation of test cases. With this in mind, I have made concerted efforts to maintain consistent documentation and ensure thorough test coverage for my code. The repository serves as a comprehensive resource where you can explore the implementation details, review the documentation, and examine the test cases that ensure the code's reliability and correctness.</p> <p>You can find all the documentation inside the <code>docs</code> directory, run the docs locally with <code>mkdocs serve</code></p> <p>Also here you can the docs online</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#acoustic-model","title":"Acoustic model","text":"<p>The acoustic model is responsible for generating mel-spectrograms from input phoneme sequences, speaker identities, and language identities.</p> <p>The core of your acoustic model is the AcousticModel class, which inherits from PyTorch's <code>nn.Module</code>. This class contains several sub-modules and components that work together to generate the mel-spectrogram output.</p> <ol> <li> <p>Encoder: The encoder is a Conformer module that processes the input phoneme sequences. It takes the phoneme embeddings, speaker embeddings, language embeddings, and positional encodings as input and generates a hidden representation.</p> </li> <li> <p>Prosody Modeling:</p> </li> <li> <p>Utterance-Level Prosody: The model includes an UtteranceLevelProsodyEncoder and a PhonemeProsodyPredictor (for utterance-level prosody) to capture and predict the prosodic features at the utterance level.</p> </li> <li> <p>Phoneme-Level Prosody: Similarly, there is a PhonemeLevelProsodyEncoder and a PhonemeProsodyPredictor (for phoneme-level prosody) to model the prosodic features at the phoneme level.</p> </li> <li> <p>Variance Adaptor:</p> </li> <li> <p>Pitch Adaptor: The PitchAdaptorConv module is responsible for adding pitch embeddings to the encoder output, based on the predicted or target pitch values.</p> </li> <li> <p>Energy Adaptor: The EnergyAdaptor module adds energy embeddings to the encoder output, based on the predicted or target energy values.</p> </li> <li> <p>Length Adaptor: The LengthAdaptor module upsamples the encoder output to match the length of the target mel-spectrogram, using the predicted or target duration values.</p> </li> <li> <p>Decoder: The decoder is another Conformer module that takes the output from the variance adaptor modules and generates the final mel-spectrogram prediction.</p> </li> <li> <p>Embeddings: The model includes learnable embeddings for phonemes, speakers, and languages, which are combined and used as input to the encoder.</p> </li> <li> <p>Aligner: The Aligner module is used during training to compute the attention logits and alignments between the encoder output and the target mel-spectrogram.</p> </li> </ol> <p>The <code>forward_train</code> method defines the forward pass during training, where it takes the input phoneme sequences, speaker identities, language identities, mel-spectrograms, pitches, energies, and attention priors (if available). It computes the mel-spectrogram prediction, as well as the predicted pitch, energy, and duration values, which are used for computing the training loss. The forward method defines the forward pass during inference, where it takes the input phoneme sequences, speaker identities, language identities, and duration control value, and generates the mel-spectrogram prediction. Additionally, there are methods for freezing and unfreezing the model parameters, as well as preparing the model for export by removing unnecessary components (e.g., prosody encoders) that are not needed during inference. Implementation follows the DelightfulTTS architecture and incorporates various components for modeling prosody, pitch, energy, and duration, while leveraging the Conformer modules for the encoder and decoder.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#results","title":"Results","text":"","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#total_loss","title":"<code>total_loss</code>","text":"","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#mel_loss","title":"<code>mel_loss</code>","text":"<p>Check our online demo</p> <p>Training the DelightfulTTS model is a computationally intensive task due to its large size and numerous components. Achieving stable and consistent results requires significant computational resources and time. In my experience, using 8 Nvidia V100 GPUs resulted in extremely slow progress, with visible improvements taking weeks to manifest. However, upgrading to 8 Nvidia A100 GPUs significantly accelerated the training process, allowing visible progress to be observed within days. For optimal performance and efficiency, the ideal hardware configuration would be a cluster with 16 Nvidia A100 GPUs. The computational demands of the DelightfulTTS model highlight the importance of leveraging state-of-the-art hardware accelerators, such as the Nvidia A100 GPUs, to enable practical training times and facilitate the development of high-quality text-to-speech systems.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2024/06/11/text-to-speech-tts-models-overview-little-theory-and-math/#future-work","title":"Future work","text":"<p>The combination of autoregressive (AR) and non-autoregressive (NAR) architectures is a promising direction in the text-to-speech (TTS) field. Diffusion models can introduce sufficient variation, and the approach taken by StyledTTS2 is particularly appealing. A potential avenue for future exploration could be to integrate the FastPitch model, known for its speed and stability, with the diffusion block from StyledTTS2. This hybrid approach could leverage the strengths of both models. The NAR nature of FastPitch would contribute to the overall speed and stability of the system, while the diffusion-based component from StyledTTS2 could introduce diversity and expressiveness to the generated speech. By combining the efficient and robust FastPitch model with the expressive capabilities of the diffusion-based variator from StyledTTS2, this hybrid architecture could potentially deliver diverse and natural-sounding speech while maintaining computational efficiency and stability. However, integrating these two distinct approaches into a unified model may pose challenges, and further research would be necessary to investigate the feasibility and effectiveness of such a hybrid solution.</p>","tags":["TTS (Text to Speech)","Pytorch","Pytorch Lightning","Deep Learning","Speech Synthesis","AR Models","NAR Models","Diffusion Models","Audio Processing","Neural Networks","Transformers","Conformer","Prosody Modeling","Pitch Prediction"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/","title":"Weight Initialization Methods in Neural Networks","text":"<p>Weight initialization is crucial in training neural networks, as it sets the starting point for optimization algorithms. The activation function applies a non-linear transformation in our network. Different activation functions serve different purposes. Choosing the right weight initialization and activation function is key to better neural network performance. <code>Xavier</code> initialization is ideal for <code>Sigmoid</code> or <code>Tanh</code> in feedforward networks. <code>He</code> initialization pairs well with <code>ReLU</code> for faster convergence, especially in <code>CNNs</code>. Matching these improves training efficiency and model performance.</p> <p></p> <p>Comparison of different initialization methods</p>","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/#gaussian-initialization","title":"Gaussian Initialization","text":"<p>Gaussian (or Normal) Initialization draws weights from a normal distribution. The concept of Gaussian initialization has its roots in the early studies of neural networks, where researchers recognized the importance of weight initialization in preventing problems like vanishing and exploding gradients. The work of Glorot and Bengio (2010) further emphasized the need for proper initialization methods, leading to the development of techniques like Xavier initialization.</p> <p>Weights \\(W\\) are initialized using a normal distribution defined as:</p> \\[W \\sim \\mathcal{N}(0, \\sigma^2)\\] <p>where \\(\\mathcal{N}(0, \\sigma^2)\\) represents a normal distribution with mean 0 and variance \\(\\sigma^2\\).</p> Gaussian (Normal) Distribution <p>The standard normal distribution \\(\\mathcal{N}(0, 1)\\) can be expressed using its probability density function (PDF):</p> \\[\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}\\] <p>For the standard normal distribution, where \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\), this simplifies to:</p> \\[\\mathcal{N}(x \\mid 0, 1) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{x^2}{2}}\\] <p>Here \\(x\\) is the value for which the probability density is calculated, \\(\\mu\\) is the mean (here 0), and \\(\\sigma^2\\) is the variance (here 1).</p> <p>Here is the weights histogram plot:</p> <pre><code>np.random.seed(96)\n\n# Define input and output sizes\ninput_size, output_size = 2, 256\n\n# Generate normal distribution\nweights_normal = np.random.randn(input_size, output_size)\n\n# Plot histogram\nplt.hist(weights_normal.flatten(), bins=30, edgecolor='black')\nplt.title('Histogram of Gaussian Initialization')\nplt.xlabel('Weight Value')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Histogram of Gaussian Initialization</p> <p>In practice, Gaussian initialization is often implemented as a variant of Xavier initialization, where the weights are drawn from a normal distribution scaled by the input dimension. Most deep learning frameworks use this approach as their default initialization strategy.</p>","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/#xavier-glorot-initialization","title":"Xavier (Glorot) Initialization","text":"<p>The original paper Understanding the Difficulty of Training Deep Feedforward Neural Networks by Xavier Glorot and Yoshua Bengio introduces the concept of Xavier (Glorot) Initialization, which addresses the challenges of training deep neural networks.</p> <p>The authors explore how back-propagated gradients diminish as they move from the output layer to the input layer, particularly under standard initialization methods. This phenomenon can lead to vanishing gradients, making it difficult for deeper layers to learn effectively.</p> <p>They propose a normalized initialization method that maintains consistent variances for activations and gradients across layers.</p> <p></p> <p>Figure 6: Activation values normalized histograms with hyperbolic tangent activation, with standard (top) vs normalized initialization (bottom). Top: 0-peak increases for higher layers.</p> <p></p> <p>Figure 7: Back-propagated gradients normalized histograms with hyperbolic tangent activation, with standard (top) vs normalized (bottom) initialization. Top: 0-peak decreases for higher layers.</p> <p>This is achieved by initializing weights from a uniform distribution between:</p> \\[W \\in (-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}})\\] <p>where \\(n_{\\text{in}}\\) and \\(n_{\\text{out}}\\) are the number of input and output neurons, respectively.</p> <p>Xavier (Glorot) Initialization:</p> \\[W \\sim \\mathcal{N}(0, \\frac{1}{n_{\\text{in}}})\\] <p>where \\(\\mathcal{N}(0, \\frac{1}{n_{\\text{in}}})\\) is the standart normal distribution with the mean 0 and the variance \\(\\frac{1}{n_{\\text{in}}}\\).</p> <p>The <code>Xavier</code> initialization works particularly well with <code>Sigmoid</code> and <code>Tanh</code> activation functions because it helps prevent saturation. By ensuring that the variance of activations remains stable, it allows these functions to operate in their most effective range, thus facilitating better gradient flow during backpropagation.</p> The range of Xavier Uniform Initialization comes from the variance of the weights. <p>Weights \\(W\\) are initialized with a uniform distribution:</p> \\[W \\sim U\\left(-a, a\\right)\\] <p>For a uniform distribution, the variance is given by:</p> \\[\\text{Var}(W) = \\frac{(b - a)^2}{12}\\] <p>For a symmetric uniform distribution (\\(-a\\) to \\(a\\)):</p> \\[\\text{Var}(W) = \\frac{(a - (-a))^2}{12} = \\frac{(2a)^2}{12} = \\frac{4a^2}{12} = \\frac{a^2}{3}\\] <p>To balance the variance of inputs and outputs, we require:  </p> \\[\\text{Var}(\\text{outputs}) = \\text{Var}(\\text{inputs}) = \\frac{1}{n_{\\text{in}}}\\] <p>Thus, the total variance of the weights should satisfy:</p> \\[\\frac{a^2}{3} = \\frac{1}{n_{\\text{in}} + n_{\\text{out}}}\\] <p>Rearranging gives:</p> \\[a = \\sqrt{\\frac{3}{n_{\\text{in}} + n_{\\text{out}}}}\\] <p>To emphasize the range's symmetry and its relation to variance, the expression is often rewritten by factoring out \\(\\sqrt{6}\\). Range for \\(W\\):</p> \\[W \\in \\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right)\\] <p>This equivalence comes from the relationship between the variance \\(\\frac{a^2}{3}\\) and the range \\(-a\\) to \\(a\\). The factor \\(\\sqrt{6}\\) arises naturally because the variance involves dividing the squared range \\((2a)^2 = 4a^2\\) by 12.</p> <p>Here is the python implementation:</p> <pre><code>np.random.seed(96)\n\n# Define input and output sizes\ninput_size, output_size = 2, 256\n\n# Generate normal distribution\nN = np.random.randn(input_size, output_size)\n\n# Xavier initialization\nweights_xavier = N * np.sqrt(1. / input_size)\n\n# Plot histogram\nplt.hist(weights_xavier.flatten(), bins=30, edgecolor='black')\nplt.title('Histogram of Xavier Initialization')\nplt.xlabel('Weight Value')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Histogram of Xavier Initialization</p> <p>The main lines:</p> <pre><code># Generate normal distribution\nN = np.random.randn(input_size, output_size)\n\n# Xavier initialization\nweights_xavier = N * np.sqrt(1. / input_size)\n</code></pre> <p>This directly corresponds to:</p> \\[W = N \\cdot \\sqrt{\\frac{1}{n_{\\text{in}}}}\\] <p>Where: \\(N \\sim \\mathcal{N}(0, 1)\\), a standard normal distribution, and \\(\\sqrt{\\frac{1}{n_{\\text{in}}}}\\) scales the standard deviation to match the Xavier initialization rule.</p> The range of Xavier Uniform Initialization comes from the variance of the weights. <p>Xavier (Glorot) Initialization:</p> \\[W \\sim \\mathcal{N}(0, \\frac{1}{n_{\\text{in}}})\\] <p>This formula initializes weights \\(W\\) with a normal distribution where the variance is \\(\\frac{1}{n_{\\text{in}}}\\), ensuring the weights are scaled to prevent vanishing or exploding gradients during training.</p> <p>To scale the standard normal distribution to have the desired variance, we multiply the standard normal random variable \\(N \\sim \\mathcal{N}(0, 1)\\) by a scaling factor. The scaling factor ensures that the variance of the weights is \\(\\frac{1}{n_{\\text{in}}}\\). </p> <p>Let's define the weight initialization as:</p> \\[W = N \\cdot \\sqrt{\\frac{1}{n_{\\text{in}}}}\\] <p>Here, \\(N\\) is a random variable sampled from the standard normal distribution \\(\\mathcal{N}(0, 1)\\), and multiplying by \\(\\sqrt{\\frac{1}{n_{\\text{in}}}}\\) scales the variance of the weights. If you multiply a random variable \\(N\\) by a constant \\(c\\), the new variance becomes \\(\\text{Var}(c \\cdot N) = c^2 \\cdot \\text{Var}(N)\\). For the standard normal distribution \\(N\\), the variance is 1. By multiplying by \\(\\sqrt{\\frac{1}{n_{\\text{in}}}}\\), the variance of \\(W\\) becomes:</p> \\[\\text{Var}(W) = \\left(\\sqrt{\\frac{1}{n_{\\text{in}}}}\\right)^2 \\cdot \\text{Var}(N) = \\frac{1}{n_{\\text{in}}} \\cdot 1 = \\frac{1}{n_{\\text{in}}}\\] <p>This matches the desired variance in the Xavier initialization, ensuring that the weight distribution has the right scaling for stable training.</p> <p>In modern deep learning practice, Gaussian initialization with Xavier scaling has become the de facto standard due to its robust performance across different architectures and tasks.</p>","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/#he-initialization","title":"He Initialization","text":"<p>He initialization, introduced in the paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification by Kaiming He et al., addresses specific challenges when using <code>ReLU</code> activation functions in deep neural networks. This initialization method was developed to maintain variance across layers specifically for <code>ReLU</code>-based architectures, which became increasingly popular due to their effectiveness in reducing the vanishing gradient problem.</p> <p></p> <p>Figure 2. The convergence of a 22-layer large model (B in Table 3). The x-axis is the number of training epochs. The y-axis is the top-1 error of 3,000 random val samples, evaluated on the center crop. We use ReLU as the activation for both cases. Both our initialization (red) and \"Xavier\" (blue) lead to convergence, but ours starts reducing error earlier.</p> <p>The authors discovered that while <code>Xavier</code> initialization works well for linear and tanh activation functions, it can lead to dead neurons when used with <code>ReLU</code> activations. This occurs because <code>ReLU</code> sets all negative values to zero, effectively reducing the variance of the activations by half.</p> <p>To compensate for this effect, <code>He</code> initialization scales the weights by a factor of \\(\\sqrt{2}\\):</p> \\[W \\sim \\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})\\] <p>where \\(\\mathcal{N}(0, \\frac{2}{n_{\\text{in}}})\\) is the normal distribution with mean 0 and variance \\(\\frac{2}{n_{\\text{in}}}\\), and \\(n_{\\text{in}}\\) is the number of input neurons.</p> Mathematical Intuition Behind He Initialization <p>The factor of 2 in <code>He</code> initialization comes from the <code>ReLU</code> activation function's behavior. When using <code>ReLU</code>:</p> <ol> <li>Approximately half of the neurons will output zero (for negative inputs)</li> <li>The other half will pass through unchanged (for positive inputs)</li> </ol> <p>This means the variance is effectively halved after <code>ReLU</code> activation. To maintain the desired variance:</p> \\[\\text{Var}(W_{\\text{He}}) = \\frac{2}{n_{\\text{in}}} = 2 \\cdot \\text{Var}(W_{\\text{Xavier}})\\] <p>This compensates for the variance reduction caused by <code>ReLU</code>, ensuring proper gradient flow during training.</p> <p>Here is the python implementation:</p> <pre><code># Define input and output sizes\ninput_size, output_size = 2, 256\n\n# Generate normal distribution\nN = np.random.randn(input_size, output_size)\n\n# He initialization\nweights_he = N * np.sqrt(2. / input_size)\n\n# Plot histogram\nplt.hist(weights_he.flatten(), bins=30, edgecolor='black')\nplt.title('Histogram of He Initialization')\nplt.xlabel('Weight Value')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Histogram of He Initialization</p> <p>The main lines:</p> <pre><code># Generate normal distribution\nN = np.random.randn(input_size, output_size)\n\n# He initialization\nweights_he = N * np.sqrt(2. / input_size)\n</code></pre> <p>This directly corresponds to:</p> \\[W = N \\cdot \\sqrt{\\frac{2}{n_{\\text{in}}}}\\] <p>Where: \\(N \\sim \\mathcal{N}(0, 1)\\) is a standard normal distribution, and \\(\\sqrt{\\frac{2}{n_{\\text{in}}}}\\) scales the standard deviation to match the <code>He</code> initialization rule.</p> <p>He initialization has become particularly important in modern deep learning architectures, especially in Convolutional Neural Networks (CNNs) where <code>ReLU</code> is the dominant activation function. By properly scaling the initial weights, <code>He</code> initialization helps maintain healthy gradients throughout the network, enabling faster convergence and better overall performance in deep architectures.</p>","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/#he-initialization-for-leakyrelu","title":"He Initialization for LeakyReLU","text":"<p><code>He</code> initialization can be further adapted for <code>LeakyReLU</code> activation functions. While standard <code>He</code> initialization accounts for <code>ReLU</code>'s zero output for negative inputs, <code>LeakyReLU</code> has a small slope \\(\\alpha\\) for negative values, which affects the variance calculation.</p> <p>For <code>LeakyReLU</code> defined as:</p> \\[ f(x) = \\begin{cases}  x &amp; \\text{if } x &gt; 0 \\\\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases} \\] <p>The initialization is modified to:</p> \\[W \\sim \\mathcal{N}(0, \\frac{2}{(1 + \\alpha^2)n_{\\text{in}}})\\] <p>where \\(\\alpha\\) is the negative slope parameter of <code>LeakyReLU</code> (typically 0.01).</p> Mathematical Intuition Behind He Initialization for LeakyReLU <p>The adjustment for <code>LeakyReLU</code> comes from considering both positive and negative inputs:</p> <ol> <li>For positive inputs (approximately half), the variance remains unchanged</li> <li>For negative inputs (approximately half), the variance is multiplied by \\(\\alpha^2\\)</li> </ol> <p>The total variance after <code>LeakyReLU</code> activation becomes:</p> \\[\\text{Var}(output) = \\frac{1}{2} \\cdot \\text{Var}(input) + \\frac{1}{2} \\cdot \\alpha^2 \\cdot \\text{Var}(input) = \\frac{1 + \\alpha^2}{2} \\cdot \\text{Var}(input)\\] <p>To maintain variance across layers, we scale the initialization by \\(\\sqrt{\\frac{2}{1 + \\alpha^2}}\\) compared to standard <code>He</code> initialization.</p> <p>Here is the python implementation:</p> <pre><code># Define input and output sizes and LeakyReLU alpha\ninput_size, output_size = 2, 256\nalpha = 0.01  # LeakyReLU negative slope\n\n# Generate normal distribution\nN = np.random.randn(input_size, output_size)\n\n# He initialization for LeakyReLU\nweights_he_leaky = N * np.sqrt(2. / ((1 + alpha**2) * input_size))\n\n# Plot histogram\nplt.hist(weights_he_leaky.flatten(), bins=30, edgecolor='black')\nplt.title('Histogram of He Initialization (LeakyReLU)')\nplt.xlabel('Weight Value')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Histogram of He Initialization adapted for LeakyReLU activation</p> <p>The main lines:</p> <pre><code># Generate normal distribution\nN = np.random.randn(input_size, output_size)\n\n# He initialization for LeakyReLU\nweights_he_leaky = N * np.sqrt(2. / ((1 + alpha**2) * input_size))\n</code></pre> <p>This directly corresponds to:</p> \\[W = N \\cdot \\sqrt{\\frac{2}{(1 + \\alpha^2)n_{\\text{in}}}}\\] <p>Where: \\(N \\sim \\mathcal{N}(0, 1)\\) is a standard normal distribution, and \\(\\sqrt{\\frac{2}{(1 + \\alpha^2)n_{\\text{in}}}}\\) scales the standard deviation to account for the <code>LeakyReLU</code> activation function.</p> <p>When \\(\\alpha = 0\\), this reduces to standard <code>He</code> initialization for <code>ReLU</code>. For typical <code>LeakyReLU</code> with \\(\\alpha = 0.01\\), the difference from standard <code>He</code> initialization is minimal but can become more significant with larger \\(\\alpha\\) values.</p>","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/#plot-comparing-initialization-methods","title":"Plot: Comparing Initialization Methods","text":"<p>To better understand the differences between these initialization methods, let's examine them side by side. The following plots show the distribution of weights for each initialization method:</p> <pre><code>np.random.seed(96)\n\n# Define input and output sizes\ninput_size, output_size, bins = 2, 2000, 50\n\n# LeakyReLU negative slope\nalpha = 0.01\n\n# Random normal initialization\nweights_random = np.random.randn(input_size, output_size)\n\n# Xavier (Glorot) initialization\nweights_xavier = np.random.randn(input_size, output_size) * np.sqrt(1. / input_size)\n\n# He initialization\nweights_he = np.random.randn(input_size, output_size) * np.sqrt(2. / input_size)\n\n# Leaky He init\nweights_leaky_he = np.random.randn(input_size, output_size) * np.sqrt(2. / ((1 + alpha**2) * input_size))\n\n# Plotting the histograms for the weights initialized by different methods\nplt.figure(figsize=(18, 6))\n\n# Random init plot\nplt.subplot(2, 4, 1)\nplt.hist(weights_random.flatten(), range=[-3, 3], bins=bins, color='red', alpha=0.7)\nplt.title('Random Normal Initialization')\nplt.grid(True)\n\n# Xavier init plot\nplt.subplot(2, 4, 2)\nplt.hist(weights_xavier.flatten(), range=[-3, 3], bins=bins, color='green', alpha=0.7)\nplt.title('Xavier Initialization')\nplt.grid(True)\n\n# He initialization plot\nplt.subplot(2, 4, 3)\nplt.hist(weights_he.flatten(), range=[-3, 3], bins=bins, color='blue', alpha=0.7)\nplt.title('He Initialization')\nplt.grid(True)\n\n# Leaky He initialization plot\nplt.subplot(2, 4, 4)\nplt.hist(weights_leaky_he.flatten(), range=[-3, 3], bins=bins, color='black', alpha=0.7)\nplt.title('Leaky He Initialization')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Comparison of different initialization methods (left-right): Random Normal, Xavier, He, and Leaky He</p> <p>The plots reveal several key differences between the initialization methods:</p> <ol> <li> <p>Random Normal Initialization (red) shows the widest spread of weights, with no consideration for the network architecture. This can lead to vanishing or exploding gradients, especially in deeper networks.</p> </li> <li> <p>Xavier Initialization (green) demonstrates a more controlled distribution, with weights scaled according to the input dimension. The narrower spread helps maintain stable gradients when using sigmoid or tanh activation functions.</p> </li> <li> <p>He Initialization (blue) shows a slightly wider distribution than Xavier, but still maintains a structured spread. The increased variance compensates for the ReLU activation function's tendency to zero out negative values.</p> </li> <li> <p>Leaky He Initialization (black) scales the variance for <code>LeakyReLU</code> activations, slightly narrowing the spread compared to He Initialization. This accounts for the negative slope \\(\\alpha\\), ensuring effective propagation of both positive and negative signals.</p> </li> </ol> Distribution Characteristics <p>The variance of each distribution reflects its intended use:</p> <ul> <li>Random Normal: \\(\\sigma^2 = 1\\)</li> <li>Xavier: \\(\\sigma^2 = \\frac{1}{n_{\\text{in}}}\\)</li> <li>He: \\(\\sigma^2 = \\frac{2}{n_{\\text{in}}}\\)</li> <li>Leaky He: \\(\\sigma^2 = \\frac{2}{(1 + \\alpha^2)n_{\\text{in}}}\\)</li> </ul> <p>These differences in variance directly impact how well each method maintains the signal through deep networks with different activation functions.</p>","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/#universal-parameter-implementation","title":"Universal Parameter Implementation","text":"<p>The <code>Parameter</code> class for the weight initialization.</p> <pre><code>import numpy as np\nfrom typing import Literal\n\ndef parameter(\n    input_size: int,\n    output_size: int,\n    init_method: Literal[\"xavier\", \"he\", \"he_leaky\", \"normal\", \"uniform\"] = \"xavier\",\n    gain: float = 1,\n    alpha: float = 0.01\n) -&gt; np.ndarray:\n    \"\"\"\n    Initialize weights using specified initialization method.\n\n    Args:\n        input_size (int): Number of input neurons.\n        output_size (int): Number of output neurons.\n        init_method (str): Method of initialization (\"xavier\", \"he\", \"he_leaky\", \"normal\", \"uniform\").\n        gain (float): Scaling factor for weight initialization.\n        alpha (float): Slope for Leaky ReLU in \"he_leaky\" initialization.\n\n    Returns:\n        np.ndarray: The initialized weight matrix.\n\n    Raises:\n        ValueError: If the initialization method is unknown.\n    \"\"\"\n    weights = np.random.randn(input_size, output_size)\n\n    if init_method == \"xavier\":\n        std = gain * np.sqrt(1.0 / input_size)\n        return std * weights\n    if init_method == \"he\":\n        std = gain * np.sqrt(2.0 / input_size)\n        return std * weights\n    if init_method == \"he_leaky\":\n        std = gain * np.sqrt(2.0 / (1 + alpha**2) * (1 / input_size))\n        return std * weights\n    if init_method == \"normal\":\n        return gain * weights\n    if init_method == \"uniform\":\n        return gain * np.random.uniform(-1, 1, size=(input_size, output_size))\n\n    raise ValueError(f\"Unknown initialization method: {init_method}\")\n\n\nclass Parameter:\n    \"\"\"\n    A class to represent and initialize neural network parameters (weights).\n\n    Attributes:\n        gain (float): Scaling factor for weight initialization.\n        input_size (int): Number of input neurons.\n        output_size (int): Number of output neurons.\n\n    Methods:\n        he(): Initializes weights using He initialization.\n        he_leaky(alpha): Initializes weights using He initialization with Leaky ReLU.\n        xavier(): Initializes weights using Xavier initialization.\n        random(): Initializes weights with a normal distribution.\n        uniform(): Initializes weights with a uniform distribution.\n    \"\"\"\n    def __init__(self, input_size: int, output_size: int, gain: float = 1):\n        \"\"\"\n        Initialize the Parameter object with input size, output size, and scaling factor.\n\n        Args:\n            input_size (int): Number of input neurons.\n            output_size (int): Number of output neurons.\n            gain (float): Scaling factor for initialization.\n        \"\"\"\n        self.input_size = input_size\n        self.output_size = output_size\n        self.gain = gain\n\n    def he(self) -&gt; np.ndarray:\n        \"\"\"\n        Initializes weights using He initialization (for ReLU activations).\n\n        Returns:\n            np.ndarray: The initialized weight matrix.\n        \"\"\"\n        return parameter(self.input_size, self.output_size, \"he\", self.gain)\n\n    def he_leaky(self, alpha: float = 0.01) -&gt; np.ndarray:\n        \"\"\"\n        Initializes weights using He initialization with Leaky ReLU.\n\n        Args:\n            alpha (float): Slope of Leaky ReLU.\n\n        Returns:\n            np.ndarray: The initialized weight matrix.\n        \"\"\"\n        return parameter(self.input_size, self.output_size, \"he_leaky\", self.gain, alpha)\n\n    def xavier(self) -&gt; np.ndarray:\n        \"\"\"\n        Initializes weights using Xavier initialization.\n\n        Returns:\n            np.ndarray: The initialized weight matrix.\n        \"\"\"\n        return parameter(self.input_size, self.output_size, \"xavier\", self.gain)\n\n    def random(self) -&gt; np.ndarray:\n        \"\"\"\n        Initializes weights with a standard normal distribution.\n\n        Returns:\n            np.ndarray: The initialized weight matrix.\n        \"\"\"\n        return parameter(self.input_size, self.output_size, \"normal\", self.gain)\n\n    def uniform(self) -&gt; np.ndarray:\n        \"\"\"\n        Initializes weights using a uniform distribution.\n\n        Returns:\n            np.ndarray: The initialized weight matrix.\n        \"\"\"\n        return parameter(self.input_size, self.output_size, \"uniform\", self.gain)\n</code></pre> <p>Example of Usage:</p> <pre><code>param = Parameter(input_size=1, output_size=10, gain=0.1)\nweights = param.he()  # Will use \"he\" initialization\nweights\n</code></pre> <p>Output:</p> <pre><code>array([[-0.13668641,  0.06869297,  0.14488719,  0.13556128, -0.10379101,\n        -0.02760059, -0.20395249,  0.08597498, -0.11700688, -0.21882887]])\n</code></pre>","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2025/01/14/weight-initialization-methods-in-neural-networks/#conclusion","title":"Conclusion","text":"<p>The choice of weight initialization method significantly impacts neural network training dynamics. Through our analysis, we can draw several key conclusions:</p> <ol> <li> <p>Random (Normal) initialization, while simple, lacks the mathematical foundation to ensure stable gradient flow in deep networks.</p> </li> <li> <p><code>Xavier</code> initialization provides a robust solution for networks using <code>sigmoid</code> or <code>tanh</code> activation functions by maintaining variance across layers.</p> </li> <li> <p><code>He</code> initialization builds upon <code>Xavier's</code> insights to specifically address the characteristics of <code>ReLU</code> activation functions, making it the preferred choice for modern architectures using <code>ReLU</code> and its variants.</p> </li> <li> <p><code>Leaky He</code> initialization extends <code>He</code> initialization to account for the non-zero negative slope of <code>LeakyReLU</code> activations, ensuring both positive and negative signals propagate effectively through the network. The slight adjustment in variance makes it ideal for networks with <code>LeakyReLU</code> or similar activations.</p> </li> </ol> <p>The evolution from random to <code>Xavier</code> to <code>He</code> initialization reflects our growing understanding of deep neural networks. Each method addresses specific challenges in training deep networks, with <code>He</code> initialization currently standing as the most widely used approach in modern architectures, particularly those employing <code>ReLU</code> activations.</p> <p>While batch normalization has made weight initialization less critical, it\u2019s still important. Batch normalization normalizes layer inputs, reducing the effects of poor initialization. Even so, starting with proper methods like <code>Xavier</code> or <code>He</code> can improve convergence speed. Combining good initialization with batch normalization gives the best results for training deep networks.</p>","tags":["Classification","Deep Learning","Weight Initialization","Xavier Initialization","He Initialization","ReLU","Neural Network Training"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/","title":"Why Does the Gradient Point Upwards?","text":"<p>The gradient, \\( \\nabla f(\\textbf{x}) \\), tells us the direction in which a function increases the fastest. But why?</p> <p></p> <p>Gradient direction in 3D from Min =&gt; Max</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#check-the-jupyter-notebook","title":"Check the jupyter notebook","text":"","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#what-is-gradient","title":"What is gradient?","text":"<p>Gradient is a vector of partial derivatives of a function. It's a vector of derivatives of a function with respect to its variables. For example, if we have a function \\(f(x, y)\\), then the gradient of this function is a vector of its partial derivatives with respect to \\(x\\) and \\(y\\).</p> \\[\\nabla f(x, y) = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right)\\] <p>The Gradient is the Derivative of a multivariable (multi-dimensional) function in every direction. Strictly speaking, it's the Partial Derivatives; we apply the derivative to each variable one at a time. We can simply understand the process in one direction and then do the same for all directions!</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#derivative-definition","title":"Derivative definition","text":"<p>The derivative measures how the function's value changes as we move along the input axis. Here's why based on the limit definition - the derivative at a point \\(x\\) is defined as:</p> \\[f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\\] <p>This expression calculates the rate of change of the function \\(f(x)\\) over a small interval \\(h\\).</p> <p>The fraction shows how much the function value (\\(f(x)\\)) changes per unit change in \\(x\\):</p> \\[\\frac{f(x + h) - f(x)}{h}\\] <p>This is the slope of the line connecting \\(x\\) and \\(x + h\\).</p> <p>As \\(h\\) gets smaller the limit \\(\\lim_{h \\to 0}\\) makes \\(h\\) shrink to an infinitesimally small value. This zooms in on the curve around \\(x\\), ensuring we measure the exact slope at \\(x\\).</p> <p>If you follow the derivative means you follow to the steepest ascent all the time. The derivative gives the direction of the fastest increase in \\(f(x)\\) because it captures how \\(f(x)\\) changes most rapidly with respect to \\(x\\). </p> <p>At any point \\( x \\), the derivative \\( f'(x) \\) indicates the slope of the tangent line, showing how \\( f(x) \\) changes most rapidly with respect to \\( x \\). If you move in the direction of \\( f'(x) \\), you are indeed moving towards the steepest ascent of \\( f(x) \\).</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#direction-of-movement","title":"Direction of Movement","text":"<ul> <li> <p>If \\( f'(x) \\) is positive, moving in the direction of the derivative increases \\( f(x) \\).</p> </li> <li> <p>If \\( f'(x) \\) is negative, following \"the negative\" side from the perspective of the \\( x \\) - axis means moving in the direction opposite to the derivative to increase \\( f(x) \\).</p> </li> </ul> <p>Mathematical Expression: Given a function \\( f(x) \\) and starting at \\( x_0 \\), the update rule for moving along the steepest ascent can be written as:</p> \\[ x_{n+1} = x_n + \\alpha \\cdot f'(x_n) \\] <p>where:</p> <ul> <li> <p>\\( x_n \\) is your current position on the \\( x \\)-axis.</p> </li> <li> <p>\\( f'(x_n) \\) is the derivative at \\( x_n \\).</p> </li> <li> <p>\\( \\alpha \\) (learning rate) determines how big a step we take towards the steepest ascent.</p> </li> </ul> <p>Explanation: </p> <ul> <li>If \\( f'(x_n) &gt; 0 \\), \\( x_{n+1} \\) will be greater than \\( x_n \\), moving you uphill along \\( f(x) \\).</li> </ul> <p></p> <p>Before the extrema point</p> <ul> <li>If \\( f'(x_n) &lt; 0 \\), \\( x_{n+1} \\) will still be greater than \\( x_n \\) because you're adding a negative value (which is correct if we interpret \"following the negative\" as moving in the opposite direction of the derivative to increase the function).</li> </ul> <p></p> <p>After the extrema point</p> <p>This <code>ascent</code> function does exactly this, iteratively applying the derivative to move towards where the function \\( f(x) \\) increases most rapidly, whether that's by directly following a positive derivative or by counteracting a negative derivative.</p> <pre><code>def ascent(f, x0, steps=5, lr=0.3):\n    result = [x0]\n\n    for _ in range(steps):\n        dfx0 = cdiff(f, x0)\n        x1 = x0 + dfx0 * lr\n        result.append(x1)\n        x0 = x1\n\n    return result\n</code></pre> <p><code>cdiff</code> here is the Central Difference</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#gradient-and-steepest-ascent-in-multiple-dimensions","title":"Gradient and Steepest Ascent in Multiple Dimensions","text":"<p>For the gradient we just follow the same logic but for every variable in the gradient vector. Just as in the single-variable case, the gradient in multiple dimensions gives us the direction of the fastest increase for a multivariable function \\( f(x, y) \\). The gradient vector \\( \\nabla f(x, y) \\) contains partial derivatives with respect to each variable:</p> \\[ \\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) \\] <p>Direction of Movement for Gradient: Each component of the gradient vector points in the direction where \\( f \\) increases most rapidly for that particular variable.</p> <p>Mathematical Expression for Gradient Ascent:</p> <p>To ascend along the gradient, we update each variable according to its corresponding partial derivative:</p> \\[ \\mathbf{x}_{n+1} = \\mathbf{x}_n + \\alpha \\cdot \\nabla f(\\mathbf{x}_n) \\] <p>where:</p> <ul> <li> <p>\\( \\mathbf{x}_n = \\begin{pmatrix} x_n \\\\ y_n \\end{pmatrix} \\) represents the current point in the space.</p> </li> <li> <p>\\( \\nabla f(\\mathbf{x}_n) \\) is the gradient at that point.</p> </li> <li> <p>\\( \\alpha \\) (learning rate) scales the step size.</p> </li> </ul> <p>Explanation:</p> <ul> <li>Positive Component: If a partial derivative is positive, moving in the direction of that component increases \\( f \\).</li> <li>Negative Component: If a partial derivative is negative, moving opposite to that component (i.e., in the direction of \\( -\\nabla f \\)) still aims to increase \\( f \\), as you're moving away from where the function decreases.</li> </ul> <p>Gradient Ascent Function:</p> <pre><code>def gradient_ascent(f, initial_coords, steps=5, lr=0.3):\n    result = [initial_coords]\n    x = initial_coords  # Use a list or numpy array for vector-like operations\n\n    for _ in range(steps):\n        # Assuming we have a function to compute the gradient\n        grad = compute_gradient(f, *x)\n        x = [xi + gi * lr for xi, gi in zip(x, grad)]\n        result.append(x.copy())\n\n    return result\n</code></pre> <p>Here, <code>compute_gradient(f, *x)</code> would return the gradient vector at <code>x</code>, where <code>x</code> is a list or array of coordinates. This function iteratively moves in the direction of the gradient to find where the function <code>f(x, y)</code>, increases most rapidly, much like the single-variable case but now in multiple dimensions.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#gradient-ascent-in-action","title":"Gradient ascent in action!","text":"<pre><code>import numpy as np\n\n# Example function and its gradient\ndef f(x, y):\n    return -(x**2 + y**2) + 4  # Paraboloid (maximum at (0, 0))\n\ndef grad_f(x, y):\n    return [-2 * x, -2 * y]  # Gradient of the function\n\n\ndef gradient_ascent(grad_f, initial_coords, steps=5, lr=0.3):\n    \"\"\"\n    Perform gradient ascent to maximize the function f.\n\n    Parameters:\n    - grad_f: A function that returns the gradient of f as a list or array.\n    - initial_coords: A list or array of initial coordinates for the ascent.\n    - steps: Number of iterations for the ascent.\n    - lr: Learning rate, which controls the step size.\n\n    Returns:\n    - result: A list of coordinates visited during the ascent.\n    \"\"\"\n    result = [initial_coords.copy()]  # Copy to avoid modification issues\n    current_point = initial_coords    # Use descriptive names for clarity\n\n    for _ in range(steps):\n        # Compute the gradient at the current point\n        grad = grad_f(*current_point)\n        # Update each coordinate using the gradient and learning rate\n        current_point = [xi + gi * lr for xi, gi in zip(current_point, grad)]\n        result.append(current_point.copy())  # Store a copy of the current point\n\n    return result\n\n\n# Perform gradient ascent\ntrajectory = gradient_ascent(grad_f, initial_coords=[1, 1], steps=10, lr=0.1)\ntrajectory\n</code></pre>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#output","title":"Output","text":"<p>You can see how it goes upwards to the maxima!</p> <pre><code>[[1, 1],\n [0.8, 0.8],\n [0.64, 0.64],\n [0.512, 0.512],\n [0.4096, 0.4096],\n [0.32768, 0.32768],\n [0.26214400000000004, 0.26214400000000004],\n [0.20971520000000005, 0.20971520000000005],\n [0.16777216000000003, 0.16777216000000003],\n [0.13421772800000004, 0.13421772800000004],\n [0.10737418240000003, 0.10737418240000003]]\n</code></pre>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#the-dot-product-formula-is-the-key","title":"The Dot Product Formula is the key!","text":"<p>For a given unit vector \\( \\vec{v} \\), the directional derivative measures this, and it's defined as:</p> \\[\\nabla_{\\vec{v}} f = \\nabla f(\\textbf{x}) \\cdot \\vec{v}\\] <p>Using the dot product formula, we can rewrite this as:</p> \\[\\nabla_{\\vec{v}} f = |\\nabla f(\\textbf{x})||\\vec{v}|\\cos \\theta\\] <p>Since \\( \\vec{v} \\) is a unit vector (\\( |\\vec{v}| = 1 \\)), this simplifies to:</p> \\[\\nabla_{\\vec{v}} f = |\\nabla f(\\textbf{x})| \\cos \\theta\\] <p>Here, \\( \\theta \\) is the angle between \\( \\nabla f(\\textbf{x}) \\) and \\( \\vec{v} \\). The key insight? The cosine of the angle, \\( \\cos \\theta \\), determines how large the directional derivative is:</p> <p>When \\( \\theta = 0^\\circ \\): \\( \\cos(0) = 1 \\), so \\( \\nabla_{\\vec{v}} f \\) reaches its maximum value:</p> \\[\\nabla_{\\vec{v}} f = |\\nabla f(\\textbf{x})|\\] <p>For any other angle: \\( \\cos \\theta &lt; 1 \\), so the directional derivative is smaller.</p> <p>Thus, the gradient \\( \\nabla f(\\textbf{x}) \\) points in the steepest ascent direction because that's where \\( \\cos \\theta = 1 \\) the function increases the fastest when you move directly in the direction of the gradient!</p> <p>The gradient vector \\( \\nabla f(\\textbf{x}) \\) not only gives the direction of steepest ascent but also quantifies how steep this ascent is, and by using the cosine of the angle between the gradient and any other direction, we can determine the rate of change in that direction. This understanding is fundamental in fields like optimization where we aim to find the path of maximum increase or decrease of a function.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#visualizing-the-gradient-field","title":"Visualizing the Gradient Field","text":"<p>The function \\( f(x, y) = x^2 + y^2 \\) is a classic example. Its gradient points outward, showing how the function rises steeply as you move away from the origin. Here's an interactive plot to explore the gradient field:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, IntSlider, FloatSlider\n\n\ndef f2d(x, y):\n    r\"\"\"\n    3d Paraboloid function $f(x, y) = x^2 + y^2$.\n\n    Args:\n    x: x coordinate\n    y: y coordinate\n\n    Returns:\n    Value of the function at point (x, y)\n    \"\"\"\n    return x**2 + y**2\n\n\ndef grad_f2d(x, y):\n    \"\"\"\n    Gradient of the function $f(x, y) = x^2 + y^2$.\n\n    Args:\n    x: x coordinate\n    y: y coordinate\n\n    Returns:\n    Gradient of the function at point (x, y)\n    \"\"\"\n    return np.array([2*x, 2*y])\n\n\ndef plot_gradient_field(density=10, arrow_scale=20):\n    r\"\"\"\n    Plot the gradient field of the function $f(x, y) = x^2 + y^2$.\n\n    Args:\n    density: density of the grid\n    arrow_scale: scale of the arrows\n    \"\"\"\n    # Create the x/y grid\n    x = np.linspace(-5, 5, density)\n    y = np.linspace(-5, 5, density)\n\n    # Create the meshgrid X/Y\n    X, Y = np.meshgrid(x, y)\n    # Compute the function values\n    Z = f2d(X, Y)\n\n    # Create the figure and subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n\n    # 3D surface plot\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('Z')\n    ax1.set_title('3D Surface Plot')\n\n    # 2D contour plot with gradient field\n    ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\n\n    # Compute the gradient\n    U, V = grad_f2d(X, Y)\n    # Plot the gradient field\n    ax2.quiver(X, Y, U, V, scale=arrow_scale, scale_units='inches', color='w', alpha=0.7)\n\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_title('Gradient Field')\n\n    plt.tight_layout()\n    plt.close(fig)\n    return fig\n\n# Create interactive plot\ninteract(plot_gradient_field, \n         density=IntSlider(min=5, max=30, step=1, value=10, description='Grid Density'),\n         arrow_scale=FloatSlider(min=1, max=100, step=1, value=20, description='Arrow Scale'))\n</code></pre> <p></p> <p>Gradient field direction in 3D towards the maximum</p> <p>When exploring this plot, notice how the arrows point directly away from the origin\u2014the direction of the steepest ascent. By following these arrows in reverse (negative gradient), you can descend to the minimum.</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"2024/12/04/why-does-the-gradient-point-upwards/#gradient-descent-in-3d","title":"Gradient descent in 3D","text":"<p>I just leave this code as an exercise for you.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider, IntSlider\n\n\ndef f2d(x, y):\n    r\"\"\"\n    3d Paraboloid function $f(x, y) = x^2 + y^2$.\n\n    Args:\n    x: x coordinate\n    y: y coordinate\n\n    Returns:\n    Value of the function at point (x, y)\n    \"\"\"\n    return x**2 + y**2\n\n\ndef grad_f2d(x, y):\n    \"\"\"\n    Gradient of the function $f(x, y) = x^2 + y^2$.\n\n    Args:\n    x: x coordinate\n    y: y coordinate\n\n    Returns:\n    Gradient of the function at point (x, y)\n    \"\"\"\n    return np.array([2*x, 2*y])\n\n\ndef gradient_descent3d(start_x, start_y, learning_rate, num_iterations):\n    path = [(start_x, start_y)]\n    x, y = start_x, start_y\n    for _ in range(num_iterations):\n        grad = grad_f2d(x, y)\n        x -= learning_rate * grad[0]\n        y -= learning_rate * grad[1]\n        path.append((x, y))\n    return np.array(path)\n\n\ndef plot_gradient_descent(start_x, start_y, learning_rate, num_iterations):\n    # Create the surface\n    x = np.linspace(-10, 10, 100)\n    y = np.linspace(-10, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = f2d(X, Y)\n\n    # Perform gradient descent\n    path = gradient_descent3d(start_x, start_y, learning_rate, num_iterations)\n\n    # Create the figure and subplots\n    fig = plt.figure(figsize=(20, 10))\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax2 = fig.add_subplot(122)\n\n    # 3D plot\n    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n    ax1.plot(path[:, 0], path[:, 1], f2d(path[:, 0], path[:, 1]), 'r.-', linewidth=2, markersize=20)\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('Z')\n    ax1.set_title('3D Gradient Descent Visualization')\n\n    # 2D contour plot with filled contours for depth\n    contour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\n    ax2.plot(path[:, 0], path[:, 1], 'r&gt;-', linewidth=2, markersize=10)\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_title('2D Contour Plot with Gradient Descent Path')\n\n    # Add color bars\n    fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n    fig.colorbar(contour, ax=ax2)\n\n    plt.tight_layout()\n    plt.close(fig)\n    return fig\n\n\n# Create interactive plot\ninteract(plot_gradient_descent, \n         start_x=FloatSlider(min=-10, max=10, step=0.1, value=8, description='Start X'),\n         start_y=FloatSlider(min=-10, max=10, step=0.1, value=8, description='Start Y'),\n         learning_rate=FloatSlider(min=0.01, max=0.95, step=0.01, value=0.1, description='Learning Rate'),\n         num_iterations=IntSlider(min=1, max=50, step=1, value=20, description='Iterations'))\n</code></pre> <p></p> <p>Gradient descent in 3D</p>","tags":["Gradient Descent","Optimization Algorithms","Machine Learning Basics","Python Visualization","Matplotlib"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/deep-learning/","title":"Deep Learning","text":""},{"location":"category/machine-learning/","title":"Machine Learning","text":""},{"location":"category/neural-networks/","title":"Neural Networks","text":""},{"location":"category/mathematics/","title":"Mathematics","text":""},{"location":"category/programming/","title":"Programming","text":""},{"location":"category/classification/","title":"Classification","text":""},{"location":"category/optimizations/","title":"Optimizations","text":""},{"location":"category/data-science/","title":"Data Science","text":""},{"location":"category/loss-functions/","title":"Loss Functions","text":""},{"location":"category/computational-methods/","title":"Computational Methods","text":""},{"location":"category/numerical-methods/","title":"Numerical Methods","text":""},{"location":"category/dimensionality-reduction/","title":"Dimensionality Reduction","text":""},{"location":"category/feature-engineering/","title":"Feature Engineering","text":""},{"location":"category/data-visualization/","title":"Data Visualization","text":""},{"location":"category/linear-algebra/","title":"Linear Algebra","text":""},{"location":"category/data-transformations/","title":"Data Transformations","text":""},{"location":"category/tts-text-to-speech/","title":"TTS (Text to Speech)","text":""},{"location":"category/speech-and-audio-processing/","title":"Speech and Audio Processing","text":""},{"location":"category/natural-language-processing/","title":"Natural Language Processing","text":""},{"location":"page/2/","title":"Welcome to DataSanta","text":""},{"location":"page/3/","title":"Welcome to DataSanta","text":""},{"location":"page/4/","title":"Welcome to DataSanta","text":""},{"location":"archive/2025/page/2/","title":"2025","text":""},{"location":"archive/2024/page/2/","title":"2024","text":""},{"location":"category/deep-learning/page/2/","title":"Deep Learning","text":""},{"location":"category/machine-learning/page/2/","title":"Machine Learning","text":""},{"location":"category/machine-learning/page/3/","title":"Machine Learning","text":""},{"location":"category/mathematics/page/2/","title":"Mathematics","text":""},{"location":"category/neural-networks/page/2/","title":"Neural Networks","text":""},{"location":"category/programming/page/2/","title":"Programming","text":""}]}